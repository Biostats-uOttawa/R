{"title":"Introduction to `GLMM`","markdown":{"yaml":{"params":{"longrun":false}},"headingText":"Introduction to `GLMM`","containsRefs":false,"markdown":"\n\n\n## Lecture\n\ntheoretical intro to glmm and introduce DHarma package to evaluate fit of glmm\n\n```{r}\n#| echo: false\n#| out-width: 50%\n#| fig-align: center\n#| fig-cap: Dream pet dragon\nknitr::include_graphics(\"images/fun_dragon.jpg\")\n```\n\n\n\n## Practical\n\nThis is an adapted version largely inspired by the tutorial in [@bolker_generalized_2009].\nSpatial variation in nutrient availability and herbivory is likely to cause population differentiation and maintain genetic diversity in plant populations.Here we measure the extent to which mouse-ear cress (Arabidopsis thaliana)exhibits population and genotypic variation in their responses to these im-portant environmental factors. We are particularly interested in whether these populations exhibit nutrient mediated compensation, where higher nutrient levels allow genotypes to better tolerate herbivory [@banta_comprehensive_2010]. We use GLMMs to estimate the effect of nutrient levels, simulated herbivory, and their interaction on fruit production in Arabidopsis thaliana(fixed effects), and the extent to which populations vary in their responses(random effects, or variance components)\n\n### Packages and functions\n\nYou need to download the \"extra_funs.R\" script for some functions used in the Practical\n\n```{r}\n#| eval: true\n#| warning: false\n#| message: false\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(lattice)\nlibrary(DHARMa)\nsource(\"data/extra_funs.R\")\n```\n\n### The data set\n\nIn this data set, the response variable is the number of fruits (i.e. seed capsules) per plant. The number of fruits produced by an individual plant(the experimental unit) was hypothesized to be a function of fixed effects,including nutrient levels (low vs. high), simulated herbivory (none vs. apical meristem damage), region (Sweden, Netherlands, Spain), and interactions among these. Fruit number was also a function of random effects including both the population and individual genotype. Because Arabidopsis is highly selfing, seeds of a single individual served as replicates of that individual.There were also nuisance variables, including the placement of the plant in the greenhouse, and the method used to germinate seeds. These were estimated as fixed effects but interactions were excluded.\n\n- `X` observation number (we will use this observation number later, when we are accounting for overdispersion)\n- `reg` a factor for region (Netherlands, Spain, Sweden).\n- `popu` a factor with a level for each population.\n- `gen` a factor with a level for each genotype.\n- `rack` a nuisance factor for one of two greenhouse racks.\n- `nutrient` a factor with levels for minimal or additional nutrients.\n- `amd` a factor with levels for no damage or simulated herbivory (apical meristem damage; we will sometimes refer to this as “clipping”)\n- `status` a nuisance factor for germination method.\n- `total.fruits` the response; an integer count of the number of fruits per plant.\n\n### Specifying fixed and random Effects\n\nHere we need to select a realistic full model, based on the scientific questions and the data actually at hand. We first load the data set and make sure that each variable is appropriately designated as numeric or factor (i.e.categorical variable).\n\n```{r}\ndat_tf <- read.csv(\"data/Banta_TotalFruits.csv\")\nstr(dat_tf)\n```\n\nThe `X`, `gen`, `rack` and `nutrient` variables are coded as integers, but we want them to be factors.\n We use `mutate()` `dplyr` `r emoji::emoji(\"package\")`, which operates within the data set, to avoid typing lots of commands like `dat_tf$rack <- factor(dat_tf$rack)`\n At the same time, we reorder the `clipping` variable so that `\"unclipped\"` is the reference level (we could also have used `relevel(amd,\"unclipped\")`).\n\n```{r}\ndat_tf <- mutate(\n  dat_tf,\n  X = factor(X),\n  gen = factor(gen),\n  rack = factor(rack),\n  amd = factor(amd, levels = c(\"unclipped\", \"clipped\")),\n  nutrient = factor(nutrient, label = c(\"Low\", \"High\"))\n)\n```\n\nNow we check replication for each genotype (columns) within each population (rows).\n\n```{r}\n(reptab <- with(dat_tf, table(popu, gen)))\n```\n\n::: {.callout-caution}\n# Exercise\n**Exercise**: this mode of inspection is OK for this data set but might fail for much larger data sets or for more levels of nesting. See if you can think of some other numerical or graphical methods for inspecting the structure of data sets. \n\n1. plot(reptab) gives a mosaic plot of the two-way table; examine this, see if you can figure out how to interpret it, and decide whether you think it might be useful\n2. try the commands colSums(reptab>0) (and the equivalent for rowSums) and figure out what they are telling you.\n3. Using this recipe, how would you compute the range of number of genotypes per treatment combination?\n:::\n\n::: {.callout-tip collapse='true'}\n\n# Solution\n\n1. Do you find the mosaic plot you obtained ugly and super hard to read? Me too `r emoji::emoji(\"laugh\")`\n```{r}\n#| fig-cap: A truly useless plot no one can understand\nplot(reptab)\n```\n\n2. `colSums()` do the sum of all the rows for each columns of a table. So `colSums(reptab>0)` gives you for each genotype the number of populations (lines) where you have at least 1 observations.\n\n```{r}\ncolSums(reptab > 0)\nrowSums(reptab > 0)\n```\n\n3. You firts need to create a new table of number of observations per treatment and genotypes\n```{r}\nreptab2 <- with(dat_tf, table(paste(amd, nutrient, sep = \"_\"), gen))\nrange(reptab2)\n```\n\n:::\n\nThis reveals that we have only 2–4 populations per region and 2–3 genotypes per population. However, we also have 2–13 replicates per genotype for each treatment combination (four unique treatment combinations: 2 levels of nutrients by 2 levels of simulated herbivory). Thus, even though this was a reasonably large experiment (625 plants), there were a very small number of replicates with which to estimate variance components, and many more potential interactions than our data can support. Therefore, judicious selection of model terms, based on both biology and the data, is warranted. We note that we don’t really have enough levels per random effect, nor enough replication per unique treatment combination. Therefore, we decide to omit the fixed effect of “region”, although we recognize that populations in different regions are widely geographically separated.\n\nHowever, as in all GLMMs where the scale parameter is treated as fixed and deviations from the fixed scale parameter would be identifiable (i.e. Poisson and binomial (N > 1), but not binary, models) we may have to deal with overdispersion.\n\n\n### Look at overall patterns in data\n\n\nI usually like to start with a relatively simple overall plot of the data, disregarding the random factors, just to see what’s going on. For reasons to be discussed below, we choose to look at the data on the log (or log(1 + x) scale. Let’s plot either box-and-whisker plots (useful summaries) or dot plots (more detailed, good for seeing if we missed anything).\n\n```{r}\n#| echo: false\n#| purl: false\n#| fig-cap: Number of fruits (log + 1) as a function of treatments\np1 <- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf, geom = \"boxplot\") +\n  facet_wrap(~reg, nrow = 1) +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Boxplot\")\np2 <- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf) +\n  facet_wrap(~reg, nrow = 1) +\n  stat_sum() +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Dot plot\")\np1 + p2\n```\n\n::: {.callout-caution}\n# Exercise\nGenerate these plots and figure out how they work before continuing. Try conditioning/faceting on population rather than region: for facet_wrap you might want to take out the nrow = 1 specification. If you want try reorder the subplots by overall mean fruit set and/or colour the points according to the region they come from.\n:::\n\n::: {.callout-tip collapse='true'}\n\n# Solution\n\n```{r}\n#| eval: false\n#| fig-cap: Number of fruits (log + 1) as a function of treatments\np1 <- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf, geom = \"boxplot\") +\n  facet_wrap(~reg, nrow = 1) +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Boxplot\")\np2 <- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf) +\n  facet_wrap(~reg, nrow = 1) +\n  stat_sum() +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Dot plot\")\np1 + p2\n```\n\n:::\n\n\n### Choose an error distribution\n\nThe data are non-normal in principle (i.e., count data, so our first guess would be a Poisson distribution). If we transform total fruits with the canonical link function (log), we hope to see relatively homogeneous variances across categories and groups.\n\nFirst we define a new factor that represents every combination of genotype and treatment (nutrient × clipping) treatment, and sort it in order of increasing mean fruit set.\n\n```{r}\ndat_tf <- dat_tf %>%\n  mutate(\n    gna = reorder(interaction(gen, nutrient, amd), total.fruits, mean)\n  )\n```\n\nNow time to plot it\n\n```{r}\n#| fig-cap: Boxplot of total fruits (log + 1) per genotypes and treatments\nggplot(dat_tf, aes(x = gna, y = log(1 + total.fruits))) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90))\n```\n\nWe could also calculate the variance for each genotype × treatment combination and provide a statistical summary of these variances.\nThis reveals substantial variation among the sample variances on the transformed data. In addition to heterogeneous variances across groups, Figure 1 reveals many zeroes in groups, and some groups with a mean and variance of zero, further suggesting we need a non-normal error distribution, and perhaps something other than a Poisson distribution.\n\nWe could calculate λ(mean) for each genotype × treatment combination and provide a statistical summary of each group’s λ.\n\n```{r}\ngrp_means <- with(dat_tf, tapply(total.fruits, list(gna), mean))\nsummary(grp_means)\n```\n\nA core property of the Poisson distribution is that the variance is equal to the mean. A simple diagnostic is a plot of the group variances against the group means:\n\n- Poisson-distributed data will result in a linear pattern with slope = 1\n- as long as the variance is generally greater than the mean, we call the data overdispersed. Overdispersion comes in various forms:\n    - a linear mean-variance relationship with Var = φµ (a line through the origin) with φ > 1 is called a quasi-Poisson pattern (this term describes the mean-variance relationship, not any particular proability distribution); we can implement it statistically via quasilikelihood (Venables and Ripley, 2002) or by using a particular parameterization of the negative binomial distribution (“NB1” inthe terminology of Hardin and Hilbe (2007))\n    - a semi-quadratic pattern, Var = µ(1 + αµ) or µ(1 + µ/k), is characteristic of overdispersed data that is driven by underlying heterogeneity among samples, either the negative binomial (gamma-Poisson) or the lognormal-Poisson [@elston2001]\n\nWe’ve already calculated the group (genotype × treatment) means, we calculate the variances in the same way.\n\n```{r}\ngrp_vars <- with(\n  dat_tf,\n  tapply(\n    total.fruits,\n    list(gna), var\n  )\n)\n```\n\nWe can get approximate estimates of the quasi-Poisson (linear) and negative binomial (linear/quadratic) pattern using lm.\n\n```{r}\nlm1 <- lm(grp_vars ~ grp_means - 1) ## `quasi-Poisson' fit\nphi_fit <- coef(lm1)\nlm2 <- lm((grp_vars - grp_means) ~ I(grp_means^2) - 1)\nk_fit <- 1 / coef(lm2)\n```\n\nNow we can plot them.\n\n```{r}\n#| fig-cap: Graphical evaluation of distribution to use\nplot(grp_vars ~ grp_means, xlab = \"group means\", ylab = \"group variances\")\nabline(c(0, 1), lty = 2)\ntext(105, 500, \"Poisson\")\ncurve(phi_fit * x, col = 2, add = TRUE)\n## bquote() is used to substitute numeric values\n## in equations with symbols\ntext(110, 3900,\n  bquote(paste(\"QP: \", sigma^2 == .(round(phi_fit, 1)) * mu)),\n  col = 2\n)\ncurve(x * (1 + x / k_fit), col = 4, add = TRUE)\ntext(104, 7200, paste(\"NB: k=\", round(k_fit, 1), sep = \"\"), col = 4)\nl_fit <- loess(grp_vars ~ grp_means)\nmvec <- 0:120\nlines(mvec, predict(l_fit, mvec), col = 5)\ntext(100, 2500, \"loess\", col = 5)\n```\n\nSame with ggplot\n```{r}\n#| fig-cap: Graphical evaluation of distribution to use with ggplot\nggplot(\n  data.frame(grp_means, grp_vars),\n  aes(x = grp_means, y = grp_vars)) +\n  geom_point() +\n  geom_smooth(\n    aes(colour = \"Loess\"), se = FALSE) +\n  geom_smooth(\n    method = \"lm\", formula = y ~ x - 1, se = FALSE,\n    aes(colour = \"Q_Pois\")) +\n  stat_function(\n    fun = function(x) x * (1 + x / k_fit),\n    aes(colour = \"Neg_bin\")\n  ) +\n  geom_abline(\n    aes(intercept = 0, slope = 1, colour = \"Poisson\")) +\n  scale_colour_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\", \"red\")) +\n  scale_fill_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\", \"red\")) +\n  guides(fill = FALSE)\n```\n<!-- Todo need to edit the color legend -->\n\nThese fits are not rigorous statistical tests — they violate a variety of assumptions of linear regression (e.g. constant variance, independence), but they are good enough to give us an initial guess about what distributions we should use.\n\n**Exercise**\n\n- compare a simple quadratic fit to the data (i.e., without the linear part) with the negative binomial and quasipoisson fits\n<!-- -  Draw a plot to suggest whether one might be able to stabilize the variance of the data by log(1 + x)-transforming the data. -->\n\n::: {.callout-tip collapse='true'}\n\n# Solution\n\n```{r}\n#| fig-cap: Graphical evaluation of distribution to use including quadratic effect\nlm3 <- lm(grp_vars ~ I(grp_means)^2 - 1) ## quadratic fit\nquad_fit <- coef(lm3)\n\nggplot(\n  data.frame(grp_means, grp_vars),\n  aes(x = grp_means, y = grp_vars)) +\n  geom_point() +\n  geom_smooth(\n    method = \"lm\", formula = y ~ x - 1, se = FALSE,\n    aes(colour = \"Q_Pois\")) +\n  stat_function(\n    fun = function(x) x * (1 + x / k_fit),\n    aes(colour = \"Neg_bin\")\n  ) +\n  geom_smooth(\n    method = \"lm\", formula = y ~ I(x^2) - 1, se = FALSE,\n    aes(colour = \"Quad\")) +\n  scale_colour_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\")) +\n  scale_fill_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\")) +\n  guides(fill = FALSE)\n```\n\n:::\n\n#### Plotting the response vs treatments\n\nJust to avoid surprises\n\n```{r}\n#| fig-cap: Fruit production by treatments by population\nggplot(dat_tf, aes(x = amd, y = log(total.fruits + 1), colour = nutrient)) +\n  geom_point() +\n  ## need to use as.numeric(amd) to get lines\n  stat_summary(aes(x = as.numeric(amd)), fun = mean, geom = \"line\") +\n  theme_bw() +\n  theme(panel.spacing = unit(0, \"lines\")) +\n  facet_wrap(~popu)\n```\n\n```{r}\n#| fig-cap: Fruit production by genotype by treatments\nggplot(dat_tf, aes(x = amd, y = log(total.fruits + 1), colour = gen)) +\n  geom_point() +\n  stat_summary(aes(x = as.numeric(amd)), fun = mean, geom = \"line\") +\n  theme_bw() +\n  ## label_both adds variable name ('nutrient') to facet labels\n  facet_grid(. ~ nutrient, labeller = label_both)\n```\n\n\n### Fitting group-wise GLM\n\nAnother general starting approach is to fit GLMs to each group of data separately, equivalent to treating the grouping variables as fixed effects.\nThis should result in reasonable variation among treatment effects. We first fit the models, and then examine the coefficients.\n\n```{r}\n#| fig-cap: Model coefficients for GLM fits on each genotype\nglm_lis <- lmList(\n  total.fruits ~ nutrient * amd | gen,\n  data = dat_tf,\n  family = \"poisson\")\nplot.lmList(glm_lis)\n```\n\nThree genotypes (5, 6, 34) have extreme coefficients (Fig. 5). A mixed model assumes that the underlying random effects are normally distributed, although we shouldn’t take these outliers too seriously at this point — we are not actually plotting the random effects, or even estimates of random effects (which are not themselves guaranteed to be normally distributed), but rather separate estimates for each group.\nCreate a plotting function for Q-Q plots of these coefficients to visualize the departure from normality.\n\n```{r}\n#| fig-cap: Q-Q plots of model coefficients for GLM fits on each genotype\nqqmath.lmList(glm_lis)\n```\nWe see that these extreme coefficients fall far outside a normal error distribution. We shouldn’t take these outliers too seriously at this point — we are not actually plotting the random effects, or even estimates of random effects, but rather separate estimates for each group. \nEspecially if these groups have relatively small sample sizes, the estimates may eventually be “shrunk” closer to the mean when we do the mixed model.\nWe should nonetheless take care to see if the coefficients for these genotypes from the GLMM are still outliers, and take the same precautions as we usually do for outliers. For example, we can look back at the original data to see if there is something weird about the way those genotypes were collected, or try re-running the analysis without those genotypes to see if the results are robust.\n\n### Fitting and evaluating GLMMs\n\nNow we (try to) build and fit a full model, using `glmer` in the  `emoji::emoji(\"pacakage\")` `lme4`. This model has random effects for all genotype and population × treatment random effects, and for the nuisance variables for the rack and germination method (status). (Given the mean-variance relationship we saw it’s pretty clear that we are going to have to proceed eventually to a model with overdispersion, but we fit the Poisson model first for illustration.)\n\n```{r}\nmp1 <- glmer(total.fruits ~ nutrient * amd +\n  rack + status +\n  (amd * nutrient | popu) +\n  (amd * nutrient | gen),\ndata = dat_tf, family = \"poisson\"\n)\noverdisp_fun(mp1)\n```\n\n<!-- add description of the overdispersion function -->\n\nThe `overdisp_fun()` is described [here] https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-for-overdispersioncomputing-overdispersion-factor) on the absolutely fantastic FAQ about GLMMs by Ben Bolker https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\nWe can ignore the model convergence for the moment. This shows that the data are (extremely) over-dispersed, given the model.\n\nWe can also use the excellent `DHARMa` `r emoji::emoji(\"package\")` [@DHARMa] to evaluate fit of *glm* and *glmm*. So instead of using the function `overdisp_fun()`, we can simply use the function `testDispersion()`.\n\n```{r}\ntestDispersion(mp1)\n```\n\nAs you can see, DHARMa suggests that there is no overdispersion based on the distribution of residuals from simulated data. We are going to consider that we have overdispersion and adjust the model accordingly.\n\nNow we add the observation-level random effect to the model to account for overdispersion [@elston2001].\n\n```{r}\nmp2 <- update(mp1, . ~ . + (1 | X))\n```\n\nThe model takes much longer to fit (and gives warnings).\nWe look just at the variance components. In particular, if we look at the correlation matrix among the genotype random effects, we see a perfect\ncorrelation.\n\n```{r}\nattr(VarCorr(mp2)$gen, \"correlation\")\n```\n\nWe’ll try getting rid of the correlations between clipping (`amd`) and nutrients, using `amd+nutrient` instead of `amd*nutrient` in the random effects specification (here it seems easier to re-do the model rather than using update to add and subtract terms).\n\n```{r}\nmp3 <- glmer(total.fruits ~ nutrient * amd +\n  rack + status +\n  (amd + nutrient | popu) +\n  (amd + nutrient | gen) + (1 | X),\ndata = dat_tf, family = \"poisson\"\n)\n\nattr(VarCorr(mp3)$gen, \"correlation\")\nattr(VarCorr(mp3)$popu, \"correlation\")\n```\n\nUnfortunately, we still have perfect correlations among the random effects terms. For some models (e.g. random-slope models), it is possible to fit random effects models in such a way that the correlation between the different parameters (intercept and slope in the case of random-slope models) is constrained to be zero, by fitting a model like `(1|f)+(0+x|f)`; unfortunately, because of the way lme4 is set up, this is considerably more difficult with categorical predictors (factors).\n\nWe have to reduce the model further in some way in order not to overfit (i.e., in order to not have perfect ±1 correlations among random effects). It looks like we can’t allow both nutrients and clipping in the random effect model at either the population or the genotype level. However, it’s hard to know whether we should proceed with amd or nutrient, both, or neither in the model.\n\nA convenient way to proceed if we are going to try fitting several different combinations of random effects is to fit the model with all the fixed effects but only observation-level random effects, and then to use update to add various components to it.\n\n```{r}\nmp_obs <- glmer(total.fruits ~ nutrient * amd +\n  rack + status +\n  (1 | X),\ndata = dat_tf, family = \"poisson\"\n)\n```\n\nNow, for example, `update(mp_obs,.~.+(1|gen)+(amd|popu))` fits the model with intercept random effects at the genotype level and variation in clipping effects across populations.\n\n::: {.callout-caution}\n# Exercise\n**Exercise** using update, fit the models with \n\n1. clipping variation at both genotype and population levels;\n2. nutrient variation at both genotype and populations; convince yourself that trying to fit variation in either clipping or nutrients leads to overfitting (perfect correlations).\n3. Fit the model with only intercept variation at the population and genotype levels, saving it as mp4; show that there is non-zero variance estimated\n:::\n\n::: {.callout-tip collapse='true'}\n\n# Solution\n\n1. \n```{r}\nmpcli <- update(mp_obs, . ~ . + (amd | gen) + (amd | popu))\nVarCorr(mpcli)\n```\n\n2. \n```{r}\nmpnut <- update(mp_obs, . ~ . + (nutrient | gen) + (nutrient | popu))\nVarCorr(mpnut)\n```\n\n3. \n```{r}\nmp4 <- update(mp_obs, . ~ . + (1 | gen) + (1 | popu))\nVarCorr(mp4)\n```\n\n:::\n\nIn other words, while it’s biologically plausible that there is some variation in the nutrient or clipping effect at the genotype or population levels, with this modeling approach we really don’t have enough data to speak confidently about these effects.\nLet’s check that mp4 no longer incorporates overdispersion (the observationlevel random effect should have taken care of it):\n\n```{r}\noverdisp_fun(mp4)\n```\n\n\nUsing the `DHARMa` `r emoji::emoji(\"package\")`, we will also check the model. To do so we first need to simulate some data and get the *scaled residuals* following the DHARMa notation.\nThen we can check the distributional properties of the *scaled residuals* and see if they follow the classic assumption using the different functions provided.\n\n```{r}\nscaled_res <- simulateResiduals(mp4)\nplot(scaled_res)\ntestZeroInflation(mp4, plot = TRUE)\n```\n\n```{r}\n# note about overdispersion\nsum(dat_tf$total.fruits == 0)\na <- predict(mp4, type = \"response\")\nb <- rep(0, 500)\nfor (j in 1:500) {\n  b[j] <- sum(sapply(seq(nrow(dat_tf)), function(i) rpois(1, a[i])) == 0)\n}\nhist(b)\n```\n\n\n### Inference\n\n\n#### Random effects\n\n`glmer` (`lmer`) does not return information about the standard errors or confidence intervals of the variance components.\n```{r}\nVarCorr(mp4)\n```\n\n##### Testing for random Effects\n\nIf we want to test the significance of the random effects we can fit reduced models and run likelihood ratio tests via anova, keeping in mind that in this case (testing a null hypothesis of zero variance, where the parameter is on the boundary of its feasible region) the reported p value is approximately twice what it should be.\n\n```{r}\nmp4v1 <- update(mp_obs, . ~ . + (1 | popu)) ## popu only (drop gen)\nmp4v2 <- update(mp_obs, . ~ . + (1 | gen)) ## gen only (drop popu)\nanova(mp4, mp4v1)\nanova(mp4, mp4v2)\n```\n\nFor various forms of linear mixed models, the RLRsim package can do efficient simulation-based hypothesis testing of variance components — un- fortunately, that doesn’t include GLMMs.\nIf we are sufficiently patient we can do hypothesis testing via brute-force parametric bootstrapping where we repeatedly simulate data from the reduced (null) model, fit both the re- duced and full models to the simulated data, and compute the distribution of the deviance (change in -2 log likelihood).\nThe code below took about half an hour on a reasonably modern desktop computer.\n\n```{r}\n#| label: simdev_glmm\n#| eval: false\n#| echo: true\nsimdev <- function() {\n  newdat <- simulate(mp4v1)\n  reduced <- lme4::refit(mp4v1, newdat)\n  full <- lme4::refit(mp4, newdat)\n  2 * (c(logLik(full) - logLik(reduced)))\n}\n\nset.seed(101)\nnulldist0 <- replicate(2, simdev())\n## zero spurious (small) negative values\nnulldist[nulldist < 0 & abs(nulldist) < 1e-5] <- 0\nobsdev <- 2 * c(logLik(mp4) - logLik(mp4v1))\n```\n\n```{r}\n#| label: simdev_glmm_saved\n#| eval: !expr params$longrun\n#| echo: false\n#| include: false\n#| purl: false\nsimdev <- function() {\n  newdat <- simulate(mp4v1)\n  reduced <- lme4::refit(mp4v1, newdat)\n  full <- lme4::refit(mp4, newdat)\n  2 * (c(logLik(full) - logLik(reduced)))\n}\n\nset.seed(101)\nnulldist <- raply(200, simdev(), .progress = \"text\")\nsave(nulldist, file = \"data/r_obj/glmm_simdev.rda\")\nnulldist[nulldist < 0 & abs(nulldist) < 1e-5] <- 0\nobsdev <- 2 * c(logLik(mp4) - logLik(mp4v1))\n```\n```{r}\n#| echo: false\n#| include: false\n#| purl: false\n#| eval: !expr '!params$longrun'\nload(\"data/r_obj/glmm_simdev.rda\")\nnulldist[nulldist < 0 & abs(nulldist) < 1e-5] <- 0\nobsdev <- 2 * c(logLik(mp4) - logLik(mp4v1))\n```\n\n```{r}\n#| eval: true\nmean(c(nulldist, obsdev) >= obsdev)\n```\n\nThe true p-value is actually closer to 0.05 than 0.02. In other words, here the deviations from the original statistical model from that for which the original “p value is inflated by 2” rule of thumb was derived — fitting a GLMM instead of a LMM, and using a moderate-sized rather than an arbitrarily large (asymptotic) data set — have made the likelihood ratio test liberal (increased type I error) rather than conservative (decreased type I error).\n\nWe can also inspect the random effects estimates themselves (in proper statistical jargon, these might be considered “predictions” rather than “estimates” (Robinson, 1991)). We use the built-in dotplot method for the random effects extracted from glmer fits (i.e. ranef(model,condVar=TRUE)), which returns a list of plots, one for each random effect level in the model.\n\n```{r}\n#| fig-cap: Distribution of BLUPs for genotypes and populations\nr1 <- as.data.frame(ranef(mp4, condVar = TRUE, whichel = c(\"gen\", \"popu\")))\np1 <- ggplot(subset(r1, grpvar == \"gen\"), aes(y = grp, x = condval)) +\n  geom_point() +\n  geom_pointrange(\n    aes(xmin = condval - condsd * 1.96, xmax = condval + condsd * 1.96)\n  ) +\n  geom_vline(aes(xintercept = 0, color = \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\np2 <- ggplot(subset(r1, grpvar == \"popu\"), aes(y = grp, x = condval)) +\n  geom_point() +\n  geom_pointrange(\n    aes(xmin = condval - condsd * 1.96, xmax = condval + condsd * 1.96)\n  ) +\n  geom_vline(aes(xintercept = 0, color = \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\np1 + p2\n```\n\nAs expected from the similarity of the variance estimates, the population-level estimates (the only shared component) do not differ much between the two models. There is a hint of regional differentiation — the Spanish populations have higher fruit sets than the Swedish and Dutch populations. Genotype 34 again looks a little bit unusual.\n\n#### Fixed effects\n\n Now we want to do inference on the fixed effects. We use the drop1 func- tion to assess both the AIC difference and the likelihood ratio test between models. (In glmm_funs.R we define a convenience function dfun to convert the AIC tables returned by drop1 (which we will create momentarily) into ∆AIC tables.) Although the likelihood ratio test (and the AIC) are asymptotic tests, comparing fits between full and reduced models is still more accurate than the Wald (curvature-based) tests shown in the summary tables for glmer fits.\n\n```{r}\n(dd_aic <- dfun(drop1(mp4)))\n(dd_lrt <- drop1(mp4, test = \"Chisq\"))\n```\n\nOn the basis of these comparisons, there appears to be a very strong effect of rack and weak effects of status and of the interaction term. Dropping the nutrient:amd interaction gives a (slightly) increased AIC (∆AIC = 1.4), so the full model has the best expected predictive capability (by a small margin). On the other hand, the p-value is slightly above 0.05 (p = 0.06). At this point we remove the non-significant interaction term so we can test the main effects. (We don’t worry about removing status because it measures an aspect of experimental design that we want to leave in the model whether it is significant or not.) Once we have fitted the reduced model, we can run the LRT via anova.\n\n```{r}\nmp5 <- update(mp4, . ~ . - amd:nutrient)\nanova(mp5, mp4)\n```\n\n**Exercise**\nTest now the reduced model.\n\nIn the reduced model, we find that both nutrients and clipping have strong effects, whether measured by AIC or LRT. If we wanted to be still more careful about our interpretation, we would try to relax the asymptotic assumption. In classical linear models, we would do this by doing F tests with the appropriate denominator degrees of freedom. In “modern” mixed model approaches, we might try to use denominator-degree-of-freedom approximations such as the Kenward-Roger (despite the controversy over these approximations, they are actually available in `lmerTest`, but they do not apply to GLMMs. We can use a parametric bootstrap comparison between nested models to test fixed effects, as we did above for random effects, with the caveat that is computationally slow.\n\nIn addition, we can check the normality of the random effects and find they are reasonable (Fig. 10). \n\n```{r}\n#| fig-cap: Q-Q plot of BLUPs from model mp5\nr5 <- as.data.frame(ranef(mp5))\nggplot(data = r5, aes(sample = condval)) +\n  geom_qq() + geom_qq_line() +\n  facet_wrap(~ grpvar) +\n  theme_classic()\n```\n\nChecking everything with DHARMa also\n\n\n```{r}\nscaled_res <- simulateResiduals(mp5)\nplot(scaled_res)\ntestZeroInflation(mp5, plot = TRUE)\n```\n\nIt is better than before but not perfect. I think this is completely OK and that it will extremely rarely be perfect. You need to learn what is acceptable (by that I mean you find acceptable) and be happy to justify and discuss your decisions.\n\n### Conclusions\n Our final model includes fixed effects of nutrients and clipping, as well as the nuisance variables rack and status; observation-level random effects to ac- count for overdispersion; and variation in overall fruit set at the population and genotype levels. However, we don’t (apparently) have quite enough in- formation to estimate the variation in clipping and nutrient effects, or their interaction, at the genotype or population levels. There is a strong overall positive effect of nutrients and a slightly weaker negative effect of clipping. The interaction between clipping and nutrients is only weakly supported (i.e. the p-value is not very small), but it is positive and about the same magnitude as the clipping effect, which is consistent with the statement that “nutrients cancel out the effect of herbivory”.\n\n\n::: {.callout-caution}\n# Exercise\n**Exercise**\n\n- Re-do the analysis with region as a fixed effect.\n- Re-do the analysis with a one-way layout as suggested above\n:::\n\n### Happy generalized mixed-modelling\n\n```{r}\n#| echo: false\n#| out-width: 50%\n#| fig-align: center\n#| fig-cap: A GLMM character\nknitr::include_graphics(\"images/Thorn.png\")\n```\n","srcMarkdownNoYaml":"\n\n# Introduction to `GLMM`\n\n## Lecture\n\ntheoretical intro to glmm and introduce DHarma package to evaluate fit of glmm\n\n```{r}\n#| echo: false\n#| out-width: 50%\n#| fig-align: center\n#| fig-cap: Dream pet dragon\nknitr::include_graphics(\"images/fun_dragon.jpg\")\n```\n\n\n\n## Practical\n\nThis is an adapted version largely inspired by the tutorial in [@bolker_generalized_2009].\nSpatial variation in nutrient availability and herbivory is likely to cause population differentiation and maintain genetic diversity in plant populations.Here we measure the extent to which mouse-ear cress (Arabidopsis thaliana)exhibits population and genotypic variation in their responses to these im-portant environmental factors. We are particularly interested in whether these populations exhibit nutrient mediated compensation, where higher nutrient levels allow genotypes to better tolerate herbivory [@banta_comprehensive_2010]. We use GLMMs to estimate the effect of nutrient levels, simulated herbivory, and their interaction on fruit production in Arabidopsis thaliana(fixed effects), and the extent to which populations vary in their responses(random effects, or variance components)\n\n### Packages and functions\n\nYou need to download the \"extra_funs.R\" script for some functions used in the Practical\n\n```{r}\n#| eval: true\n#| warning: false\n#| message: false\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(lattice)\nlibrary(DHARMa)\nsource(\"data/extra_funs.R\")\n```\n\n### The data set\n\nIn this data set, the response variable is the number of fruits (i.e. seed capsules) per plant. The number of fruits produced by an individual plant(the experimental unit) was hypothesized to be a function of fixed effects,including nutrient levels (low vs. high), simulated herbivory (none vs. apical meristem damage), region (Sweden, Netherlands, Spain), and interactions among these. Fruit number was also a function of random effects including both the population and individual genotype. Because Arabidopsis is highly selfing, seeds of a single individual served as replicates of that individual.There were also nuisance variables, including the placement of the plant in the greenhouse, and the method used to germinate seeds. These were estimated as fixed effects but interactions were excluded.\n\n- `X` observation number (we will use this observation number later, when we are accounting for overdispersion)\n- `reg` a factor for region (Netherlands, Spain, Sweden).\n- `popu` a factor with a level for each population.\n- `gen` a factor with a level for each genotype.\n- `rack` a nuisance factor for one of two greenhouse racks.\n- `nutrient` a factor with levels for minimal or additional nutrients.\n- `amd` a factor with levels for no damage or simulated herbivory (apical meristem damage; we will sometimes refer to this as “clipping”)\n- `status` a nuisance factor for germination method.\n- `total.fruits` the response; an integer count of the number of fruits per plant.\n\n### Specifying fixed and random Effects\n\nHere we need to select a realistic full model, based on the scientific questions and the data actually at hand. We first load the data set and make sure that each variable is appropriately designated as numeric or factor (i.e.categorical variable).\n\n```{r}\ndat_tf <- read.csv(\"data/Banta_TotalFruits.csv\")\nstr(dat_tf)\n```\n\nThe `X`, `gen`, `rack` and `nutrient` variables are coded as integers, but we want them to be factors.\n We use `mutate()` `dplyr` `r emoji::emoji(\"package\")`, which operates within the data set, to avoid typing lots of commands like `dat_tf$rack <- factor(dat_tf$rack)`\n At the same time, we reorder the `clipping` variable so that `\"unclipped\"` is the reference level (we could also have used `relevel(amd,\"unclipped\")`).\n\n```{r}\ndat_tf <- mutate(\n  dat_tf,\n  X = factor(X),\n  gen = factor(gen),\n  rack = factor(rack),\n  amd = factor(amd, levels = c(\"unclipped\", \"clipped\")),\n  nutrient = factor(nutrient, label = c(\"Low\", \"High\"))\n)\n```\n\nNow we check replication for each genotype (columns) within each population (rows).\n\n```{r}\n(reptab <- with(dat_tf, table(popu, gen)))\n```\n\n::: {.callout-caution}\n# Exercise\n**Exercise**: this mode of inspection is OK for this data set but might fail for much larger data sets or for more levels of nesting. See if you can think of some other numerical or graphical methods for inspecting the structure of data sets. \n\n1. plot(reptab) gives a mosaic plot of the two-way table; examine this, see if you can figure out how to interpret it, and decide whether you think it might be useful\n2. try the commands colSums(reptab>0) (and the equivalent for rowSums) and figure out what they are telling you.\n3. Using this recipe, how would you compute the range of number of genotypes per treatment combination?\n:::\n\n::: {.callout-tip collapse='true'}\n\n# Solution\n\n1. Do you find the mosaic plot you obtained ugly and super hard to read? Me too `r emoji::emoji(\"laugh\")`\n```{r}\n#| fig-cap: A truly useless plot no one can understand\nplot(reptab)\n```\n\n2. `colSums()` do the sum of all the rows for each columns of a table. So `colSums(reptab>0)` gives you for each genotype the number of populations (lines) where you have at least 1 observations.\n\n```{r}\ncolSums(reptab > 0)\nrowSums(reptab > 0)\n```\n\n3. You firts need to create a new table of number of observations per treatment and genotypes\n```{r}\nreptab2 <- with(dat_tf, table(paste(amd, nutrient, sep = \"_\"), gen))\nrange(reptab2)\n```\n\n:::\n\nThis reveals that we have only 2–4 populations per region and 2–3 genotypes per population. However, we also have 2–13 replicates per genotype for each treatment combination (four unique treatment combinations: 2 levels of nutrients by 2 levels of simulated herbivory). Thus, even though this was a reasonably large experiment (625 plants), there were a very small number of replicates with which to estimate variance components, and many more potential interactions than our data can support. Therefore, judicious selection of model terms, based on both biology and the data, is warranted. We note that we don’t really have enough levels per random effect, nor enough replication per unique treatment combination. Therefore, we decide to omit the fixed effect of “region”, although we recognize that populations in different regions are widely geographically separated.\n\nHowever, as in all GLMMs where the scale parameter is treated as fixed and deviations from the fixed scale parameter would be identifiable (i.e. Poisson and binomial (N > 1), but not binary, models) we may have to deal with overdispersion.\n\n\n### Look at overall patterns in data\n\n\nI usually like to start with a relatively simple overall plot of the data, disregarding the random factors, just to see what’s going on. For reasons to be discussed below, we choose to look at the data on the log (or log(1 + x) scale. Let’s plot either box-and-whisker plots (useful summaries) or dot plots (more detailed, good for seeing if we missed anything).\n\n```{r}\n#| echo: false\n#| purl: false\n#| fig-cap: Number of fruits (log + 1) as a function of treatments\np1 <- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf, geom = \"boxplot\") +\n  facet_wrap(~reg, nrow = 1) +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Boxplot\")\np2 <- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf) +\n  facet_wrap(~reg, nrow = 1) +\n  stat_sum() +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Dot plot\")\np1 + p2\n```\n\n::: {.callout-caution}\n# Exercise\nGenerate these plots and figure out how they work before continuing. Try conditioning/faceting on population rather than region: for facet_wrap you might want to take out the nrow = 1 specification. If you want try reorder the subplots by overall mean fruit set and/or colour the points according to the region they come from.\n:::\n\n::: {.callout-tip collapse='true'}\n\n# Solution\n\n```{r}\n#| eval: false\n#| fig-cap: Number of fruits (log + 1) as a function of treatments\np1 <- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf, geom = \"boxplot\") +\n  facet_wrap(~reg, nrow = 1) +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Boxplot\")\np2 <- qplot(\n    interaction(nutrient, amd),\n    log(1 + total.fruits),\n    data = dat_tf) +\n  facet_wrap(~reg, nrow = 1) +\n  stat_sum() +\n  theme(axis.text.x = element_text(angle = 45)) +\n  ggtitle(\"Dot plot\")\np1 + p2\n```\n\n:::\n\n\n### Choose an error distribution\n\nThe data are non-normal in principle (i.e., count data, so our first guess would be a Poisson distribution). If we transform total fruits with the canonical link function (log), we hope to see relatively homogeneous variances across categories and groups.\n\nFirst we define a new factor that represents every combination of genotype and treatment (nutrient × clipping) treatment, and sort it in order of increasing mean fruit set.\n\n```{r}\ndat_tf <- dat_tf %>%\n  mutate(\n    gna = reorder(interaction(gen, nutrient, amd), total.fruits, mean)\n  )\n```\n\nNow time to plot it\n\n```{r}\n#| fig-cap: Boxplot of total fruits (log + 1) per genotypes and treatments\nggplot(dat_tf, aes(x = gna, y = log(1 + total.fruits))) +\n  geom_boxplot() +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle = 90))\n```\n\nWe could also calculate the variance for each genotype × treatment combination and provide a statistical summary of these variances.\nThis reveals substantial variation among the sample variances on the transformed data. In addition to heterogeneous variances across groups, Figure 1 reveals many zeroes in groups, and some groups with a mean and variance of zero, further suggesting we need a non-normal error distribution, and perhaps something other than a Poisson distribution.\n\nWe could calculate λ(mean) for each genotype × treatment combination and provide a statistical summary of each group’s λ.\n\n```{r}\ngrp_means <- with(dat_tf, tapply(total.fruits, list(gna), mean))\nsummary(grp_means)\n```\n\nA core property of the Poisson distribution is that the variance is equal to the mean. A simple diagnostic is a plot of the group variances against the group means:\n\n- Poisson-distributed data will result in a linear pattern with slope = 1\n- as long as the variance is generally greater than the mean, we call the data overdispersed. Overdispersion comes in various forms:\n    - a linear mean-variance relationship with Var = φµ (a line through the origin) with φ > 1 is called a quasi-Poisson pattern (this term describes the mean-variance relationship, not any particular proability distribution); we can implement it statistically via quasilikelihood (Venables and Ripley, 2002) or by using a particular parameterization of the negative binomial distribution (“NB1” inthe terminology of Hardin and Hilbe (2007))\n    - a semi-quadratic pattern, Var = µ(1 + αµ) or µ(1 + µ/k), is characteristic of overdispersed data that is driven by underlying heterogeneity among samples, either the negative binomial (gamma-Poisson) or the lognormal-Poisson [@elston2001]\n\nWe’ve already calculated the group (genotype × treatment) means, we calculate the variances in the same way.\n\n```{r}\ngrp_vars <- with(\n  dat_tf,\n  tapply(\n    total.fruits,\n    list(gna), var\n  )\n)\n```\n\nWe can get approximate estimates of the quasi-Poisson (linear) and negative binomial (linear/quadratic) pattern using lm.\n\n```{r}\nlm1 <- lm(grp_vars ~ grp_means - 1) ## `quasi-Poisson' fit\nphi_fit <- coef(lm1)\nlm2 <- lm((grp_vars - grp_means) ~ I(grp_means^2) - 1)\nk_fit <- 1 / coef(lm2)\n```\n\nNow we can plot them.\n\n```{r}\n#| fig-cap: Graphical evaluation of distribution to use\nplot(grp_vars ~ grp_means, xlab = \"group means\", ylab = \"group variances\")\nabline(c(0, 1), lty = 2)\ntext(105, 500, \"Poisson\")\ncurve(phi_fit * x, col = 2, add = TRUE)\n## bquote() is used to substitute numeric values\n## in equations with symbols\ntext(110, 3900,\n  bquote(paste(\"QP: \", sigma^2 == .(round(phi_fit, 1)) * mu)),\n  col = 2\n)\ncurve(x * (1 + x / k_fit), col = 4, add = TRUE)\ntext(104, 7200, paste(\"NB: k=\", round(k_fit, 1), sep = \"\"), col = 4)\nl_fit <- loess(grp_vars ~ grp_means)\nmvec <- 0:120\nlines(mvec, predict(l_fit, mvec), col = 5)\ntext(100, 2500, \"loess\", col = 5)\n```\n\nSame with ggplot\n```{r}\n#| fig-cap: Graphical evaluation of distribution to use with ggplot\nggplot(\n  data.frame(grp_means, grp_vars),\n  aes(x = grp_means, y = grp_vars)) +\n  geom_point() +\n  geom_smooth(\n    aes(colour = \"Loess\"), se = FALSE) +\n  geom_smooth(\n    method = \"lm\", formula = y ~ x - 1, se = FALSE,\n    aes(colour = \"Q_Pois\")) +\n  stat_function(\n    fun = function(x) x * (1 + x / k_fit),\n    aes(colour = \"Neg_bin\")\n  ) +\n  geom_abline(\n    aes(intercept = 0, slope = 1, colour = \"Poisson\")) +\n  scale_colour_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\", \"red\")) +\n  scale_fill_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\", \"red\")) +\n  guides(fill = FALSE)\n```\n<!-- Todo need to edit the color legend -->\n\nThese fits are not rigorous statistical tests — they violate a variety of assumptions of linear regression (e.g. constant variance, independence), but they are good enough to give us an initial guess about what distributions we should use.\n\n**Exercise**\n\n- compare a simple quadratic fit to the data (i.e., without the linear part) with the negative binomial and quasipoisson fits\n<!-- -  Draw a plot to suggest whether one might be able to stabilize the variance of the data by log(1 + x)-transforming the data. -->\n\n::: {.callout-tip collapse='true'}\n\n# Solution\n\n```{r}\n#| fig-cap: Graphical evaluation of distribution to use including quadratic effect\nlm3 <- lm(grp_vars ~ I(grp_means)^2 - 1) ## quadratic fit\nquad_fit <- coef(lm3)\n\nggplot(\n  data.frame(grp_means, grp_vars),\n  aes(x = grp_means, y = grp_vars)) +\n  geom_point() +\n  geom_smooth(\n    method = \"lm\", formula = y ~ x - 1, se = FALSE,\n    aes(colour = \"Q_Pois\")) +\n  stat_function(\n    fun = function(x) x * (1 + x / k_fit),\n    aes(colour = \"Neg_bin\")\n  ) +\n  geom_smooth(\n    method = \"lm\", formula = y ~ I(x^2) - 1, se = FALSE,\n    aes(colour = \"Quad\")) +\n  scale_colour_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\")) +\n  scale_fill_manual(\n    name = \"legend\",\n    values = c(\"blue\", \"purple\", \"black\")) +\n  guides(fill = FALSE)\n```\n\n:::\n\n#### Plotting the response vs treatments\n\nJust to avoid surprises\n\n```{r}\n#| fig-cap: Fruit production by treatments by population\nggplot(dat_tf, aes(x = amd, y = log(total.fruits + 1), colour = nutrient)) +\n  geom_point() +\n  ## need to use as.numeric(amd) to get lines\n  stat_summary(aes(x = as.numeric(amd)), fun = mean, geom = \"line\") +\n  theme_bw() +\n  theme(panel.spacing = unit(0, \"lines\")) +\n  facet_wrap(~popu)\n```\n\n```{r}\n#| fig-cap: Fruit production by genotype by treatments\nggplot(dat_tf, aes(x = amd, y = log(total.fruits + 1), colour = gen)) +\n  geom_point() +\n  stat_summary(aes(x = as.numeric(amd)), fun = mean, geom = \"line\") +\n  theme_bw() +\n  ## label_both adds variable name ('nutrient') to facet labels\n  facet_grid(. ~ nutrient, labeller = label_both)\n```\n\n\n### Fitting group-wise GLM\n\nAnother general starting approach is to fit GLMs to each group of data separately, equivalent to treating the grouping variables as fixed effects.\nThis should result in reasonable variation among treatment effects. We first fit the models, and then examine the coefficients.\n\n```{r}\n#| fig-cap: Model coefficients for GLM fits on each genotype\nglm_lis <- lmList(\n  total.fruits ~ nutrient * amd | gen,\n  data = dat_tf,\n  family = \"poisson\")\nplot.lmList(glm_lis)\n```\n\nThree genotypes (5, 6, 34) have extreme coefficients (Fig. 5). A mixed model assumes that the underlying random effects are normally distributed, although we shouldn’t take these outliers too seriously at this point — we are not actually plotting the random effects, or even estimates of random effects (which are not themselves guaranteed to be normally distributed), but rather separate estimates for each group.\nCreate a plotting function for Q-Q plots of these coefficients to visualize the departure from normality.\n\n```{r}\n#| fig-cap: Q-Q plots of model coefficients for GLM fits on each genotype\nqqmath.lmList(glm_lis)\n```\nWe see that these extreme coefficients fall far outside a normal error distribution. We shouldn’t take these outliers too seriously at this point — we are not actually plotting the random effects, or even estimates of random effects, but rather separate estimates for each group. \nEspecially if these groups have relatively small sample sizes, the estimates may eventually be “shrunk” closer to the mean when we do the mixed model.\nWe should nonetheless take care to see if the coefficients for these genotypes from the GLMM are still outliers, and take the same precautions as we usually do for outliers. For example, we can look back at the original data to see if there is something weird about the way those genotypes were collected, or try re-running the analysis without those genotypes to see if the results are robust.\n\n### Fitting and evaluating GLMMs\n\nNow we (try to) build and fit a full model, using `glmer` in the  `emoji::emoji(\"pacakage\")` `lme4`. This model has random effects for all genotype and population × treatment random effects, and for the nuisance variables for the rack and germination method (status). (Given the mean-variance relationship we saw it’s pretty clear that we are going to have to proceed eventually to a model with overdispersion, but we fit the Poisson model first for illustration.)\n\n```{r}\nmp1 <- glmer(total.fruits ~ nutrient * amd +\n  rack + status +\n  (amd * nutrient | popu) +\n  (amd * nutrient | gen),\ndata = dat_tf, family = \"poisson\"\n)\noverdisp_fun(mp1)\n```\n\n<!-- add description of the overdispersion function -->\n\nThe `overdisp_fun()` is described [here] https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#testing-for-overdispersioncomputing-overdispersion-factor) on the absolutely fantastic FAQ about GLMMs by Ben Bolker https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html\n\nWe can ignore the model convergence for the moment. This shows that the data are (extremely) over-dispersed, given the model.\n\nWe can also use the excellent `DHARMa` `r emoji::emoji(\"package\")` [@DHARMa] to evaluate fit of *glm* and *glmm*. So instead of using the function `overdisp_fun()`, we can simply use the function `testDispersion()`.\n\n```{r}\ntestDispersion(mp1)\n```\n\nAs you can see, DHARMa suggests that there is no overdispersion based on the distribution of residuals from simulated data. We are going to consider that we have overdispersion and adjust the model accordingly.\n\nNow we add the observation-level random effect to the model to account for overdispersion [@elston2001].\n\n```{r}\nmp2 <- update(mp1, . ~ . + (1 | X))\n```\n\nThe model takes much longer to fit (and gives warnings).\nWe look just at the variance components. In particular, if we look at the correlation matrix among the genotype random effects, we see a perfect\ncorrelation.\n\n```{r}\nattr(VarCorr(mp2)$gen, \"correlation\")\n```\n\nWe’ll try getting rid of the correlations between clipping (`amd`) and nutrients, using `amd+nutrient` instead of `amd*nutrient` in the random effects specification (here it seems easier to re-do the model rather than using update to add and subtract terms).\n\n```{r}\nmp3 <- glmer(total.fruits ~ nutrient * amd +\n  rack + status +\n  (amd + nutrient | popu) +\n  (amd + nutrient | gen) + (1 | X),\ndata = dat_tf, family = \"poisson\"\n)\n\nattr(VarCorr(mp3)$gen, \"correlation\")\nattr(VarCorr(mp3)$popu, \"correlation\")\n```\n\nUnfortunately, we still have perfect correlations among the random effects terms. For some models (e.g. random-slope models), it is possible to fit random effects models in such a way that the correlation between the different parameters (intercept and slope in the case of random-slope models) is constrained to be zero, by fitting a model like `(1|f)+(0+x|f)`; unfortunately, because of the way lme4 is set up, this is considerably more difficult with categorical predictors (factors).\n\nWe have to reduce the model further in some way in order not to overfit (i.e., in order to not have perfect ±1 correlations among random effects). It looks like we can’t allow both nutrients and clipping in the random effect model at either the population or the genotype level. However, it’s hard to know whether we should proceed with amd or nutrient, both, or neither in the model.\n\nA convenient way to proceed if we are going to try fitting several different combinations of random effects is to fit the model with all the fixed effects but only observation-level random effects, and then to use update to add various components to it.\n\n```{r}\nmp_obs <- glmer(total.fruits ~ nutrient * amd +\n  rack + status +\n  (1 | X),\ndata = dat_tf, family = \"poisson\"\n)\n```\n\nNow, for example, `update(mp_obs,.~.+(1|gen)+(amd|popu))` fits the model with intercept random effects at the genotype level and variation in clipping effects across populations.\n\n::: {.callout-caution}\n# Exercise\n**Exercise** using update, fit the models with \n\n1. clipping variation at both genotype and population levels;\n2. nutrient variation at both genotype and populations; convince yourself that trying to fit variation in either clipping or nutrients leads to overfitting (perfect correlations).\n3. Fit the model with only intercept variation at the population and genotype levels, saving it as mp4; show that there is non-zero variance estimated\n:::\n\n::: {.callout-tip collapse='true'}\n\n# Solution\n\n1. \n```{r}\nmpcli <- update(mp_obs, . ~ . + (amd | gen) + (amd | popu))\nVarCorr(mpcli)\n```\n\n2. \n```{r}\nmpnut <- update(mp_obs, . ~ . + (nutrient | gen) + (nutrient | popu))\nVarCorr(mpnut)\n```\n\n3. \n```{r}\nmp4 <- update(mp_obs, . ~ . + (1 | gen) + (1 | popu))\nVarCorr(mp4)\n```\n\n:::\n\nIn other words, while it’s biologically plausible that there is some variation in the nutrient or clipping effect at the genotype or population levels, with this modeling approach we really don’t have enough data to speak confidently about these effects.\nLet’s check that mp4 no longer incorporates overdispersion (the observationlevel random effect should have taken care of it):\n\n```{r}\noverdisp_fun(mp4)\n```\n\n\nUsing the `DHARMa` `r emoji::emoji(\"package\")`, we will also check the model. To do so we first need to simulate some data and get the *scaled residuals* following the DHARMa notation.\nThen we can check the distributional properties of the *scaled residuals* and see if they follow the classic assumption using the different functions provided.\n\n```{r}\nscaled_res <- simulateResiduals(mp4)\nplot(scaled_res)\ntestZeroInflation(mp4, plot = TRUE)\n```\n\n```{r}\n# note about overdispersion\nsum(dat_tf$total.fruits == 0)\na <- predict(mp4, type = \"response\")\nb <- rep(0, 500)\nfor (j in 1:500) {\n  b[j] <- sum(sapply(seq(nrow(dat_tf)), function(i) rpois(1, a[i])) == 0)\n}\nhist(b)\n```\n\n\n### Inference\n\n\n#### Random effects\n\n`glmer` (`lmer`) does not return information about the standard errors or confidence intervals of the variance components.\n```{r}\nVarCorr(mp4)\n```\n\n##### Testing for random Effects\n\nIf we want to test the significance of the random effects we can fit reduced models and run likelihood ratio tests via anova, keeping in mind that in this case (testing a null hypothesis of zero variance, where the parameter is on the boundary of its feasible region) the reported p value is approximately twice what it should be.\n\n```{r}\nmp4v1 <- update(mp_obs, . ~ . + (1 | popu)) ## popu only (drop gen)\nmp4v2 <- update(mp_obs, . ~ . + (1 | gen)) ## gen only (drop popu)\nanova(mp4, mp4v1)\nanova(mp4, mp4v2)\n```\n\nFor various forms of linear mixed models, the RLRsim package can do efficient simulation-based hypothesis testing of variance components — un- fortunately, that doesn’t include GLMMs.\nIf we are sufficiently patient we can do hypothesis testing via brute-force parametric bootstrapping where we repeatedly simulate data from the reduced (null) model, fit both the re- duced and full models to the simulated data, and compute the distribution of the deviance (change in -2 log likelihood).\nThe code below took about half an hour on a reasonably modern desktop computer.\n\n```{r}\n#| label: simdev_glmm\n#| eval: false\n#| echo: true\nsimdev <- function() {\n  newdat <- simulate(mp4v1)\n  reduced <- lme4::refit(mp4v1, newdat)\n  full <- lme4::refit(mp4, newdat)\n  2 * (c(logLik(full) - logLik(reduced)))\n}\n\nset.seed(101)\nnulldist0 <- replicate(2, simdev())\n## zero spurious (small) negative values\nnulldist[nulldist < 0 & abs(nulldist) < 1e-5] <- 0\nobsdev <- 2 * c(logLik(mp4) - logLik(mp4v1))\n```\n\n```{r}\n#| label: simdev_glmm_saved\n#| eval: !expr params$longrun\n#| echo: false\n#| include: false\n#| purl: false\nsimdev <- function() {\n  newdat <- simulate(mp4v1)\n  reduced <- lme4::refit(mp4v1, newdat)\n  full <- lme4::refit(mp4, newdat)\n  2 * (c(logLik(full) - logLik(reduced)))\n}\n\nset.seed(101)\nnulldist <- raply(200, simdev(), .progress = \"text\")\nsave(nulldist, file = \"data/r_obj/glmm_simdev.rda\")\nnulldist[nulldist < 0 & abs(nulldist) < 1e-5] <- 0\nobsdev <- 2 * c(logLik(mp4) - logLik(mp4v1))\n```\n```{r}\n#| echo: false\n#| include: false\n#| purl: false\n#| eval: !expr '!params$longrun'\nload(\"data/r_obj/glmm_simdev.rda\")\nnulldist[nulldist < 0 & abs(nulldist) < 1e-5] <- 0\nobsdev <- 2 * c(logLik(mp4) - logLik(mp4v1))\n```\n\n```{r}\n#| eval: true\nmean(c(nulldist, obsdev) >= obsdev)\n```\n\nThe true p-value is actually closer to 0.05 than 0.02. In other words, here the deviations from the original statistical model from that for which the original “p value is inflated by 2” rule of thumb was derived — fitting a GLMM instead of a LMM, and using a moderate-sized rather than an arbitrarily large (asymptotic) data set — have made the likelihood ratio test liberal (increased type I error) rather than conservative (decreased type I error).\n\nWe can also inspect the random effects estimates themselves (in proper statistical jargon, these might be considered “predictions” rather than “estimates” (Robinson, 1991)). We use the built-in dotplot method for the random effects extracted from glmer fits (i.e. ranef(model,condVar=TRUE)), which returns a list of plots, one for each random effect level in the model.\n\n```{r}\n#| fig-cap: Distribution of BLUPs for genotypes and populations\nr1 <- as.data.frame(ranef(mp4, condVar = TRUE, whichel = c(\"gen\", \"popu\")))\np1 <- ggplot(subset(r1, grpvar == \"gen\"), aes(y = grp, x = condval)) +\n  geom_point() +\n  geom_pointrange(\n    aes(xmin = condval - condsd * 1.96, xmax = condval + condsd * 1.96)\n  ) +\n  geom_vline(aes(xintercept = 0, color = \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\np2 <- ggplot(subset(r1, grpvar == \"popu\"), aes(y = grp, x = condval)) +\n  geom_point() +\n  geom_pointrange(\n    aes(xmin = condval - condsd * 1.96, xmax = condval + condsd * 1.96)\n  ) +\n  geom_vline(aes(xintercept = 0, color = \"red\")) +\n  theme_classic() +\n  theme(legend.position = \"none\")\np1 + p2\n```\n\nAs expected from the similarity of the variance estimates, the population-level estimates (the only shared component) do not differ much between the two models. There is a hint of regional differentiation — the Spanish populations have higher fruit sets than the Swedish and Dutch populations. Genotype 34 again looks a little bit unusual.\n\n#### Fixed effects\n\n Now we want to do inference on the fixed effects. We use the drop1 func- tion to assess both the AIC difference and the likelihood ratio test between models. (In glmm_funs.R we define a convenience function dfun to convert the AIC tables returned by drop1 (which we will create momentarily) into ∆AIC tables.) Although the likelihood ratio test (and the AIC) are asymptotic tests, comparing fits between full and reduced models is still more accurate than the Wald (curvature-based) tests shown in the summary tables for glmer fits.\n\n```{r}\n(dd_aic <- dfun(drop1(mp4)))\n(dd_lrt <- drop1(mp4, test = \"Chisq\"))\n```\n\nOn the basis of these comparisons, there appears to be a very strong effect of rack and weak effects of status and of the interaction term. Dropping the nutrient:amd interaction gives a (slightly) increased AIC (∆AIC = 1.4), so the full model has the best expected predictive capability (by a small margin). On the other hand, the p-value is slightly above 0.05 (p = 0.06). At this point we remove the non-significant interaction term so we can test the main effects. (We don’t worry about removing status because it measures an aspect of experimental design that we want to leave in the model whether it is significant or not.) Once we have fitted the reduced model, we can run the LRT via anova.\n\n```{r}\nmp5 <- update(mp4, . ~ . - amd:nutrient)\nanova(mp5, mp4)\n```\n\n**Exercise**\nTest now the reduced model.\n\nIn the reduced model, we find that both nutrients and clipping have strong effects, whether measured by AIC or LRT. If we wanted to be still more careful about our interpretation, we would try to relax the asymptotic assumption. In classical linear models, we would do this by doing F tests with the appropriate denominator degrees of freedom. In “modern” mixed model approaches, we might try to use denominator-degree-of-freedom approximations such as the Kenward-Roger (despite the controversy over these approximations, they are actually available in `lmerTest`, but they do not apply to GLMMs. We can use a parametric bootstrap comparison between nested models to test fixed effects, as we did above for random effects, with the caveat that is computationally slow.\n\nIn addition, we can check the normality of the random effects and find they are reasonable (Fig. 10). \n\n```{r}\n#| fig-cap: Q-Q plot of BLUPs from model mp5\nr5 <- as.data.frame(ranef(mp5))\nggplot(data = r5, aes(sample = condval)) +\n  geom_qq() + geom_qq_line() +\n  facet_wrap(~ grpvar) +\n  theme_classic()\n```\n\nChecking everything with DHARMa also\n\n\n```{r}\nscaled_res <- simulateResiduals(mp5)\nplot(scaled_res)\ntestZeroInflation(mp5, plot = TRUE)\n```\n\nIt is better than before but not perfect. I think this is completely OK and that it will extremely rarely be perfect. You need to learn what is acceptable (by that I mean you find acceptable) and be happy to justify and discuss your decisions.\n\n### Conclusions\n Our final model includes fixed effects of nutrients and clipping, as well as the nuisance variables rack and status; observation-level random effects to ac- count for overdispersion; and variation in overall fruit set at the population and genotype levels. However, we don’t (apparently) have quite enough in- formation to estimate the variation in clipping and nutrient effects, or their interaction, at the genotype or population levels. There is a strong overall positive effect of nutrients and a slightly weaker negative effect of clipping. The interaction between clipping and nutrients is only weakly supported (i.e. the p-value is not very small), but it is positive and about the same magnitude as the clipping effect, which is consistent with the statement that “nutrients cancel out the effect of herbivory”.\n\n\n::: {.callout-caution}\n# Exercise\n**Exercise**\n\n- Re-do the analysis with region as a fixed effect.\n- Re-do the analysis with a one-way layout as suggested above\n:::\n\n### Happy generalized mixed-modelling\n\n```{r}\n#| echo: false\n#| out-width: 50%\n#| fig-align: center\n#| fig-cap: A GLMM character\nknitr::include_graphics(\"images/Thorn.png\")\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":true,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","from":"markdown+emoji","number-sections":true,"output-file":"52-intro_glmm.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en-CA","fig-responsive":true,"quarto-version":"1.5.45","version":"0.6.0","year":2024,"bibliography":["biblio/book.bib","biblio/grateful-refs.bib"],"biblio-style":"apalike","csl":"biblio/ecology.csl","params":{"echo_sol":true,"longrun":false,"html_pdf":true},"cit-title":"Do what you think is interesting,  \ndo something that you think is fun and worthwhile,  \nbecause otherwise you won’t do it well anyway.\n","cit-author":"Brian W. Kernighan","theme":{"light":"cosmo","dark":["cosmo","css/theme-dark.scss"]},"author-meta":"Julien Martin"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"paged","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"lualatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","from":"markdown+emoji","number-sections":true,"output-file":"52-intro_glmm.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":false,"version":"0.6.0","year":2024,"bibliography":["biblio/book.bib","biblio/grateful-refs.bib"],"biblio-style":"apalike","csl":"biblio/ecology.csl","params":{"echo_sol":true,"longrun":false,"html_pdf":true},"lang":"en-CA","cit-title":"Do what you think is interesting,  \ndo something that you think is fun and worthwhile,  \nbecause otherwise you won’t do it well anyway.\n","cit-author":"Brian W. Kernighan","documentclass":"scrreprt","classoption":["chapterprefix=true","headings=big","twoside=semi"],"papersize":"letter","fontsize":"11pt","geometry":["top=2cm","bottom=2cm","left=2cm","right=2cm","footskip=1cm"],"colorlinks":true,"linestretch":1.5,"template-partials":["latex/before-title.tex","latex/before-body.tex"]},"extensions":{"book":{"selfContainedOutput":true}}},"epub":{"identifier":{"display-name":"ePub","target-format":"epub","base-format":"epub"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"epub","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":false,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"default-image-extension":"png","html-math-method":"mathml","to":"epub","from":"markdown+emoji","toc":true,"output-file":"52-intro_glmm.epub"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"version":"0.6.0","year":2024,"bibliography":["biblio/book.bib","biblio/grateful-refs.bib"],"biblio-style":"apalike","csl":"biblio/ecology.csl","params":{"echo_sol":true,"longrun":false,"html_pdf":true},"lang":"en-CA","cit-title":"Do what you think is interesting,  \ndo something that you think is fun and worthwhile,  \nbecause otherwise you won’t do it well anyway.\n","cit-author":"Brian W. Kernighan","dev":"svglite","stylesheet":"css/epub.css"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf","epub"]}