[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sur le chemin de l’enf-R",
    "section": "",
    "text": "Préface",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#lobjectif-de-ce-livre",
    "href": "index.html#lobjectif-de-ce-livre",
    "title": "Sur le chemin de l’enf-R",
    "section": "L’objectif de ce livre",
    "text": "L’objectif de ce livre\nL’objectif de ce livre est de te présenter R, un environnement interactif puissant et flexible pour le calcul et la recherche statistiques. R n’est pas difficile à apprendre en soi, mais comme pour l’apprentissage de toute nouvelle langue (parlée ou informatique), la courbe d’apprentissage initiale peut être abrupte et quelque peu décourageante. Il ne s’agit pas de tout couvrir, mais simplement de t’aider à gravir la courbe d’apprentissage initiale (potentiellement plus rapidement) et de te fournir les compétences de base (et la confiance !) nécessaires pour commencer ton propre voyage avec R.",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#livre-multilingue",
    "href": "index.html#livre-multilingue",
    "title": "Sur le chemin de l’enf-R",
    "section": "Livre multilingue",
    "text": "Livre multilingue\nLe livre est fourni comme un livre multilingue qui brise la barrière de la langue et permet potentiellement de faciliter l’apprentissage de R et de son environnement principalement anglophone. Nous sommes toujours à la recherche de bénévoles pour nous aider à développer le livre et à ajouter d’autres langues à la liste qui ne cesse de s’allonger . N’hésite pas à Contacte-nous si tu veux nous aider\nSur la page web, tu peux changer de langue via le  dans la barre de navigation. Après avoir changer de langue, tu peux télécharger le document en pdf ou epub pour cet langue .\nListe des langues :\n\nanglais (publié mais à peaufiner)\nfrançais (en développement, en attendant que l’anglais soit peaufiné)\nespagnol (en développement, en attendant que l’anglais soit peaufiné)",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#comment-utiliser-ce-livre",
    "href": "index.html#comment-utiliser-ce-livre",
    "title": "Sur le chemin de l’enf-R",
    "section": "Comment utiliser ce livre",
    "text": "Comment utiliser ce livre\nPour une meilleure expérience, nous te recommandons de lire la version web de ce livre que tu peux trouver à https://biostats-uottawa.github.io/R/fr.\nLa version web inclut une barre de navigation incluant des options pour faciliter la lecture , de recherche , pour changer la couleur  et pour suggérer des modifications ou reporter des problèmes . Tu peux aussi télécharger le document  au format pdf ou epub.\nNous utilisons quelques conventions typographiques tout au long de ce livre.\nLe code R et la sortie qui en résulte sont présentés dans des blocs de code dans notre livre.\n\n2 + 2\n\n[1] 4\n\n\nLes fonctions dans le texte sont présentées avec des parenthèses à la fin en utilisant la police de code, c’est-à-dire mean() ou sd() etc.\nLes objets sont représentés à l’aide de la police de code sans les parenthèses, c’est-à-dire obj1, obj2 etc.\nLes paquets R dans le texte sont indiqués en utilisant la police de code et suivis de l’icone 📦, exemple tidyverse 📦.\nUne série d’actions nécessaires pour accéder aux commandes de menu dans RStudio ou VSCode sont identifiées comme suit File -&gt; New File -&gt; R Script ce qui se traduit par “clique sur le menu Fichier, puis clique sur Nouveau fichier et sélectionne R Script”.\nLorsque nous faisons référence à IDE (Intégrée Ddéveloppement Environnement) dans la suite du texte, il s’agit de RStudio ou de VScode.\nLorsque nous parlons de .[Rq]md Nous entendons par là les documents R markdown (.Rmd) ou Quarto (.qmd) et nous parlerons généralement des documents R markdown en faisant référence à l’un ou l’autre des fichiers .Rmd ou .qmd.\nLe manuel tente de mettre en évidence certaines parties du texte à l’aide des encadrés et icônes suivants.\n\n\n\n\n\n\nExercices\n\n\n\nDes choses à faire pour toi\n\n\n\n\n\n\n\n\nSolutions\n\n\n\nCode R et explications\n\n\n\n\n\n\n\n\nAvertissement\n\n\n\navertissements\n\n\n\n\n\n\n\n\nImportant\n\n\n\npoints importants\n\n\n\n\n\n\n\n\nNote\n\n\n\nnotes",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#sec-qui",
    "href": "index.html#sec-qui",
    "title": "Sur le chemin de l’enf-R",
    "section": "Qui sommes-nous ?",
    "text": "Qui sommes-nous ?\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nJulien Martin est professeur à l’Université d’Ottawa en Écologie évolutive. Il a découvert le merveilleux monde R avec la version 1.8.1 et l’enseigne depuis R v2.4.0.\n\n\n: site uOttawa, site labo\n\n\n: jgamartin\n\n\n: juliengamartin",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#remerciements",
    "href": "index.html#remerciements",
    "title": "Sur le chemin de l’enf-R",
    "section": "Remerciements",
    "text": "Remerciements\nCe livre a commencé comme un fork sur github à partir de l’excellent An introduction to Rde Douglas, Roos, Mancini, Couto et Lusseau. (Douglas 2023). Il a été forké le 23 avril 2023 à partir de Dépôt github Alexd106 puis modifié et mis à jour en suivant mes propres besoins et ma perspective d’enseignement de R. Cela fait également partie d’un projet de livre R multilingue visant à améliorer l’équité et la diversité. Il a commencé par une traduction en français et a été/sera étendu à de nombreuses autres langues.",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "Sur le chemin de l’enf-R",
    "section": "Licence",
    "text": "Licence\nJe partage cette version modifiée du livre original sous la licence Licence Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n\n\nLicence Creative Commons\n\nSi tu enseignes R, n’hésite pas à utiliser tout ou partie du contenu de ce livre pour aider tes propres élèves. La seule chose que je te demande, c’est de citer la source originale et les auteurs. Si tu trouves ce livre utile ou si tu as des commentaires ou des suggestions, j’aimerais beaucoup que tu me les fasses parvenir (contact info).",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "index.html#citer-le-livre",
    "href": "index.html#citer-le-livre",
    "title": "Sur le chemin de l’enf-R",
    "section": "Citer le livre",
    "text": "Citer le livre\nJulien Martin. (2024). Sur le chemin de l’enf-R. Un livre multilingue d’introduction à R. Version: 0.6.0 (2024-06-21).DOI: 10.5281/zenodo.10929585\n\n\n\n\nDouglas, A. 2023. An introduction to R.",
    "crumbs": [
      "Données",
      "Préface"
    ]
  },
  {
    "objectID": "01-debut.html",
    "href": "01-debut.html",
    "title": "1  Pour commencer",
    "section": "",
    "text": "Quelques conseils sur R\nBonne chance et n’oublie pas de t’amuser.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#quelques-conseils-sur-r",
    "href": "01-debut.html#quelques-conseils-sur-r",
    "title": "1  Pour commencer",
    "section": "",
    "text": "Utilise R souvent et régulièrement. Cela t’aidera à créer et à maintenir une dynamique très importante.\nApprendre R n’est pas un test de mémoire. L’un des avantages d’un langage de script est que tu auras toujours ton code (bien annoté) auquel te référer lorsque tu auras oublié comment faire quelque chose.\nTu n’as pas besoin de tout savoir sur R pour être productif.\nSi tu es bloqué, fais des recherches en ligne, ce n’est pas de la triche et écrire une bonne requête de recherche est une compétence en soi.\nSi tu te retrouves à fixer du code pendant des heures en essayant de comprendre pourquoi il ne fonctionne pas, alors va t’éloigner pendant quelques minutes.\nEn R, il existe de nombreuses façons d’aborder un problème particulier. Si ton code fait ce que tu veux qu’il fasse dans un temps raisonnable et de manière robuste, alors ne t’inquiète pas.\nR n’est qu’un outil pour t’aider à répondre à tes questions intéressantes. Ne perds pas de vue ce qui est important : ta ou tes questions de recherche et tes données. Aucune compétence en matière d’utilisation de R ne te sera utile si ta collecte de données est fondamentalement défectueuse ou si ta question est vague.\nReconnais qu’il y aura des moments où les choses deviendront un peu difficiles ou frustrantes. Essaie d’accepter ces périodes comme faisant partie du processus naturel d’apprentissage d’une nouvelle compétence (nous sommes tous passés par là) et souviens-toi que le temps et l’énergie que tu investis maintenant seront largement remboursés dans un avenir pas trop lointain.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#installation",
    "href": "01-debut.html#installation",
    "title": "1  Pour commencer",
    "section": "\n1.1 Installation",
    "text": "1.1 Installation\n\n1.1.1 Installer R\nPour être opérationnel, la première chose à faire est d’installer R. R est disponible gratuitement pour les systèmes d’exploitation Windows, Mac et Linux à partir du [site Web du Comprehensive R Archive Network (C]RAN) CRAN. Pour les utilisateurs de Windows et de Mac, nous te suggérons de télécharger et d’installer les versions binaires précompilées. Il existe des instructions assez complètes pour installer R pour chaque système d’exploitation (Windows,Mac ou linux ).\nQuel que soit le système d’exploitation que tu utilises, une fois que tu as installé R, tu dois vérifier qu’il fonctionne correctement. Le plus simple est de lancer R en double-cliquant sur l’icône R (Windows ou Mac) ou en tapant R dans la console (Linux). Tu devrais voir apparaître la Console R et tu devrais pouvoir taper des commandes R dans la Console après l’invite de commande &gt;. Essaie de taper le code R suivant et appuie sur Entrée :\n\nplot(1:10)\n\n\n\n\n\n\n\nUn graphique des nombres 1 à 10 sur les axes x et y devrait apparaître. Si c’est le cas, tu peux commencer. Si ce n’est pas le cas, nous te suggérons de noter toutes les erreurs produites et d’utiliser Google pour résoudre le problème.\n\n1.1.2 Installation d’un IDE\nNous recommandons fortement d’utiliser un Intégrée Ddéveloppement Environnement de développement (IDE) pour travailler avec R. Un IDE simple et extrêmement populaire est RStudio. Une alternative à RStudio est Visual Studio Code, ou VSCode. Un IDE peut être considéré comme un complément à R qui fournit une interface plus conviviale, incorporant la console R, un éditeur de scripts et d’autres fonctionnalités utiles (comme R markdown et l’intégration de Git Hub).\n\n\n\n\n\n\nMise en garde\n\n\n\nTu dois installer R avant d’installer un IDE (voir Section 1.1.1 pour plus de détails).\n\n\n\n\n\n\n\n\nNote\n\n\n\nLorsque nous nous référons à L’IDE dans la suite du texte, il s’agit de RStudio ou de VScode.\n\n\n\n1.1.2.1 RStudio\nRStudio est disponible gratuitement pour les systèmes d’exploitation Windows, Mac et Linux et peut être téléchargé à partir du site de RStudio. Tu dois sélectionner la version ‘RStudio Desktop’.\n\n1.1.2.2 VSCode\nVSCode est disponible gratuitement pour les systèmes d’exploitation Windows, Mac et Linux et peut être téléchargé à partir du site de VS Code.\nEn outre, tu dois installer le l’extension R de VSCode. Pour faire de VSCode une véritable centrale pour travailler avec R, nous te recommandons fortement d’installer également :\n\n\nradian radian : Une console R moderne qui corrige de nombreuses limitations du terminal R officiel et prend en charge de nombreuses fonctionnalités telles que la coloration syntaxique et l’autocomplétion.\n\nVSCode-R-Debugger VSCode-R-Debugger : Une extension de VS Code pour prendre en charge les capacités de débogage de R.\n\nhttpgd: Un paquetage R 📦 pour fournir un dispositif graphique qui sert de façon asynchrone des graphiques SVG via HTTP et WebSockets.\n\n1.1.2.3 Alternatives à RStudio et VSCode\nPlutôt que d’utiliser un IDE “tout en un”, de nombreuses personnes choisissent d’utiliser R et un éditeur de script séparé pour écrire et exécuter le code R. Si tu ne sais pas ce qu’est un éditeur de script, tu peux l’assimiler à un traitement de texte, mais spécialement conçu pour écrire du code. Heureusement, de nombreux éditeurs de scripts sont disponibles gratuitement, alors n’hésite pas à les télécharger et à expérimenter jusqu’à ce que tu en trouves un qui te plaise. Certains éditeurs de scripts ne sont disponibles que pour certains systèmes d’exploitation et tous ne sont pas spécifiques à R. Tu trouveras ci-dessous des suggestions d’éditeurs de scripts. C’est à toi de choisir celui que tu veux : l’une des grandes qualités de R est que TU c’est que tu peux choisir comment tu veux utiliser R.\n\n1.1.2.3.1 Éditeurs de texte avancés\nUn moyen léger mais efficace de travailler avec R est d’utiliser des éditeurs de texte avancés tels que :\n\n\nAtom (tous les systèmes d’exploitation)\n\nBBedit (Mac OS)\n\ngedit (Linux ; livré avec la plupart des distributions Linux)\n\nMacVim (Mac OS)\n\nNano (Linux)\n\nNotepad++ [bloc-notes] (Windows)\n\nSublime Text (tous les systèmes d’exploitation)\n\nvim et son extension NVim-R (Linux)\n\n1.1.2.3.2 Environnements de développement intégrés\nCes environnements sont plus puissants que de simples éditeurs de texte, et sont similaires à RStudio :\n\n\nEmacs et son extension Emacs parle statistiques (tous les systèmes d’exploitation)\n\nRKWard (Linux)\n\nTinn-R (Windows)",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-orient",
    "href": "01-debut.html#sec-orient",
    "title": "1  Pour commencer",
    "section": "\n1.2 Orientation de l’IDE",
    "text": "1.2 Orientation de l’IDE\n\n1.2.1 RStudio\nLorsque tu ouvres R studio pour la première fois, tu devrais voir la présentation suivante (elle peut être légèrement différente sur un ordinateur Windows).\n\n\n\n\n\n\n\n\nLa grande fenêtre (ou volet) sur la gauche est le Console de la console. La fenêtre en haut à droite est la fenêtre Environnement / Histoire / Connexions et la fenêtre en bas à droite est la fenêtre Fichiers / Tracés / Paquets / Aide / Visionneuse de la fenêtre. Nous aborderons chacun de ces volets ci-dessous. Tu peux personnaliser l’emplacement de chaque volet en cliquant sur le menu “Outils” puis en sélectionnant Options globales –&gt; Disposition des volets. Tu peux redimensionner les volets en cliquant sur le milieu des bords de la fenêtre et en le faisant glisser dans la direction souhaitée. Il existe une multitude d’autres façons de personnaliser RStudio.\n\n1.2.1.1 Console\nLa console est le cheval de bataille de R. C’est là que R évalue tout le code que tu écris. Tu peux taper du code R directement dans la console à l’invite de la ligne de commande, &gt;. Par exemple, si tu tapes 2 + 2 dans la console, tu devrais obtenir la réponse 4 (rassurante). Ne t’inquiète pas pour le [1] au début de la ligne pour l’instant.\n\n\n\n\n\n\n\n\nCependant, dès que tu commences à écrire plus de code R, cela devient plutôt encombrant. Au lieu de taper le code R directement dans la console, une meilleure approche consiste à créer un script R. Un script R est un simple fichier texte avec une balise .R qui contient tes lignes de code R. Ces lignes de code sont ensuite introduites dans la console R, ligne par ligne. Pour créer un nouveau script R, clique sur le menu “Fichier” puis sélectionne Nouveau fichier –&gt; Script R.\n\n\n\n\n\n\n\n\nTu remarqueras qu’une nouvelle fenêtre (appelée volet Source) apparaît en haut à gauche de RStudio et que la console se trouve maintenant en bas à gauche. La nouvelle fenêtre est un éditeur de script et c’est là que tu écriras ton code.\n\n\n\n\n\n\n\n\nPour faire passer ton code de ton éditeur de script à la console, place ton curseur sur la ligne de code, puis clique sur le bouton “Exécuter” en haut à droite de la fenêtre de l’éditeur de script.\n\n\n\n\n\n\n\n\nTu devrais voir le résultat dans la fenêtre de la console. Si le fait de cliquer sur le bouton “Exécuter” devient fastidieux, tu peux utiliser le raccourci clavier “ctrl + entrée” (sous Windows et Linux) ou “cmd + entrée” (sous Mac). Tu peux enregistrer tes scripts R sous la forme d’un fichier .R en sélectionnant le menu ‘Fichier’ et en cliquant sur enregistrer. Remarque que le nom du fichier dans l’onglet devient rouge pour te rappeler que tu as des modifications non enregistrées. Pour ouvrir ton script R dans RStudio, sélectionne le menu ‘Fichier’ puis ‘Ouvrir le fichier…’. Enfin, il est bon de noter que, bien que les scripts R soient enregistrés avec un nom de fichier .R il s’agit en fait de simples fichiers texte qui peuvent être ouverts avec n’importe quel éditeur de texte.\n\n1.2.1.2 Environnement/Histoire/Connexions\nLa fenêtre Environnement / Historique / Connexions te montre de nombreuses informations utiles. Tu peux accéder à chaque composant en cliquant sur l’onglet approprié dans le volet.\n\nL’onglet ‘Environnement’ affiche tous les objets que tu as créés dans l’environnement actuel (global). Ces objets peuvent être des données que tu as importées ou des fonctions que tu as écrites. Les objets peuvent être affichés sous forme de liste ou de grille en sélectionnant ton choix dans le bouton déroulant situé en haut à droite de la fenêtre. Si tu es dans le format Grille, tu peux supprimer des objets de l’environnement en cochant la case vide à côté du nom de l’objet, puis en cliquant sur l’icône du balai. Il existe également un bouton “Importer un ensemble de données” qui permet d’importer des données sauvegardées dans différents formats de fichiers. Cependant, nous te conseillons de ne pas utiliser cette approche pour importer tes données car elle n’est pas reproductible et donc pas robuste (voir ?sec-data-r pour plus de détails).\nL’onglet ‘Historique’ contient une liste de toutes les commandes que tu as entrées dans la console R. Tu peux rechercher dans ton historique la ligne de code que tu as oubliée, renvoyer le code sélectionné dans la Console ou la fenêtre Source. En général, nous n’utilisons jamais cette fonction car nous nous référons toujours à notre script R.\nL’onglet “Connexions” te permet de te connecter à diverses sources de données telles que des bases de données externes.\n\n1.2.1.3 Fichiers/Plots/Packages/Aide/Viewer\n\nL’onglet ‘Fichiers’ répertorie tous les fichiers et répertoires externes dans le répertoire de travail actuel de ton ordinateur. Il fonctionne comme l’explorateur de fichiers (Windows) ou le Finder (Mac). Tu peux ouvrir, copier, renommer, déplacer et supprimer les fichiers listés dans la fenêtre.\nL’onglet ‘Tracés’ est l’endroit où tous les tracés que tu as créés dans R sont affichés (à moins que tu ne dises le contraire à R). Tu peux “zoomer” sur les tracés pour les agrandir à l’aide du bouton de la loupe, et faire défiler les tracés créés précédemment à l’aide des boutons fléchés. Il est également possible d’exporter les tracés vers un fichier externe à l’aide du menu déroulant “Exporter”. Les tracés peuvent être exportés dans différents formats de fichiers tels que jpeg, png, pdf, tiff ou copiés dans le presse-papiers (bien qu’il soit probablement préférable d’utiliser les fonctions R appropriées pour ce faire - voir ?sec-graphics_r pour plus de détails).\nL’onglet ‘Paquets’ répertorie tous les paquets que tu as installés sur ton ordinateur. Tu peux également installer de nouveaux paquets et mettre à jour les paquets existants en cliquant respectivement sur les boutons ‘Installer’ et ‘Mettre à jour’.\nL’onglet ‘Aide’ affiche la documentation d’aide R pour n’importe quelle fonction. Nous verrons comment afficher les fichiers d’aide et comment rechercher de l’aide dans le ?sec-basics_r.\nL’onglet ‘Visionneuse’ affiche le contenu web local tel que les graphiques web générés par certains paquets.\n\n1.2.2 VSCode\n\n\n\n\n\n\n\n\n\n1.2.2.1 Panneau gauche\nContient :\n\nGestionnaire de fichiers et aperçu des fichiers.\nSupport R incluant l’environnement R / la recherche R / l’aide R / l’installation de paquets\nInteraction avec Github\n\n\n\n\n\n\n\n\n\n\n(a) fichier\n\n\n\n\n\n\n\n\n\n(b) git\n\n\n\n\n\n\n\n\n\n(c) r-ext\n\n\n\n\n\n\nFigure 1.1: VS Code panneau de gauche\n\n\n\n1.2.2.2 Onglets de l’éditeur\nComprend :\n\npanneau de tracé (avec historique et navigation)\nédition de scripts\npanneaux de prévisualisation\n\n\n\n\n\n\n\n\n\n\n1.2.2.3 Fenêtre du terminal\nContient :\n\nle terminal permettant d’avoir une session R ou tout autre type de terminaux nécessaires (bash/tmux/). Il peut être divisé et exécuter plusieurs sessions en même temps.\nun panneau de problèmes mettant en évidence les problèmes de grammaire et de codage",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-packages",
    "href": "01-debut.html#sec-packages",
    "title": "1  Pour commencer",
    "section": "\n1.3 Paquets R",
    "text": "1.3 Paquets R\nL’installation de base de R est livrée en standard avec de nombreux paquets utiles. Ces paquets contiennent de nombreuses fonctions que tu utiliseras au quotidien. Cependant, lorsque tu commenceras à utiliser R pour des projets plus variés (et que ton utilisation de R évoluera), tu verras qu’il arrivera un moment où tu auras besoin d’étendre les capacités de R. Heureusement, plusieurs milliers d’utilisateurs de R ont développé un code utile et l’ont partagé sous forme de paquets installables. Tu peux considérer un paquet comme une collection de fonctions, de données et de fichiers d’aide rassemblés dans une structure standard bien définie que tu peux télécharger et installer dans R. Ces paquets peuvent être téléchargés à partir de diverses sources, mais les plus populaires sont les suivantes CRAN, Bioconductor et GitHub. Actuellement, le CRAN héberge plus de 15 000 paquets et est le dépôt officiel des paquets R fournis par les utilisateurs. Bioconductor fournit des logiciels open source orientés vers la bio-informatique et héberge plus de 1800 paquets R. GitHub est un site Web qui héberge des dépôts git pour toutes sortes de logiciels et de projets (pas seulement R). Souvent, les versions de développement de pointe des paquets R sont hébergées sur GitHub, donc si tu as besoin de toutes les nouvelles cloches et sifflets, cela peut être une option. Cependant, l’inconvénient potentiel de l’utilisation de la version de développement d’un paquetage R est qu’elle peut ne pas être aussi stable que la version hébergée sur CRAN (elle est en cours de développement !) et que la mise à jour des paquets n’est pas automatique.\n\n1.3.1 Paquets CRAN\nPour installer un paquet à partir du CRAN, tu peux utiliser la commande install.packages() fonction. Par exemple, si tu veux installer le paquet remotes entre le code suivant dans la fenêtre Console de RStudio (note : tu auras besoin d’une connexion internet pour faire cela)\n\ninstall.packages(\"remotes\", dependencies = TRUE)\n\nIl te sera peut-être demandé de choisir un miroir CRAN, sélectionne simplement ‘0-cloud’ ou un miroir proche de ton emplacement. Le dependencies = TRUE permet de s’assurer que les paquets supplémentaires nécessaires seront également installés.\nC’est une bonne pratique de mettre régulièrement à jour tes paquets précédemment installés pour avoir accès aux nouvelles fonctionnalités et aux corrections de bogues. Pour mettre à jour les paquets CRAN, tu peux utiliser la commande update.packages() (tu auras besoin d’une connexion Internet fonctionnelle pour cela)\n\nupdate.packages(ask = FALSE)\n\nLa fonction ask = FALSE évite d’avoir à confirmer chaque téléchargement de paquet, ce qui peut être pénible si tu as beaucoup de paquets installés.\n\n1.3.2 Paquets Bioconductor\nPour installer les paquets de Bioconductor, le processus est un peu différent. Tu dois d’abord installer le BiocManager 📦 paquet. Tu ne dois le faire qu’une seule fois, à moins que tu ne réinstalles ou mettes à jour R.\n\ninstall.packages(\"BiocManager\", dependencies = TRUE)\n\nUne fois que le BiocManager 📦 a été installé, tu peux soit installer tous les paquets “de base” de Bioconductor à l’aide de la commande\n\nBiocManager::install()\n\nou installer des paquets spécifiques tels que le GenomicRanges 📦 et edgeR 📦 paquets\n\nBiocManager::install(c(\"GenomicRanges\", \"edgeR\"))\n\nPour mettre à jour les paquets de Bioconductor, il suffit d’utiliser la commande BiocManager::install() à nouveau\n\nBiocManager::install(ask = FALSE)\n\nEncore une fois, tu peux utiliser la fonction ask = FALSE pour éviter d’avoir à confirmer chaque téléchargement de paquet.\n\n1.3.3 Paquets GitHub\nIl existe de multiples options pour installer des paquets hébergés sur GitHub. La méthode la plus efficace est sans doute d’utiliser la fonction install_github() de la fonction remotes 📦 (tu as installé ce paquet précédemment, Section 1.3.1). Avant d’utiliser la fonction, tu devras connaître le nom d’utilisateur GitHub du propriétaire du dépôt ainsi que le nom du dépôt. Par exemple, la version de développement de dplyr 📦 de Hadley Wickham est hébergée sur le compte GitHub de tidyverse et porte le nom de dépôt ” dplyr ” (il suffit de taper ” github dplyr ” sur Google). Pour installer cette version depuis GitHub, utilise\n\nremotes::install_github(\"tidyverse/dplyr\")\n\nLe moyen le plus sûr (à notre connaissance) de mettre à jour un paquetage installé depuis GitHub est de le réinstaller en utilisant la commande ci-dessus.\n\n1.3.4 Utilisation des paquets\nUne fois que tu as installé un paquet sur ton ordinateur, il n’est pas immédiatement disponible pour que tu puisses l’utiliser. Pour utiliser un paquet, tu dois d’abord le charger à l’aide de la commande library() en utilisant la fonction Par exemple, pour charger le paquet remotes 📦 que tu as précédemment installé\n\nlibrary(remotes)\n\nLe library() chargera également tous les paquets supplémentaires nécessaires et pourra imprimer des informations supplémentaires sur les paquets. Il est important de savoir qu’à chaque fois que tu démarres une nouvelle session R (ou que tu restaures une session précédemment sauvegardée), tu dois charger les paquets que tu vas utiliser. Nous avons tendance à mettre tous nos library() nécessaires à notre analyse en haut de nos scripts R pour les rendre facilement accessibles et faciles à compléter au fur et à mesure que notre code se développe. Si tu essaies d’utiliser une fonction sans avoir au préalable chargé le package R correspondant, tu recevras un message d’erreur indiquant que R n’a pas pu trouver la fonction. Par exemple, si tu essaies d’utiliser la fonction install_github() sans charger le paquetage remotes 📦 tu recevras l’erreur suivante\n\ninstall_github(\"tidyverse/dplyr\")\n\n# Error in install_github(\"tidyverse/dplyr\") :\n#  could not find function \"install_github\"\n\nParfois, il peut être utile d’utiliser une fonction sans utiliser d’abord le package library() fonction. Si, par exemple, tu n’utilises qu’une ou deux fonctions dans ton script et que tu ne veux pas charger toutes les autres fonctions d’un paquet, tu peux accéder directement à la fonction en spécifiant le nom du paquet suivi de deux points, puis du nom de la fonction.\n\nremotes::install_github(\"tidyverse/dplyr\")\n\nC’est ainsi que nous avons pu utiliser le install() et install_github()fonctions ci-dessus sans charger les paquets au préalableBiocManager 📦 et remotes 📦 . La plupart du temps, nous recommandons d’utiliser le library() fonction.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-work-d",
    "href": "01-debut.html#sec-work-d",
    "title": "1  Pour commencer",
    "section": "\n1.4 Répertoires de travail",
    "text": "1.4 Répertoires de travail\nLe répertoire de travail est l’emplacement par défaut où R cherchera les fichiers que tu veux charger et où il mettra les fichiers que tu enregistres. L’un des avantages de l’utilisation des projets RStudio est que lorsque tu ouvres un projet, il définit automatiquement ton répertoire de travail à l’emplacement approprié. Tu peux vérifier le chemin d’accès de ton répertoire de travail en utilisant l’une des deux méthodes suivantes getwd() ou here() fonctions.\n\ngetwd()\n\n[1] \"/home/julien/Documents/courses/biostats/R_way/lang/fr\"\n\n\nDans l’exemple ci-dessus, le répertoire de travail est un dossier appelé ‘R_way’ qui est un sous-dossier de ‘biostats’ dans le dossier ‘courses’ qui lui-même se trouve dans un dossier ‘Documents’ situé dans le dossier ‘julien’ qui lui-même se trouve dans le dossier ‘home’. Sur un ordinateur fonctionnant sous Windows, notre répertoire de travail comprendrait également une lettre de lecteur (par ex. C:\\home\\julien\\Documents\\courses\\biostats\\R_way).\nSi tu n’utilises pas d’IDE, tu dois définir ton répertoire de travail à l’aide de la commande setwd() au début de chaque script R (ce que nous avons fait pendant de nombreuses années).\n\nsetwd(\"/home/julien/Documents/courses/biostats/R_way/\")\n\nCependant, le problème avec setwd() est qu’il utilise un absolu qui est spécifique à l’ordinateur sur lequel tu travailles. Si tu veux envoyer ton script à quelqu’un d’autre (ou si tu travailles sur un autre ordinateur), ce chemin d’accès absolu ne fonctionnera pas sur l’ordinateur de ton ami/collègue car la configuration de son répertoire sera différente (il est peu probable que tu aies une structure de répertoire /home/julien/Documents/courses/biostats/ sur ton ordinateur). Il en résulte un projet qui n’est pas autonome et qui n’est pas facilement transportable. Les IDE résolvent ce problème en te permettant d’utiliser relatif qui sont relatifs au fichier racine du projet. Le répertoire du projet racine est simplement le répertoire qui contient le fichier .Rproj dans Rstudio (first_project.Rproj dans notre cas) ou le dossier de base de ton espace de travail dans VScode. Si tu veux partager ton analyse avec quelqu’un d’autre, il te suffit de copier l’ensemble du répertoire du projet et de l’envoyer à ton à ton collaborateur. Il lui suffira alors d’ouvrir le fichier du projet et tous les scripts R qui contiennent des références à des chemins de fichiers relatifs fonctionneront tout simplement. Par exemple, disons que tu as créé un sous-répertoire appelé data dans ton répertoire de projet racine qui contient un fichier délimité en csv appelé mydata.csv (nous aborderons les structures de répertoire ci-dessous (Section 1.5)). Pour importer cette base de données dans un projet RStudio à l’aide de la fonction read.csv() (ne t’inquiète pas pour l’instant, nous aborderons ce sujet de façon beaucoup plus détaillée dans le ?sec-data-r), tout ce que tu dois inclure dans ton script R est\n\ndat &lt;- read.csv(\"data/mydata.csv\")\n\nParce que le chemin d’accès au fichier data/mydata.csv est relatif au répertoire du projet, peu importe l’endroit où ton collaborateur enregistre le répertoire du projet sur son ordinateur, il fonctionnera toujours.\nSi tu n’utilises pas un projet RStudio ou un espace de travail VScode, tu devras soit définir le répertoire de travail en fournissant le chemin complet de ton répertoire, soit spécifier le chemin complet du fichier de données. Aucune de ces deux options n’est reproductible sur d’autres ordinateurs.\n\nsetwd(\"/home/julien/Documents/courses/biostats/R_way\")\n\ndat &lt;- read.csv(\"data/mydata.csv\")\n\nou\n\ndat &lt;- read.csv(\"/home/julien/Documents/courses/biostats/R_way/data/mydata.csv\")\n\nPour ceux qui veulent pousser plus loin la notion de chemins d’accès relatifs, jette un coup d’œil à l’option here() de la fonction here[paquet][ici]. Les here() te permet de construire des chemins d’accès pour n’importe quel fichier par rapport au répertoire racine du projet qui sont également indépendants du système d’exploitation (fonctionne sur une machine Mac, Windows ou Linux). Par exemple, pour importer notre mydata.csv à partir du répertoire data il suffit d’utiliser :\n\nlibrary(here) # you may need to install the here package first\ndat &lt;- read.csv(here(\"data\", \"mydata.csv\"))",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-dir-struc",
    "href": "01-debut.html#sec-dir-struc",
    "title": "1  Pour commencer",
    "section": "\n1.5 Structure du répertoire",
    "text": "1.5 Structure du répertoire\nEn plus d’utiliser RStudio Projects, c’est aussi une très bonne pratique de structurer ton répertoire de travail d’une manière cohérente et logique pour t’aider et aider tes collaborateurs. Nous utilisons fréquemment la structure de répertoire suivante dans nos projets basés sur R.\n\n\n\n\n\nroot\nracinedot01\nroot-&gt;dot01\ndot1\nroot-&gt;dot1\ndata\nDonnéesdot21\ndata-&gt;dot21\nfunctions\nFonctionsdot22\nfunctions-&gt;dot22\noutputs\nSortiesdot23\noutputs-&gt;dot23\nscripts\nScriptsdot24\nscripts-&gt;dot24\nwd\nrépertoire de travailLOT1\nbrutes traitées metadonnéesLOT2\nfonctions RLOT4\nscripts analyse documents R markdownLOT3\npdf html figuresdot01-&gt;wd\ndot1-&gt;data\ndot2\ndot1-&gt;dot2\ndot2-&gt;functions\ndot3\ndot2-&gt;dot3\ndot3-&gt;outputs\ndot4\ndot3-&gt;dot4\ndot4-&gt;scripts\ndot21-&gt;LOT1\ndot22-&gt;LOT2\ndot23-&gt;LOT3\ndot24-&gt;LOT4\n\n\n\n\n\n\nDans notre répertoire de travail, nous avons les répertoires suivants :\n\nRacine - C’est le répertoire de ton projet qui contient tes .Rprojfichier . Nous avons tendance à garder tous les scripts R ou [Rq]md nécessaires à l’analyse/au rapport dans ce dossier racine ou dans le dossier scripts lorsqu’il y en a trop.\ndonnées - Nous stockons toutes nos données dans ce répertoire. Le sous-répertoire appelé data contient des fichiers de données brutes et uniquement des fichiers de données brutes. Ces fichiers doivent être traités comme des en lecture seule et ne doivent en aucun cas être modifiés. Si tu as besoin de traiter/nettoyer/modifier tes données, fais-le dans R (et non dans MS Excel) car tu pourras documenter (et justifier) toutes les modifications apportées. Toutes les données traitées doivent être sauvegardées dans un fichier séparé et stockées dans le fichier processed_data et stockées dans le sous-répertoire Les informations sur les méthodes de collecte des données, les détails du téléchargement des données et toute autre métadonnée utile doivent être sauvegardés dans un document texte (voir les fichiers texte README ci-dessous) dans le sous-répertoire metadata dans le sous-répertoire\nfonctions - Il s’agit d’un répertoire facultatif dans lequel nous enregistrons toutes les fonctions R personnalisées que nous avons écrites pour l’analyse en cours. Celles-ci peuvent ensuite être importées dans R à l’aide de la commande source() fonction.\nscripts - Un répertoire facultatif dans lequel nous enregistrons nos documents R markdown et/ou les principaux scripts R que nous avons écrits pour le projet en cours sont enregistrés ici si ce n’est pas dans le dossier racine.\nsortie - Les sorties de nos scripts R, telles que les tracés, les fichiers HTML et les résumés de données, sont enregistrées dans ce répertoire. Cela nous aide, ainsi que nos collaborateurs, à distinguer les fichiers qui sont des sorties et ceux qui sont des fichiers sources.\n\nBien sûr, la structure décrite ci-dessus est juste ce qui fonctionne pour nous la plupart du temps et doit être considérée comme un point de départ pour tes propres besoins. Nous avons tendance à avoir une structure de répertoire assez cohérente dans tous nos projets, car cela nous permet de nous orienter rapidement lorsque nous revenons à un projet après un certain temps. Cela dit, les besoins varient d’un projet à l’autre, c’est pourquoi nous ajoutons et supprimons des répertoires selon les besoins.\nTu peux créer ta structure de répertoires à l’aide de l’Explorateur Windows (ou du Finder sur Mac) ou dans ton IDE en cliquant sur le bouton “Nouveau dossier” dans le panneau “Fichiers”.\nUne autre approche consiste à utiliser la fonction dir.create() de la console R.\n\n# create directory called 'data'\ndir.create(\"data\")",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-rsprojs",
    "href": "01-debut.html#sec-rsprojs",
    "title": "1  Pour commencer",
    "section": "\n1.6 Organisation des projets",
    "text": "1.6 Organisation des projets\nComme pour la plupart des choses dans la vie, lorsqu’il s’agit de traiter des données et de les analyser, les choses sont tellement plus simples si tu es organisé. Une organisation claire du projet permet à la fois à toi (et surtout au futur toi) et à tes collaborateurs de donner un sens à ce que tu as fait. Il n’y a rien de plus frustrant que de revenir à un projet des mois (parfois des années) plus tard et de devoir passer des jours (ou des semaines) à comprendre où tout se trouve, ce que tu as fait et pourquoi tu l’as fait. Un projet bien documenté qui a une structure cohérente et logique augmente la probabilité que tu puisses reprendre là où tu t’es arrêté avec un minimum d’agitation, peu importe le temps qui s’est écoulé. En outre, il est beaucoup plus facile d’écrire du code pour automatiser des tâches lorsque les fichiers sont bien organisés et portent des noms judicieux. Cela est d’autant plus vrai aujourd’hui qu’il n’a jamais été aussi facile de collecter de grandes quantités de données qui peuvent être sauvegardées dans des milliers, voire des centaines de milliers de fichiers de données distincts. Enfin, un projet bien organisé réduit le risque d’introduire des bogues ou des erreurs dans ton flux de travail et, s’ils se produisent (ce qui arrivera inévitablement à un moment ou à un autre), il est plus facile de repérer ces erreurs et de les traiter efficacement.\nIl y a aussi quelques mesures simples que tu peux prendre dès le début d’un projet pour aider à maintenir les choses en bon état.\nUne bonne façon d’organiser les choses est d’utiliser les espaces de travail RStudio Projects ou VSCode, appelés par la suite project. A project conserve tous tes scripts R, tes documents R markdown, tes fonctions R et tes données en un seul endroit. Ce qu’il y a de bien avec project est que chacun a son propre répertoire, son propre historique et ses propres documents sources, de sorte que les différentes analyses sur lesquelles tu travailles restent complètement séparées les unes des autres. Cela signifie que tu peux très facilement passer d’une analyse à l’autre. projects sans craindre qu’elles n’interfèrent l’une avec l’autre.\n\n1.6.1 RStudio\nPour créer un projet, ouvre RStudio et sélectionne File -&gt; New Project... dans le menu. Tu peux créer soit un tout nouveau projet, soit un projet à partir d’un répertoire existant, soit un projet à version contrôlée (voir ?sec-github_r pour plus de détails à ce sujet). Dans ce chapitre, nous allons créer un projet dans un nouveau répertoire.\n\n\n\n\n\n\n\n\nTu peux également créer un nouveau projet en cliquant sur le bouton “Projet” en haut à droite de RStudio et en sélectionnant “Nouveau projet…\n\n\n\n\n\n\n\n\nDans la fenêtre suivante, sélectionne ‘Nouveau projet’.\n\n\n\n\n\n\n\n\nSaisis maintenant le nom du répertoire que tu veux créer dans le champ “Nom du répertoire :” (nous l’appellerons first_project pour ce chapitre). Si tu veux changer l’emplacement du répertoire sur ton ordinateur, clique sur le bouton ‘Parcourir…’ et navigue jusqu’à l’endroit où tu souhaites créer le répertoire. Nous cochons toujours la case “Ouvrir dans une nouvelle session”. Enfin, clique sur ‘Créer un projet’ pour créer le nouveau projet.\n\n\n\n\n\n\n\n\nUne fois ton nouveau projet créé, tu auras un nouveau dossier sur ton ordinateur qui contiendra un fichier de projet RStudio appelé first_project.Rproj. Ce projet .Rproj contient diverses options de projet (mais tu ne devrais pas vraiment interagir avec lui) et peut également être utilisé comme raccourci pour ouvrir le projet directement à partir du système de fichiers (il suffit de double-cliquer dessus). Tu peux le vérifier dans l’onglet ‘Fichiers’ de RStudio (ou dans Finder si tu es sur un Mac ou dans l’Explorateur de fichiers sous Windows).\n\n\n\n\n\n\n\n\nLa dernière chose que nous te suggérons de faire est de sélectionner Tools -&gt; Project Options... dans le menu. Clique sur l’onglet ‘Général’ sur le côté gauche et modifie les valeurs de ‘Restaurer .RData dans l’espace de travail au démarrage’ et ‘Sauvegarder l’espace de travail dans .RData à la sortie’ de ‘Par défaut’ à ‘Non’. Ainsi, à chaque fois que tu ouvres ton projet, tu démarres avec une session R propre. Tu n’es pas obligé de faire cela (beaucoup de gens ne le font pas) mais nous préférons commencer avec un espace de travail complètement propre chaque fois que nous ouvrons nos projets pour éviter tout conflit potentiel avec des choses que nous avons faites dans des sessions précédentes (ce qui conduit parfois à des résultats surprenants et à des maux de tête pour trouver le problème). L’inconvénient de cette méthode est que tu devras réexécuter ton code R à chaque fois que tu ouvriras ton projet.\n\n\n\n\n\n\n\n\nMaintenant que tu as mis en place un projet RStudio, tu peux commencer à créer des scripts R (ou R markdown/ Quarto, ?sec-rmarkdown_r) ou tout ce dont tu as besoin pour compléter ton projet. Tous les scripts R seront maintenant contenus dans le projet RStudio et enregistrés dans le dossier du projet.\n\n1.6.2 VSCode\nsont similaires aux projets RStudio. Tu dois cependant créer un nouveau dossier avec un fichier R (ou un fichier texte) et l’enregistrer en tant qu’espace de travail.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-file_names",
    "href": "01-debut.html#sec-file_names",
    "title": "1  Pour commencer",
    "section": "\n1.7 Noms des fichiers",
    "text": "1.7 Noms des fichiers\nLe nom que tu donnes à tes fichiers a plus d’importance que tu ne le penses. Nommer les fichiers est également plus difficile que tu ne le penses. Pour qu’un nom de fichier soit “bon”, il faut qu’il soit informatif et relativement court. Ce n’est pas toujours un compromis facile et il faut souvent y réfléchir. Dans l’idéal, tu devrais essayer d’éviter les éléments suivants !\n\n\n\n\nsource:https://xkcd.com/1459/\n\n\n\nBien qu’il n’y ait pas vraiment d’approche standard reconnue pour nommer les fichiers (en fait il y a mais tout le monde ne l’utilise pas), il y a quelques points à garder à l’esprit.\n\nTout d’abord, évite d’utiliser des espaces dans les noms de fichiers en les remplaçant par des traits de soulignement ou même des traits d’union. Pourquoi cela est-il important ? L’une des raisons est que certains logiciels de ligne de commande (en particulier de nombreux outils bioinformatiques) ne reconnaissent pas un nom de fichier comportant un espace et que tu devras te livrer à toutes sortes de manigances en utilisant des caractères d’échappement pour t’assurer que les espaces sont traités correctement. Même si tu ne penses pas utiliser un jour un logiciel de ligne de commande, il se peut que tu le fasses indirectement. Prends R markdown par exemple, si tu veux convertir un document R markdown en pdf en utilisant la commande rmarkdown 📦 tu utiliseras en fait un moteur \\(\\LaTeX\\) en ligne de commande sous le capot (appelé Pandoc). Une autre bonne raison de ne pas utiliser d’espaces dans les noms de fichiers est que cela rend la recherche de noms de fichiers (ou de parties de noms de fichiers) à l’aide d’expressions régulières dans R (ou tout autre langage).\nPour les raisons évoquées plus haut, évite également d’utiliser des caractères spéciaux (c’est-à-dire @£$%^&*(:/)) dans tes noms de fichiers.\nSi tu mets en place des versions de tes fichiers avec des numéros séquentiels (par ex. fichier1, fichier2, fichier3, …) et que tu as plus de 9 fichiers, tu dois utiliser 01, 02, 03 … 10 pour t’assurer que les fichiers sont imprimés dans le bon ordre (vois ce qui se passe si tu ne le fais pas). Si tu as plus de 99 fichiers, utilise 001, 002, 003… etc.\nSi les noms de tes fichiers contiennent des dates, utilise le format ISO 8601 AAAA-MM-JJ (ou AAAAMMJJ) pour t’assurer que tes fichiers sont classés dans le bon ordre chronologique.\nN’utilise jamais le mot final dans un nom de fichier - il ne l’est jamais !\n\nQuelle que soit la convention de dénomination des fichiers que tu décides d’utiliser, essaie de l’adopter rapidement, de t’y tenir et d’être cohérent. Tu nous remercieras !",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sec-proj_doc",
    "href": "01-debut.html#sec-proj_doc",
    "title": "1  Pour commencer",
    "section": "\n1.8 Documentation sur les scripts",
    "text": "1.8 Documentation sur les scripts\nUn petit mot ou deux sur l’écriture du code R et la création de scripts R. À moins que tu ne fasses quelque chose de vraiment rapide et sale, nous te suggérons de toujours écrire ton code R sous la forme d’un script R. Les scripts R sont ce qui rend R si utile. Non seulement tu as un enregistrement complet de ton analyse, de la manipulation des données à la visualisation et à l’analyse statistique, mais tu peux aussi partager ce code (et ces données) avec tes amis, tes collègues et, surtout, lorsque tu soumets et publies tes recherches dans une revue. Dans cette optique, assure-toi d’inclure dans ton script R toutes les informations nécessaires pour rendre ton travail reproductible (noms des auteurs, dates, plan d’échantillonnage, etc). Ces informations peuvent être incluses sous la forme d’une série de commentaires # ou, mieux encore, en mélangeant le code exécutable et la narration dans un script R markdown (?sec-rmarkdown_r). C’est aussi une bonne pratique d’inclure la sortie de la fonction sessionInfo() à la fin de tout script qui imprime la version de R, les détails du système d’exploitation et les paquets chargés. Une très bonne alternative consiste à utiliser la fonction session_info() de la fonction xfun 📦 pour obtenir un résumé plus concis de l’environnement de notre session.\nVoici un exemple d’inclusion de méta-informations au début d’un script R\n\n# Title: Time series analysis of lasagna consumption\n\n# Purpose : This script performs a time series analyses on\n#           lasagna meals kids want to have each week.\n#           Data consists of counts of (dreamed) lasagna meals per week\n#           collected from 24 kids at the \"Food-dreaming\" school\n#           between 2042 and 2056.\n\n# data file: lasagna_dreams.csv\n\n# Author: A.\nStomach\n# Contact details: a.stomach@food.uni.com\n\n# Date script created: Fri Mar 29 17:06:44 2010 -----------\n# Date script last modified: Thu Dec 12 16:07:12 2019 ----\n\n# package dependencies\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nprint(\"put your lovely R code here\")\n\n# good practice to include session information\n\nxfun::session_info()\n\nCe n’est qu’un exemple et il n’y a pas de règles strictes, alors n’hésite pas à mettre au point un système qui te convient. Un raccourci très utile dans RStudio consiste à inclure automatiquement un horodatage dans ton script R. Pour ce faire, écris ts à l’endroit où tu veux insérer ton horodateur dans ton script R, puis appuie sur les touches ‘shift + tab’. RStudio convertira comme par magie ts en date et heure actuelles et commentera automatiquement cette ligne avec un #. Un autre raccourci RStudio très utile consiste à commenter plusieurs lignes de ton script à l’aide d’un # symbole. Pour ce faire, surligne les lignes de texte que tu veux commenter et appuie sur ‘ctrl + shift + c’ (ou ‘cmd + shift + c’ sur un mac). Pour décommenter les lignes, utilise à nouveau ‘ctrl + shift + c’.\nEn plus d’inclure des métadonnées dans tes scripts R, il est également courant de créer un fichier texte séparé pour enregistrer les informations importantes. Par convention, ces fichiers texte sont nommés README. Nous incluons souvent un README dans le répertoire où nous conservons nos données brutes. Dans ce fichier, nous incluons des détails sur le moment où les données ont été collectées (ou téléchargées), la manière dont elles ont été collectées, des informations sur le matériel spécialisé, les méthodes de conservation, le type et la version de toute machine utilisée (c’est-à-dire. équipement de séquençage), etc. Tu peux créer un fichier README pour ton projet dans RStudio en cliquant sur le bouton File -&gt; New File -&gt; Text File menu.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#guide-de-style-r",
    "href": "01-debut.html#guide-de-style-r",
    "title": "1  Pour commencer",
    "section": "\n1.9 Guide de style R",
    "text": "1.9 Guide de style R\nLa façon dont tu écris ton code dépend plus ou moins de toi, même si ton objectif doit être de le rendre aussi facile à lire que possible (pour toi et pour les autres). Bien qu’il n’y ait pas de règles (ni de police du code), nous t’encourageons à prendre l’habitude d’écrire un code R lisible en adoptant un style particulier. Nous te suggérons de suivre le guide de style Rde Google style-google dans la mesure du possible. Ce guide de style t’aidera à décider où utiliser les espaces, comment indenter le code et comment utiliser les carrés. [ ] et du curly { } entre autres choses.\nPour t’aider à formater ton code :\n\nVSCode il y a un formateur intégré dans l’extension R pour VSCode. Il te suffit d’utiliser le raccourci clavier pour reformater joliment et automatiquement le code.\nPour RStudio, tu peux installer l’extension styler 📦 qui comprend un complément RStudio te permettant de remanier automatiquement le code sélectionné (ou des fichiers et des projets entiers) d’un simple clic de souris. Tu peux trouver plus d’informations sur le styler 📦y compris comment l’installer [ici][styliste]. Une fois installé, tu peux mettre en évidence le code que tu veux remanier, cliquer sur le bouton “Addins” en haut de RStudio et sélectionner l’option “Style Selection”. Voici un exemple de code R mal formaté\n\n\n\n\n\n\n\n\n\nMets maintenant le code en surbrillance et utilise l’option styler 📦 pour reformater\n\n\n\n\n\n\n\n\nPour produire un code joliment formaté",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#sauvegarde-des-projets",
    "href": "01-debut.html#sauvegarde-des-projets",
    "title": "1  Pour commencer",
    "section": "\n1.10 Sauvegarde des projets",
    "text": "1.10 Sauvegarde des projets\nNe sois pas cette personne qui perd des données et des analyses durement gagnées (et souvent coûteuses). Ne sois pas cette personne qui pense que ça ne m’arrivera jamais - ça arrivera ! Pense toujours au pire scénario absolu, à quelque chose qui te donne des sueurs froides en te réveillant la nuit, et fais tout ton possible pour que cela n’arrive jamais. Pour être clair, si tu comptes copier tes précieux fichiers sur un disque dur externe ou une clé USB, ce n’est… PAS une stratégie de sauvegarde efficace. Ces choses tournent mal tout le temps lorsque tu les glisses dans ton sac à dos ou ton “sac pour la vie” et que tu les trimballes entre ton bureau et ta maison. Même si tu les laisses branchés sur ton ordinateur, que se passe-t-il lorsque le bâtiment brûle (nous avons bien dit le pire des cas !) ?\nIdéalement, tes sauvegardes devraient être hors site et incrémentielles. Heureusement, il existe de nombreuses options pour sauvegarder tes fichiers. La première chose à faire est de chercher dans ton propre institut. La plupart (toutes ?) des universités disposent d’une forme de stockage en réseau qui devrait être facilement accessible et qui est également soutenue par un plan complet de reprise après sinistre. D’autres options incluent des services basés sur le cloud tels que Google Drive et Dropbox (pour n’en citer que quelques-uns), mais assure-toi que tu ne stockes pas de données sensibles sur ces services et que tu es à l’aise avec les politiques de confidentialité qui font souvent l’effet d’une douche froide.\nAlors que ces services sont plutôt bons pour stocker des fichiers, ils ne sont pas vraiment utiles pour les sauvegardes incrémentielles. Pour trouver les versions précédentes des fichiers, il faut souvent passer un temps fou à parcourir plusieurs fichiers nommés ‘final.doc’, ‘final_v2.doc’ et ‘final_usethisone.doc’ etc. jusqu’à ce que tu trouves celui que tu cherches. Le meilleur moyen que nous connaissons pour à la fois sauvegarder les fichiers et gérer les différentes versions des fichiers est d’utiliser Git et GitHub. Pour en savoir plus sur la façon dont tu peux utiliser RStudio, Git et GitHub ensemble, consulte le ?sec-github_r.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "01-debut.html#citation-de-r",
    "href": "01-debut.html#citation-de-r",
    "title": "1  Pour commencer",
    "section": "\n1.11 Citation de R",
    "text": "1.11 Citation de R\nDe nombreuses personnes ont investi énormément de temps et d’énergie pour faire de R l’excellent logiciel que tu utilises aujourd’hui. Si tu utilises R dans ton travail (et nous espérons que tu le feras), n’oublie pas de le citer. Pour obtenir la citation la plus récente de R, tu peux utiliser le site citation() fonction.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nSi tu veux citer un logiciel particulier que tu as utilisé pour l’analyse de tes données.\n\ncitation(package = \"here\")\n\nTo cite package 'here' in publications use:\n\n  Müller K (2020). _here: A Simpler Way to Find Your Files_. R package\n  version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {here: A Simpler Way to Find Your Files},\n    author = {Kirill Müller},\n    year = {2020},\n    note = {R package version 1.0.1},\n    url = {https://CRAN.R-project.org/package=here},\n  }\n\n\nPour citer plusieurs paquets, le paquet grateful 📦 est extrêmement utile. Voir Table 1 pour un exemple de résultat produit avec grateful.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Pour commencer</span>"
    ]
  },
  {
    "objectID": "02-introR.html",
    "href": "02-introR.html",
    "title": "\n2  Introduction à R\n",
    "section": "",
    "text": "2.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction à R</span>"
    ]
  },
  {
    "objectID": "02-introR.html#set-intro",
    "href": "02-introR.html#set-intro",
    "title": "\n2  Introduction à R\n",
    "section": "",
    "text": "les paquets R:\n\nquestionr\nggplot2\n\n\nles fichiers de données\n\nErablesGatineau.csv\nsturgeon.csv",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction à R</span>"
    ]
  },
  {
    "objectID": "02-introR.html#premier-pas-avec-r",
    "href": "02-introR.html#premier-pas-avec-r",
    "title": "\n2  Introduction à R\n",
    "section": "\n2.2 Premier pas avec R",
    "text": "2.2 Premier pas avec R\nUne fois R et RStudio installés sur votre machine, nous n’allons pas lancer R mais plutôt RStudio.\nRStudio n’est pas à proprement parler une interface graphique qui permettrait d’utiliser R de manière “classique” via la souris, des menus et des boîtes de dialogue. Il s’agit plutôt de ce qu’on appelle un Environnement de développement intégré (IDE) qui facilite l’utilisation de R et le développement de scripts.\n\n2.2.1 La console\n\n2.2.1.1 L’invite de commandes\nAu premier lancement de RStudio, l’écran principal est découpé en trois grandes zones :\n\n\n\n\n\nInterface de RStudio\n\n\n\nLa zone de gauche se nomme Console. À son démarrage, RStudio a lancé une nouvelle session de R et c’est dans cette fenêtre que nous allons pouvoir interagir avec lui.\nLa Console doit normalement afficher un texte de bienvenue ressemblant à ceci :\nR version 4.0.2 (2020-06-22) -- \"Taking Off Again\"\nCopyright (C) 2020 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR est un logiciel libre livré sans AUCUNE GARANTIE.\nVous pouvez le redistribuer sous certaines conditions.\nTapez 'license()' ou 'licence()' pour plus de détails.\n\nR est un projet collaboratif avec de nombreux contributeurs.\nTapez 'contributors()' pour plus d'information et\n'citation()' pour la façon de le citer dans les publications.\n\nTapez 'demo()' pour des démonstrations, 'help()' pour l'aide\nen ligne ou 'help.start()' pour obtenir l'aide au format HTML.\nTapez 'q()' pour quitter R.\nsuivi d’une ligne commençant par le caractère &gt; et sur laquelle devrait se trouver votre curseur. Cette ligne est appelée l’invite de commande (ou prompt en anglais). Elle signifie que R est disponible et en attente de votre prochaine commande.\nNous pouvons tout de suite lui fournir une première commande, en saisissant le texte suivant puis en appuyant sur Entrée :\n\n2 + 2\n\n[1] 4\n\n\nR nous répond immédiatement, et nous pouvons constater avec soulagement qu’il sait faire des additions à un chiffre1. On peut donc continuer avec d’autres opérations :\n\n5 - 7\n\n[1] -2\n\n4 * 12\n\n[1] 48\n\n-10 / 3\n\n[1] -3.333333\n\n5^2\n\n[1] 25\n\n\nCette dernière opération utilise le symbole ^ qui représente l’opération puissance. 5^2 signifie donc “5 au carré”, soit 25.\n\n2.2.1.2 Précisions concernant la saisie des commandes\nLorsqu’on saisit une commande, les espaces autour des opérateurs n’ont pas d’importance. Les trois commandes suivantes sont donc équivalentes, mais on privilégie en général la deuxième pour des raisons de lisibilité du code.\n\n10+2\n10 + 2\n10       +       2\n\nQuand vous êtes dans la console, vous pouvez utiliser les flèches vers le haut et vers le bas pour naviguer dans l’historique des commandes que vous avez tapées précédemment. Vous pouvez à tout moment modifier la commande affichée, et l’exécuter en appuyant sur Entrée.\nEnfin, il peut arriver qu’on saisisse une commande de manière incomplète : oubli d’une parenthèse, faute de frappe, etc. Dans ce cas, R remplace l’invite de commande habituel par un signe + :\n\n4 *\n+\n\nCela signifie qu’il “attend la suite”. On peut alors soit compléter la commande sur cette nouvelle ligne et appuyer sur Entrée, soit, si on est perdu, tout annuler et revenir à l’invite de commandes normal en appuyant sur Esc ou Échap.\n\n2.2.2 Objets\n\n2.2.2.1 Objets simples\nFaire des calculs c’est bien, mais il serait intéressant de pouvoir stocker un résultat quelque part pour pouvoir le réutiliser ultérieurement sans avoir à faire du copier/coller.\nPour conserver le résultat d’une opération, on peut le stocker dans un objet à l’aide de l’opérateur d’assignation &lt;-. Cette “flèche” stocke ce qu’il y a à sa droite dans un objet dont le nom est indiqué à sa gauche.\nPrenons tout de suite un exemple :\n\nx &lt;- 2\n\nCette commande peut se lire “prend la valeur 2 et mets la dans un objet qui s’appelle x”.\nSi on exécute une commande comportant juste le nom d’un objet, R affiche son contenu :\n\nx\n\n[1] 2\n\n\nOn voit donc que notre objet x contient bien la valeur 2.\nOn peut évidemment réutiliser cet objet dans d’autres opérations. R le remplacera alors par sa valeur :\n\nx + 4\n\n[1] 6\n\n\nOn peut créer autant d’objets qu’on le souhaite.\n\nx &lt;- 2\ny &lt;- 5\nresultat &lt;- x + y\nresultat\n\n[1] 7\n\n\n\n\n\n\n\n\nImportant\n\n\n\nLes noms d’objets peuvent contenir des lettres, des chiffres, les symboles . et _. Ils ne peuvent pas commencer par un chiffre. Attention, R fait la différence entre minuscules et majuscules dans les noms d’objets, ce qui signifie que x et X seront deux objets différents, tout comme resultat et Resultat.\nDe manière générale, il est préférable d’éviter les majuscules (pour les risques d’erreur) et les caractères accentués (pour des questions d’encodage) dans les noms d’objets.\nDe même, il faut essayer de trouver un équilibre entre clarté du nom (comprendre à quoi sert l’objet, ce qu’il contient) et sa longueur. Par exemple, on préfèrera comme nom d’objet taille_conj1 à taille_du_conjoint_numero_1 (trop long) ou à t1 (pas assez explicite).\n\n\nQuand on assigne une nouvelle valeur à un objet déjà existant, la valeur précédente est perdue. Les objets n’ont pas de mémoire.\n\nx &lt;- 2\nx &lt;- 5\nx\n\n[1] 5\n\n\nDe la même manière, assigner un objet à un autre ne crée pas de “lien” entre les deux. Cela copie juste la valeur de l’objet de droite dans celui de gauche :\n\nx &lt;- 1\ny &lt;- 3\nx &lt;- y\nx\n\n[1] 3\n\n## Si on modifie y, cela ne modifie pas x\ny &lt;- 4\nx\n\n[1] 3\n\n\nOn le verra, les objets peuvent contenir tout un tas d’informations. Jusqu’ici on n’a stocké que des nombres, mais ils peuvent aussi contenir des chaînes de caractères (du texte), qu’on délimite avec des guillemets simples ou doubles (' ou \") :\n\nchien &lt;- \"Chihuahua\"\nchien\n\n[1] \"Chihuahua\"\n\n\n\n2.2.3 Vecteurs\nImaginons maintenant qu’on a demandé la taille en centimètres de 5 personnes et qu’on souhaite calculer leur taille moyenne. On pourrait créer autant d’objets que de tailles et faire l’opération mathématique qui va bien :\n\ntaille1 &lt;- 156\ntaille2 &lt;- 164\ntaille3 &lt;- 197\ntaille4 &lt;- 147\ntaille5 &lt;- 173\n(taille1 + taille2 + taille3 + taille4 + taille5) / 5\n\n[1] 167.4\n\n\nCette manière de faire n’est évidemment pas pratique du tout. On va plutôt stocker l’ensemble de nos tailles dans un seul objet, de type vecteur, avec la syntaxe suivante :\n\ntailles &lt;- c(156, 164, 197, 147, 173)\n\nSi on affiche le contenu de cet objet, on voit qu’il contient bien l’ensemble des tailles saisies :\n\ntailles\n\n[1] 156 164 197 147 173\n\n\nUn vecteur dans R est un objet qui peut contenir plusieurs informations du même type, potentiellement en très grand nombre.\nL’avantage d’un vecteur est que lorsqu’on lui applique une opération, celle-ci s’applique à toutes les valeurs qu’il contient. Ainsi, si on veut la taille en mètres plutôt qu’en centimètres, on peut faire :\n\ntailles_m &lt;- tailles / 100\ntailles_m\n\n[1] 1.56 1.64 1.97 1.47 1.73\n\n\nCela fonctionne pour toutes les opérations de base :\n\ntailles + 10\n\n[1] 166 174 207 157 183\n\ntailles^2\n\n[1] 24336 26896 38809 21609 29929\n\n\nImaginons maintenant qu’on a aussi demandé aux cinq mêmes personnes leur poids en kilos. On peut alors créer un deuxième vecteur :\n\npoids &lt;- c(45, 59, 110, 44, 88)\n\nOn peut alors effectuer des calculs utilisant nos deux vecteurs tailles et poids. On peut par exemple calculer l’indice de masse corporelle (IMC) de chacun de nos enquêtés en divisant leur poids en kilo par leur taille en mètre au carré :\n\nimc &lt;- poids / (tailles / 100) ^ 2\nimc\n\n[1] 18.49112 21.93635 28.34394 20.36189 29.40292\n\n\nUn vecteur peut contenir des nombres, mais il peut aussi contenir du texte. Imaginons qu’on a demandé aux 5 mêmes personnes leur niveau de diplôme : on peut regrouper l’information dans un vecteur de chaînes de caractères. Une chaîne de caractère contient du texte libre, délimité par des guillemets simples ou doubles :\n\ndiplome &lt;- c(\"PHD\", \"Bac\", \"MSc\", \"MSc\", \"Bac\")\ndiplome\n\n[1] \"PHD\" \"Bac\" \"MSc\" \"MSc\" \"Bac\"\n\n\nL’opérateur :, lui, permet de générer rapidement un vecteur comprenant tous les nombres entre deux valeurs, opération assez courante sous R :\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nEnfin, notons qu’on peut accéder à un élément particulier d’un vecteur en faisant suivre le nom du vecteur de crochets contenant le numéro de l’élément désiré. Par exemple :\n\ndiplome[2]\n\n[1] \"Bac\"\n\n\nCette opération, qui utilise l’opérateur [], permet donc la sélection d’éléments d’un vecteur.\nDernière remarque, si on affiche dans la console un vecteur avec beaucoup d’éléments, ceux-ci seront répartis sur plusieurs lignes. Par exemple, si on a un vecteur de 50 nombres on peut obtenir quelque chose comme :\n [1] 294 425 339 914 114 896 716 648 915 587 181 926 489\n[14] 848 583 182 662 888 417 133 146 322 400 698 506 944\n[27] 237 324 333 443 487 658 793 288 897 588 697 439 697\n[40] 914 694 126 969 744 927 337 439 226 704 635\nOn remarque que R ajoute systématiquement un nombre entre crochets au début de chaque ligne : il s’agit en fait de la position du premier élément de la ligne dans le vecteur. Ainsi, le 848 de la deuxième ligne est le 14e élément du vecteur, le 914 de la dernière ligne est le 40e, etc.\nCeci explique le [1] qu’on obtient quand on affiche un simple nombre et permet de constater que pour R, un nombre est un vecteur à un seul élément :\n [1] 4\n\n2.2.4 Fonctions\n\n2.2.4.1 Principe\nNous savons désormais effectuer des opérations arithmétiques de base sur des nombres et des vecteurs, et stocker des valeurs dans des objets pour pouvoir les réutiliser plus tard.\nPour aller plus loin, nous devons aborder les fonctions qui sont, avec les objets, un deuxième concept de base de R. On utilise des fonctions pour effectuer des calculs, obtenir des résultats et accomplir des actions.\nFormellement, une fonction a un nom, elle prend en entrée entre parenthèses un ou plusieurs arguments (ou paramètres), et retourne un résultat.\nPrenons tout de suite un exemple. Si on veut connaître le nombre d’éléments du vecteur tailles que nous avons construit précédemment, on peut utiliser la fonction length, de cette manière :\n\nlength(tailles)\n\n[1] 5\n\n\nIci, length est le nom de la fonction, on l’appelle en lui passant un argument entre parenthèses (en l’occurrence notre vecteur tailles), et elle nous renvoie un résultat, à savoir le nombre d’éléments du vecteur passé en paramètre.\nAutre exemple, les fonctions min et max retournent respectivement les valeurs minimales et maximales d’un vecteur de nombres :\n\nmin(tailles)\n\n[1] 147\n\nmax(tailles)\n\n[1] 197\n\n\nLa fonction mean calcule et retourne la moyenne d’un vecteur de nombres :\n\nmean(tailles)\n\n[1] 167.4\n\n\nLa fonction sum retourne la somme de tous les éléments du vecteur :\n\nsum(tailles)\n\n[1] 837\n\n\nJusqu’à présent on n’a vu que des fonctions qui calculent et retournent un unique nombre. Mais une fonction peut renvoyer d’autres types de résultats. Par exemple, la fonction range (étendue) renvoie un vecteur de deux nombres, le minimum et le maximum :\n\nrange(tailles)\n\n[1] 147 197\n\n\nOu encore, la fonction unique, qui supprime toutes les valeurs en double dans un vecteur, qu’il s’agisse de nombres ou de chaînes de caractères :\n\ndiplome\n\n[1] \"PHD\" \"Bac\" \"MSc\" \"MSc\" \"Bac\"\n\nunique(diplome)\n\n[1] \"PHD\" \"Bac\" \"MSc\"\n\n\n\n2.2.4.2 Arguments\nUne fonction peut prendre plusieurs arguments, dans ce cas on les indique toujours entre parenthèses, séparés par des virgules.\nOn a déjà rencontré un exemple de fonction acceptant plusieurs arguments : la fonction c, qui combine l’ensemble de ses arguments en un vecteur2 :\n\ntailles &lt;- c(156, 164, 197, 181, 173)\n\nIci, c est appelée en lui passant cinq arguments, les cinq tailles séparées par des virgules, et elle renvoie un vecteur numérique regroupant ces cinq valeurs.\nSupposons maintenant que dans notre vecteur tailles nous avons une valeur manquante (une personne a refusé de répondre, ou notre mètre mesureur était en panne). On symbolise celle-ci dans R avec le code interne NA :\n\ntailles &lt;- c(156, 164, 197, NA, 173)\ntailles\n\n[1] 156 164 197  NA 173\n\n\n\n\n\n\n\n\nNote\n\n\n\nNA est l’abbréviation de Not available, non disponible. Cette valeur particulière peut être utilisée pour indiquer une valeur manquante, qu’il s’agisse d’un nombre, d’une chaîne de caractères, etc.\n\n\nSi je calcule maintenant la taille moyenne à l’aide de la fonction mean, j’obtiens :\n\nmean(tailles)\n\n[1] NA\n\n\nEn effet, R considère par défaut qu’il ne peut pas calculer la moyenne si une des valeurs n’est pas disponible. Il considère alors que cette moyenne est elle-même “non disponible” et renvoie donc comme résultat NA.\nOn peut cependant indiquer à mean d’effectuer le calcul en ignorant les valeurs manquantes. Ceci se fait en ajoutant un argument supplémentaire, nommé na.rm (abbréviation de NA remove, “enlever les NA”), et de lui attribuer la valeur TRUE (code interne de R signifiant vrai) :\n\nmean(tailles, na.rm = TRUE)\n\n[1] 172.5\n\n\nPositionner le paramètre na.rm à TRUE indique à la fonction mean de ne pas tenir compte des valeurs manquantes dans le calcul.\nSi on ne dit rien à la fonction mean, cet argument a une valeur par défaut, en l’occurrence FALSE (faux), qui fait qu’il ne supprime pas les valeurs manquantes. Les deux commandes suivantes sont donc rigoureusement équivalentes :\n\nmean(tailles)\n\n[1] NA\n\nmean(tailles, na.rm = FALSE)\n\n[1] NA\n\n\n\n\n\n\n\n\nNote\n\n\n\nLorsqu’on passe un argument à une fonction de cette manière, c’est-à-dire sous la forme nom = valeur, on parle d’argument nommé.\n\n\n\n2.2.4.3 Aide sur une fonction\nIl est fréquent de ne pas savoir (ou d’avoir oublié) quels sont les arguments d’une fonction, ou comment ils se nomment. On peut à tout moment faire appel à l’aide intégrée à R en passant le nom de la fonction (entre guillemets) à la fonction help :\n\nhelp(\"mean\")\n\nOn peut aussi utiliser le raccourci ?mean.\nCes deux commandes affichent une page (en anglais) décrivant la fonction, ses paramètres, son résultat, le tout accompagné de diverses notes, références et exemples. Ces pages d’aide contiennent à peu près tout ce que vous pourrez chercher à savoir, mais elles ne sont pas toujours d’une lecture aisée.\nDans RStudio, les pages d’aide en ligne s’ouvriront par défaut dans la zone en bas à droite, sous l’onglet Help. Un clic sur l’icône en forme de maison vous affichera la page d’accueil de l’aide.\n\n2.2.5 Regrouper ses commandes dans des scripts\nJusqu’ici on a utilisé R de manière “interactive”, en saisissant des commandes directement dans la console. Ça n’est cependant pas la manière dont on va utiliser R au quotidien, pour une raison simple : lorsque R redémarre, tout ce qui a été effectué dans la console est perdu.\nPlutôt que de saisir nos commandes dans la console, on va donc les regrouper dans des scripts (de simples fichiers texte), qui vont garder une trace de toutes les opérations effectuées, et ce sont ces scripts, sauvegardés régulièrement, qui seront le “coeur” de notre travail. C’est en rouvrant les scripts et en réexécutant les commandes qu’ils contiennent qu’on pourra “reproduire” les données, leur traitement, les analyses et leurs résultats.\nPour créer un script, il suffit de sélectionner le menu File, puis New file et R script. Une quatrième zone apparaît alors en haut à gauche de l’interface de RStudio. On peut enregistrer notre script à tout moment dans un fichier avec l’extension .R, en cliquant sur l’icône de disquette ou en choissant File puis Save.\nUn script est un fichier texte brut, qui s’édite de la manière habituelle. À la différence de la console, quand on appuie sur Entrée, cela n’exécute pas la commande en cours mais insère un saut de ligne (comme on pouvait s’y attendre).\nPour exécuter une commande saisie dans un script, il suffit de positionner le curseur sur la ligne de la commande en question, et de cliquer sur le bouton Run dans la barre d’outils juste au-dessus de la zone d’édition du script. On peut aussi utiliser le raccourci clavier Ctrl + Entrée (Cmd + Entrée sous Mac). On peut enfin sélectionner plusieurs lignes avec la souris ou le clavier et cliquer sur Run (ou utiliser le raccourci clavier), et l’ensemble des lignes est exécuté d’un coup.\nAu final, un script pourra ressembler à quelque chose comme ça :\n\ntailles &lt;- c(156, 164, 197, 147, 173)\npoids &lt;- c(45, 59, 110, 44, 88)\n\nmean(tailles)\nmean(poids)\n\nimc &lt;- poids / (tailles / 100) ^ 2\nmin(imc)\nmax(imc)\n\n\n2.2.5.1 Commentaires\nLes commentaires sont un élément très important d’un script. Il s’agit de texte libre, ignoré par R, et qui permet de décrire les étapes du script, sa logique, les raisons pour lesquelles on a procédé de telle ou telle manière… Il est primordial de documenter ses scripts à l’aide de commentaires, car il est très facile de ne plus se retrouver dans un programme qu’on a produit soi-même, même après une courte interruption.\nPour ajouter un commentaire, il suffit de le faire précéder d’un ou plusieurs symboles #. En effet, dès que R rencontre ce caractère, il ignore tout ce qui se trouve derrière, jussqu’à la fin de la ligne.\nOn peut donc documenter le script précédent :\n\n# Saisie des tailles et poids des enquêtés\ntailles &lt;- c(156, 164, 197, 147, 173)\npoids &lt;- c(45, 59, 110, 44, 88)\n\n# Calcul des tailles et poids moyens\nmean(tailles)\nmean(poids)\n\n# Calcul de l'IMC (poids en kilo divisé par les tailles en mètre au carré)\nimc &lt;- poids / (tailles / 100) ^ 2\n# Valeurs extrêmes de l'IMC\nmin(imc)\nmax(imc)\n\n\n2.2.6 Installer et charger des extensions (packages)\nR étant un logiciel libre, il bénéficie d’un développement communautaire riche et dynamique. L’installation de base de R permet de faire énormément de choses, mais le langage dispose en plus d’un système d’extensions permettant d’ajouter facilement de nouvelles fonctionnalités. La plupart des extensions sont développées et maintenues par la communauté des utilisateurs de R, et diffusées via un réseau de serveurs nommé CRAN (Comprehensive R Archive Network).\nPour installer une extension, si on dispose d’une connexion Internet, on peut utiliser le bouton Install de l’onglet Packages de RStudio.\n\n\n\n\n\nInstaller une extension\n\n\n\nIl suffit alors d’indiquer le nom de l’extension dans le champ Package et de cliquer sur Install.\n\n\n\n\n\nInstallation une extension\n\n\n\nimages/screenshots/rstudio_package_install.png On peut aussi installer des extensions en utilisant la fonction install.packages() directement dans la console. Par exemple, pour installer le package questionr on peut exécuter la commande :\n\ninstall.packages(\"questionr\")\n\nInstaller une extension via l’une des deux méthodes précédentes va télécharger l’ensemble des fichiers nécessaires depuis l’une des machines du CRAN, puis installer tout ça sur le disque dur de votre ordinateur. Vous n’avez besoin de le faire qu’une fois, comme vous le faites pour installer un programme sur votre Mac ou PC.\nUne fois l’extension installée, il faut la “charger” avant de pouvoir utiliser les fonctions qu’elle propose. Ceci se fait avec la fonction library. Par exemple, pour pouvoir utiliser les fonctions de questionr, vous devrez exécuter la commande suivante :\n\nlibrary(questionr)\n\nAinsi, bien souvent, on regroupe en début de script toute une série d’appels à library qui permettent de charger tous les packages utilisés dans le script. Quelque chose comme :\n\nlibrary(readxl)\nlibrary(ggplot2)\nlibrary(questionr)\n\nSi vous essayez d’exécuter une fonction d’une extension et que vous obtenez le message d’erreur impossible de trouver la fonction, c’est certainement parce que vous n’avez pas exécuté la commande library correspondante.\n\n2.2.7 Exercices\n\n2.2.7.1 Exercice 1\nConstruire le vecteur x suivant :\n\n\n[1] 120 134 256  12\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- c(120, 134, 256, 12)\n\n\n\n\nUtiliser ce vecteur x pour générer les deux vecteurs suivants :\n\n\n[1] 220 234 356 112\n\n\n[1] 240 268 512  24\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx + 100\nx * 2\n\n\n\n\n\n2.2.7.2 Exercice 2\nOn a demandé à 4 ménages le revenu des deux conjoints, et le nombre de personnes du ménage :\n\nconjoint1 &lt;- c(1200, 1180, 1750, 2100)\nconjoint2 &lt;- c(1450, 1870, 1690, 0)\nnb_personnes &lt;- c(4, 2, 3, 2)\n\nCalculer le revenu total de chaque ménage, puis diviser par le nombre de personnes pour obtenir le revenu par personne de chaque ménage.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrevenu_total &lt;- conjoint1 + conjoint2\nrevenu_total / nb_personnes\n\n\n\n\n\n2.2.7.3 Exercice 3\nDans l’exercice précédent, calculer le revenu minimum et maximum parmi ceux du premier conjoint.\n\nconjoint1 &lt;- c(1200, 1180, 1750, 2100)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrange(conjoint1)\n\n\n\n\nRecommencer avec les revenus suivants, parmi lesquels l’un des enquetés n’a pas voulu répondre :\n\nconjoint1 &lt;- c(1200, 1180, 1750, NA)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrange(conjoint1, na.rm = TRUE)\n\n\n\n\n\n2.2.7.4 Exercice 4\nLes deux vecteurs suivants représentent les précipitations (en mm) et la température (en °C) moyennes sur la ville de Lyon, pour chaque mois de l’année, entre 1981 et 2010 :\n\ntemperature &lt;- c(3.4, 4.8, 8.4, 11.4, 15.8, 19.4, 22.2, 21.6, 17.6, 13.4, 7.6, 4.4)\nprecipitations &lt;- c(47.2, 44.1, 50.4, 74.9, 90.8, 75.6, 63.7, 62, 87.5, 98.6, 81.9, 55.2)\n\nCalculer la température moyenne sur l’année.\nCalculer la quantité totale de précipitations sur l’année.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(temperature)\nsum(precipitations)\n\n\n\n\nÀ quoi correspond et comment peut-on interpréter le résultat de la fonction suivante ? Vous pouvez vous aider de la page d’aide de la fonction si nécessaire.\n\ncumsum(precipitations)\n\n [1]  47.2  91.3 141.7 216.6 307.4 383.0 446.7 508.7 596.2 694.8 776.7 831.9\n\n\nMême question pour :\n\ndiff(temperature)\n\n [1]  1.4  3.6  3.0  4.4  3.6  2.8 -0.6 -4.0 -4.2 -5.8 -3.2\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ncumsum(precipitations) correspond à la somme cumulée des précipitations sur l’année. Par exemple, la 6e valeur du vecteur résultat correspond au total de précipitations de janvier à juin.\ndiff(temperature) correspond à la différence de température d’un mois sur l’autre. Par exemple, la 2e valeur de ce vecteur correspond à l’écart de température entre le mois de février et le mois de janvier.\n\n\n\n\n2.2.7.5 Exercice 5\nOn a relevé les notes en maths, anglais et sport d’une classe de 6 élèves et on a stocké ces données dans trois vecteurs :\n\nmaths &lt;- c(12, 16, 8, 18, 6, 10)\nanglais &lt;- c(14, 9, 13, 15, 17, 11)\nsport &lt;- c(18, 11, 14, 10, 8, 12)\n\nCalculer la moyenne des élèves de la classe en anglais.\nCalculer la moyenne générale de chaque élève.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(anglais)\n(maths + anglais + sport) / 3\n\n\n\n\nEssayez de comprendre le résultat des deux fonctions suivantes (vous pouvez vous aider de la page d’aide de ces fonctions) :\n\npmin(maths, anglais, sport)\n\n[1] 12  9  8 10  6 10\n\n\n\npmax(maths, anglais, sport)\n\n[1] 18 16 14 18 17 12\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\npmin et pmax renvoient les minimum et maximum “parallèles” des trois vecteurs passés en argument. Ainsi, pmin renvoie pour chaque élève la note minimale dans les trois matières, et pmax la note maximale.",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction à R</span>"
    ]
  },
  {
    "objectID": "02-introR.html#premier-travail-avec-des-données",
    "href": "02-introR.html#premier-travail-avec-des-données",
    "title": "\n2  Introduction à R\n",
    "section": "\n2.3 Premier travail avec des données",
    "text": "2.3 Premier travail avec des données\n\n2.3.1 Jeu de données d’exemple\nDans cette partie nous allons (enfin) travailler sur des “vraies” données, et utiliser un jeu de données présent dans l’extension questionr. Nous devons donc avant toute chose installer cette extension.\nPour installer ce package, deux possibilités :\n\nDans l’onglet Packages de la zone de l’écran en bas à droite, cliquez sur le bouton Install. Dans le dialogue qui s’ouvre, entrez “questionr” dans le champ Packages puis cliquez sur Install.\nSaisissez directement la commande suivante dans la console : install.packages(\"questionr\")\n\n\nDans les deux cas, tout un tas de messages devraient s’afficher dans la console. Attendez que l’invite de commandes &gt; apparaisse à nouveau.\nPour plus d’informations sur les extensions et leur installation, voir la section @ref(packages).\nLe jeu de données que nous allons utiliser est un extrait de l’enquête Histoire de vie réalisée par l’INSEE en 2003. Il contient 2000 individus et 20 variables.\nPour pouvoir utiliser ces données, il faut d’abord charger l’extension questionr (après l’avoir installée, bien entendu) :\n\nlibrary(questionr)\n\nL’utilisation de library permet de rendre “disponibles”, dans notre session R, les fonctions et jeux de données inclus dans l’extension.\nNous devons ensuite indiquer à R que nous souhaitons accéder au jeu de données à l’aide de la commande data :\n\ndata(hdv2003)\n\nCette commande ne renvoie aucun résultat particulier (sauf en cas d’erreur), mais vous devriez voir apparaître dans l’onglet Environment de RStudio un nouvel objet nommé hdv2003 :\n\n\n\n\n\nOnglet Environment\n\n\n\nCet objet est d’un type nouveau : il s’agit d’un tableau de données.\n\n2.3.2 Tableau de données (data frame)\nUn data frame (ou tableau de données, ou table) est un type d’objet R qui contient des données au format tabulaire, avec les observations en ligne et les variables en colonnes, comme dans une feuille de tableur de type LibreOffice ou Excel.\nSi on se contente d’exécuter le nom de notre tableau de données :\n\nhdv2003\n\nR va, comme à son habitude, nous l’afficher dans la console, ce qui est tout sauf utile.\nUne autre manière d’afficher le contenu du tableau est de cliquer sur l’icône en forme de tableau à droite du nom de l’objet dans l’onglet Environment :\n\n\n\n\n\nIcone view\n\n\n\nOu d’utiliser la fonction View :\n\nView(hdv2003)\n\nDans les deux cas votre tableau devrait s’afficher dans RStudio avec une interface de type tableur :\n\n\n\n\n\nInterface View\n\n\n\nIl est important de comprendre que l’objet hdv2003 contient l’intégralité des données du tableau. On voit donc qu’un objet peut contenir des données de types très différents (simple nombre, texte, vecteur, tableau de données entier), et être potentiellement de très grande taille3.\n\n\n\n\n\n\nNote\n\n\n\nSous R, on peut importer ou créer autant de tableaux de données qu’on le souhaite, dans les limites des capacités de sa machine.\n\n\nUn data frame peut être manipulé comme les autres objets vus précédemment. On peut par exemple faire :\n\nd &lt;- hdv2003\n\nce qui va entraîner la copie de l’ensemble de nos données dans un nouvel objet nommé d. Ceci peut paraître parfaitement inutile mais a en fait l’avantage de fournir un objet avec un nom beaucoup plus court, ce qui diminuera la quantité de texte à saisir par la suite.\nPour résumer, comme nous avons désormais décidé de saisir nos commandes dans un script et non plus directement dans la console, les premières lignes de notre fichier de travail sur les données de l’enquête Histoire de vie pourraient donc ressembler à ceci :\n\n## Chargement des extensions nécessaires\nlibrary(questionr)\n\n## Jeu de données hdv2003\ndata(hdv2003)\nd &lt;- hdv2003\n\n\n2.3.2.1 Structure du tableau\nUn tableau étant un objet comme un autre, on peut lui appliquer des fonctions. Par exemple, nrow et ncol retournent le nombre de lignes et de colonnes du tableau :\n\nnrow(d)\n\n[1] 2000\n\n\n\nncol(d)\n\n[1] 20\n\n\nLa fonction dim renvoie ses dimensions, donc les deux nombres précédents :\n\ndim(d)\n\n[1] 2000   20\n\n\nLa fonction names retourne les noms des colonnes du tableau, c’est-à-dire la liste de nos variables :\n\nnames(d)\n\n [1] \"id\"            \"age\"           \"sexe\"          \"nivetud\"      \n [5] \"poids\"         \"occup\"         \"qualif\"        \"freres.soeurs\"\n [9] \"clso\"          \"relig\"         \"trav.imp\"      \"trav.satisf\"  \n[13] \"hard.rock\"     \"lecture.bd\"    \"peche.chasse\"  \"cuisine\"      \n[17] \"bricol\"        \"cinema\"        \"sport\"         \"heures.tv\"    \n\n\nEnfin, la fonction str renvoie un descriptif plus détaillé de la structure du tableau. Elle liste les différentes variables, indique leur type 4 et affiche les premières valeurs :\n\nstr(d)\n\n'data.frame':   2000 obs. of  20 variables:\n $ id           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ age          : int  28 23 59 34 71 35 60 47 20 28 ...\n $ sexe         : Factor w/ 2 levels \"Homme\",\"Femme\": 2 2 1 1 2 2 2 1 2 1 ...\n $ nivetud      : Factor w/ 8 levels \"N'a jamais fait d'etudes\",..: 8 NA 3 8 3 6 3 6 NA 7 ...\n $ poids        : num  2634 9738 3994 5732 4329 ...\n $ occup        : Factor w/ 7 levels \"Exerce une profession\",..: 1 3 1 1 4 1 6 1 3 1 ...\n $ qualif       : Factor w/ 7 levels \"Ouvrier specialise\",..: 6 NA 3 3 6 6 2 2 NA 7 ...\n $ freres.soeurs: int  8 2 2 1 0 5 1 5 4 2 ...\n $ clso         : Factor w/ 3 levels \"Oui\",\"Non\",\"Ne sait pas\": 1 1 2 2 1 2 1 2 1 2 ...\n $ relig        : Factor w/ 6 levels \"Pratiquant regulier\",..: 4 4 4 3 1 4 3 4 3 2 ...\n $ trav.imp     : Factor w/ 4 levels \"Le plus important\",..: 4 NA 2 3 NA 1 NA 4 NA 3 ...\n $ trav.satisf  : Factor w/ 3 levels \"Satisfaction\",..: 2 NA 3 1 NA 3 NA 2 NA 1 ...\n $ hard.rock    : Factor w/ 2 levels \"Non\",\"Oui\": 1 1 1 1 1 1 1 1 1 1 ...\n $ lecture.bd   : Factor w/ 2 levels \"Non\",\"Oui\": 1 1 1 1 1 1 1 1 1 1 ...\n $ peche.chasse : Factor w/ 2 levels \"Non\",\"Oui\": 1 1 1 1 1 1 2 2 1 1 ...\n $ cuisine      : Factor w/ 2 levels \"Non\",\"Oui\": 2 1 1 2 1 1 2 2 1 1 ...\n $ bricol       : Factor w/ 2 levels \"Non\",\"Oui\": 1 1 1 2 1 1 1 2 1 1 ...\n $ cinema       : Factor w/ 2 levels \"Non\",\"Oui\": 1 2 1 2 1 2 1 1 2 2 ...\n $ sport        : Factor w/ 2 levels \"Non\",\"Oui\": 1 2 2 2 1 2 1 1 1 2 ...\n $ heures.tv    : num  0 1 0 2 3 2 2.9 1 2 2 ...\n\n\nSous RStudio, on peut afficher à tout moment la structure d’un objet en cliquant sur l’icône de triangle sur fond bleu à gauche du nom de l’objet dans l’onglet Environment :\n\n\n\n\n\nStructure d’un objet\n\n\n\n\n2.3.2.2 Accéder aux variables d’un tableau\nUne opération très importante est l’accès aux variables du tableau (à ses colonnes) pour pouvoir les manipuler, effectuer des calculs, etc. On utilise pour cela l’opérateur $, qui permet d’accéder aux colonnes du tableau. Ainsi, si l’on tape :\n\nd$sexe\n\n  [1] Femme Femme Homme Homme Femme Femme Femme Homme Femme Homme Femme Homme\n [13] Femme Femme Femme Femme Homme Femme Homme Femme Femme Homme Femme Femme\n [25] Femme Homme Femme Homme Homme Homme Homme Homme Homme Homme Femme Femme\n [37] Homme Femme Femme Homme Femme Homme Homme Femme Femme Homme Femme Femme\n [49] Femme Femme Homme Femme Homme Femme Homme Femme Femme Femme Homme Femme\n [61] Femme Homme Homme Homme Homme Femme Homme Homme Femme Femme Homme Homme\n [73] Femme Femme Femme Femme Homme Femme Femme Femme Femme Femme Femme Homme\n [85] Homme Femme Homme Homme Homme Homme Homme Femme Homme Femme Femme Femme\n [97] Homme Homme Femme Femme Femme Homme Femme Homme Homme Femme Femme Femme\n[109] Femme Homme Homme Homme Homme Homme Femme Homme Homme Femme Homme Homme\n[121] Femme Femme Femme Homme Femme Femme Homme Femme Femme Homme Femme Homme\n[133] Femme Femme Femme Homme Homme Homme Homme Homme Homme Homme Homme Femme\n[145] Homme Homme Homme Femme Femme Femme Homme Femme Femme Femme Femme Homme\n[157] Femme Homme Homme Homme Femme Homme Femme Homme Femme Homme Homme Femme\n[169] Femme Femme Homme Femme Homme Femme Femme Femme Homme Homme Homme Femme\n[181] Homme Femme Femme Homme Homme Femme Femme Femme Femme Femme Homme Homme\n[193] Femme Homme Homme Femme Homme Femme Homme Femme\n [ reached getOption(\"max.print\") -- omitted 1800 entries ]\nLevels: Homme Femme\n\n\nR va nous afficher l’ensemble des valeurs de notre variable sexe dans la console, ce qui est à nouveau fort peu utile. Mais cela nous permet de constater que d$sexe est un vecteur de chaînes de caractères tels qu’on en a déjà rencontré précédemment.\nLa fonction table$colonne renvoie donc la colonne nommée colonne du tableau table, c’est-à-dire un vecteur, en général de nombres ou de chaînes de caractères.\nSi on souhaite afficher seulement les premières ou dernières valeurs d’une variable, on peut utiliser les fonctions head et tail :\n\nhead(d$age)\n\n[1] 28 23 59 34 71 35\n\n\n\ntail(d$age, 10)\n\n [1] 52 42 50 41 46 45 46 24 24 66\n\n\nLe deuxième argument numérique permet d’indiquer le nombre de valeurs à afficher.\n\n2.3.2.3 Créer une nouvelle variable\nOn peut aussi utiliser l’opérateur $ pour créer une nouvelle variable dans notre tableau : pour cela, il suffit de lui assigner une valeur.\nPar exemple, la variable heures.tv contient le nombre d’heures passées quotidiennement devant la télé :\n\nhead(d$heures.tv, 10)\n\n [1] 0.0 1.0 0.0 2.0 3.0 2.0 2.9 1.0 2.0 2.0\n\n\nOn peut vouloir créer une nouvelle variable dans notre tableau qui contienne la même durée mais en minutes. On va donc créer une nouvelle variables minutes.tv de la manière suivante :\n\nd$minutes.tv &lt;- d$heures.tv * 60\n\nOn peut alors constater, soit visuellement soit dans la console, qu’une nouvelle variable (une nouvelle colonne) a bien été ajoutée au tableau :\n\nhead(d$minutes.tv)\n\n[1]   0  60   0 120 180 120\n\n\n\n2.3.3 Analyse univariée\nOn a donc désormais accès à un tableau de données d, dont les lignes sont des observations (des individus enquêtés), et les colonnes des variables (des caractéristiques de chacun de ces individus), et on sait accéder à ces variables grâce à l’opérateur $.\nSi on souhaite analyser ces variables, les méthodes et fonctions utilisées seront différentes selon qu’il s’agit d’une variable quantitative (variable numérique pouvant prendre un grand nombre de valeurs : l’âge, le revenu, un pourcentage…) ou d’une variable qualitative (variable pouvant prendre un nombre limité de valeurs appelées modalités : le sexe, la profession, le dernier diplôme obtenu, etc.).\n\n2.3.3.1 Analyser une variable quantitative\nUne variable quantitative est une variable de type numérique (un nombre) qui peut prendre un grand nombre de valeurs. On en a plusieurs dans notre jeu de données, notamment l’âge (variable age) ou le nombre d’heures passées devant la télé (heures.tv).\n\n2.3.3.1.1 Indicateurs de centralité\nCaractériser une variable quantitative, c’est essayer de décrire la manière dont ses valeurs se répartissent, ou se distribuent.\nPour cela on peut commencer par regarder les valeurs extrêmes, avec les fonctions min, max ou range :\n\nmin(d$age)\n\n[1] 18\n\nmax(d$age)\n\n[1] 97\n\nrange(d$age)\n\n[1] 18 97\n\n\nOn peut aussi calculer des indicateurs de centralité : ceux-ci indiquent autour de quel nombre se répartissent les valeurs de la variable. Il y en a plusieurs, le plus connu étant la moyenne, qu’on peut calculer avec la fonction mean :\n\nmean(d$age)\n\n[1] 48.157\n\n\nIl existe aussi la médiane, qui est la valeur qui sépare notre population en deux : on a la moitié de nos observations en-dessous, et la moitié au-dessus. Elle se calcule avec la fonction median :\n\nmedian(d$age)\n\n[1] 48\n\n\nUne différence entre les deux indicateurs est que la médiane est beaucoup moins sensible aux valeurs “extrêmes” : on dit qu’elle est plus robuste. Ainsi, en 2013, le salaire net moyen des salariés à temps plein en France était de 2202 euros, tandis que le salaire net médian n’était que de 1772 euros. La différence étant due à des très hauts salaires qui “tirent” la moyenne vers le haut.\n\n2.3.3.1.2 Indicateurs de dispersion\nLes indicateurs de dispersion permettent de mesurer si les valeurs sont plutôt regroupées ou au contraire plutôt dispersées.\nL’indicateur le plus simple est l’étendue de la distribution, qui décrit l’écart maximal observé entre les observations :\n\nmax(d$age) - min(d$age)\n\n[1] 79\n\n\nLes indicateurs de dispersion les plus utilisés sont la variance ou, de manière équivalente, l’écart-type (qui est égal à la racine carrée de la variance). On obtient la première avec la fonction var, et le second avec sd (abbréviation de standard deviation) :\n\nvar(d$age)\n\n[1] 287.0249\n\n\n\nsd(d$age)\n\n[1] 16.94181\n\n\nPlus la variance ou l’écart-type sont élevés, plus les valeurs sont dispersées autour de la moyenne. À l’inverse, plus ils sont faibles et plus les valeurs sont regroupées.\nUne autre manière de mesurer la dispersion est de calculer les quartiles :\n\nle premier quartile est la valeur pour laquelle on a 25% des observations en dessous et 75% au dessus\nle deuxième quartile est la valeur pour laquelle on a 50% des observations en dessous et 50% au dessus (c’est donc la médiane)\nle troisième quartile est la valeur pour laquelle on a 75% des observations en dessous et 25% au dessus\n\nOn peut les calculer avec la fonction quantile :\n\n### Premier quartile\nquantile(d$age, prob = 0.25)\n\n25% \n 35 \n\n\n\n## Troisième quartile\nquantile(d$age, prob = 0.75)\n\n75% \n 60 \n\n\nquantile prend deux arguments principaux : le vecteur dont on veut calculer le quantile, et un argument prob qui indique quel quantile on souhaite obtenir. prob prend une valeur entre 0 et 1 : 0.5 est la médiane, 0.25 le premier quartile, 0.1 le premier décile, etc.\nNotons enfin que la fonction summary permet d’obtenir d’un coup plusieurs indicateurs classiques :\n\nsummary(d$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   35.00   48.00   48.16   60.00   97.00 \n\n\n\n2.3.3.1.3 Représentation graphique\nL’outil le plus utile pour étudier la distribution des valeurs d’une variable quantitative reste la représentation graphique.\nLa représentation la plus courante est sans doute l’histogramme. On peut l’obtenir avec la fonction hist :\n\nhist(d$age)\n\n\n\nHistogramme de l’age par défaut\n\n\n\nCette fonction n’a pas pour effet direct d’effectuer un calcul ou de nous renvoyer un résultat : elle génère un graphique qui va s’afficher dans l’onglet Plots de RStudio.\nOn peut personnaliser l’apparence de l’histogramme en ajoutant des arguments supplémentaires à la fonction hist. L’argument le plus important est breaks, qui permet d’indiquer le nombre de classes que l’on souhaite.\n\nhist(d$age, breaks = 10, main = \"\")\n\n\n\nHistogramme de l’age avec 10 classes\n\n\n\n\nhist(d$age, breaks = 70, main = \"\")\n\n\n\nHistogramme de l’age avec 70 classes\n\n\n\nLe choix d’un “bon” nombre de classes pour un histogramme n’est pas un problème simple : si on a trop peu de classes, on risque d’effacer quasiment toutes les variations, et si on en a trop on risque d’avoir trop de détails et de masquer les grandes tendances.\nLes arguments de hist permettent également de modifier la présentation du graphique. On peut ainsi changer la couleur des barres avec col5, le titre avec main, les étiquettes des axes avec xlab et ylab, etc. :\n\nhist(d$age,\n  col = \"skyblue\",\n  main = \"Répartition des âges des enquêtés\",\n  xlab = \"Âge\",\n  ylab = \"Effectif\"\n)\n\n\n\nHistogramme modifié\n\n\n\nLa fonction hist fait partie des fonctions graphique de base de R. On verra plus en détail d’autres fonctions graphiques  avec l’extension ggplot2 qui permet la production et la personnalisation de graphiques complexes.\n\n2.3.3.2 Analyser une variable qualitative\nUne variable qualitative est une variable qui ne peut prendre qu’un nombre limité de valeurs, appelées modalités. Dans notre jeu de données on trouvera par exemple le sexe (sexe), le niveau d’études (nivetud), la catégorie socio-professionnelle (qualif)…\nÀ noter qu’une variable qualitative peut tout-à-fait être numérique, et que certaines variables peuvent être traitées soit comme quantitatives, soit comme qualitatives : c’est le cas par exemple du nombre d’enfants ou du nombre de frères et soeurs.\n\n2.3.3.2.1 Tri à plat\nL’outil le plus utilisé pour représenter la répartition des valeurs d’une variable qualitative est le tri à plat : il s’agit simplement de compter, pour chacune des valeurs possibles de la variable (pour chacune des modalités), le nombre d’observations ayant cette valeur. Un tri à plat s’obtient sous R à l’aide de la fonction table :\n\ntable(d$sexe)\n\n\nHomme Femme \n  899  1101 \n\n\nCe tableau nous indique donc que parmi nos enquêtés on trouve 899 hommes et 1101 femmes.\n\ntable(d$qualif)\n\n\n      Ouvrier specialise         Ouvrier qualifie               Technicien \n                     203                      292                       86 \nProfession intermediaire                    Cadre                  Employe \n                     160                      260                      594 \n                   Autre \n                      58 \n\n\nUn tableau de ce type peut être affiché ou stocké dans un objet, et on peut à son tour lui appliquer des fonctions. Par exemple, la fonction sort permet de trier le tableau selon la valeur de l’effectif. On peut donc faire :\n\ntab &lt;- table(d$qualif)\nsort(tab)\n\n\n                   Autre               Technicien Profession intermediaire \n                      58                       86                      160 \n      Ouvrier specialise                    Cadre         Ouvrier qualifie \n                     203                      260                      292 \n                 Employe \n                     594 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nAttention, par défaut la fonction table n’affiche pas les valeurs manquantes (NA). Si on souhaite les inclure il faut utiliser l’argument useNA = \"always\", soit : table(d$qualif, useNA = \"always\").\n\n\nÀ noter qu’on peut aussi appliquer summary à une variable qualitative. Le résultat est également le tri à plat de la variable, avec en plus le nombre de valeurs manquantes éventuelles :\n\nsummary(d$qualif)\n\n      Ouvrier specialise         Ouvrier qualifie               Technicien \n                     203                      292                       86 \nProfession intermediaire                    Cadre                  Employe \n                     160                      260                      594 \n                   Autre                     NA's \n                      58                      347 \n\n\nPar défaut ces tris à plat sont en effectifs et ne sont donc pas toujours très lisibles, notamment quand on a des effectifs importants. On leur rajoute donc en général la répartition en pourcentages. Pour cela, nous allons utiliser la fonction freq de l’extension questionr, qui devra donc avoir précédemment été chargée avec library(questionr) :\n\n## À rajouter en haut de script et à exécuter\nlibrary(questionr)\n\nOn peut alors utiliser la fonction :\n\nfreq(d$qualif)\n\n\n\n                           n    % val%\nOuvrier specialise       203 10.2 12.3\nOuvrier qualifie         292 14.6 17.7\nTechnicien                86  4.3  5.2\nProfession intermediaire 160  8.0  9.7\nCadre                    260 13.0 15.7\nEmploye                  594 29.7 35.9\nAutre                     58  2.9  3.5\nNA                       347 17.3   NA\n\n\nLa colonne n représente les effectifs de chaque catégorie, la colonne % le pourcentage, et la colonne val% le pourcentage calculé sur les valeurs valides, donc en excluant les NA. Une ligne a également été rajoutée pour indiquer le nombre et la proportion de NA.\nfreq accepte un certain nombre d’arguments pour personnaliser son affichage. Par exemple :\n\n\nvalid indique si on souhaite ou non afficher les pourcentages sur les valeurs valides\n\ncum indique si on souhaite ou non afficher les pourcentages cumulés\n\ntotal permet d’ajouter une ligne avec les effectifs totaux\n\nsort permet de trier le tableau par fréquence croissante (sort=\"inc\") ou décroissante (sort=\"dec\").\n\n\nfreq(d$qualif, valid = FALSE, total = TRUE, sort = \"dec\")\n\n\n\n                            n     %\nEmploye                   594  29.7\nOuvrier qualifie          292  14.6\nCadre                     260  13.0\nOuvrier specialise        203  10.2\nProfession intermediaire  160   8.0\nTechnicien                 86   4.3\nAutre                      58   2.9\nNA                        347  17.3\nTotal                    2000 100.0\n\n\n\n2.3.3.2.2 Représentations graphiques\nOn peut représenter graphiquement le tri à plat d’une variable qualitative avec un diagramme en barres, obtenu avec la fonction barplot. Attention, contrairement à hist cette fonction ne s’applique pas directement à la variable mais au résultat du tri à plat de cette variable, calculé avec table. Il faut donc procéder en deux étapes :\n\ntab &lt;- table(d$clso)\nbarplot(tab)\n\n\n\nGraphique en barre\n\n\n\nOn peut aussi trier le tri à plat avec la fonction sort avant de le représenter graphiquement, ce qui peut faciliter la lecture du graphique :\n\nbarplot(sort(tab))\n\n\n\nGraphique en barre trié\n\n\n\nUne alternative au graphique en barres est le diagramme de Cleveland, qu’on peut obtenir avec la fonction dotchart. Celle-ci s’applique elle aussi au tri à plat de la variable calculé avec table.\n\ndotchart(table(d$qualif))\n\n\n\nGraphique de Cleveland\n\n\n\nLà aussi, pour améliorer la lisibilité du graphique il est préférable de trier le tri à plat de la variable avant de le représenter :\n\ndotchart(sort(table(d$qualif)))\n\n\n\nGraphique de Cleveland trié\n\n\n\n\n2.3.4 Exercices\nExercice 1\nCréer un nouveau script qui effectue les actions suivantes :\n\ncharger l’extension questionr\n\ncharger le jeu de données nommé hdv2003\n\ncopier le jeu de données dans un nouvel objet nommé df\n\nafficher les dimensions et la liste des variables de df\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(questionr)\n\ndata(hdv2003)\ndf &lt;- hdv2003\n\ndim(df)\nnames(df)\n\n\n\n\nExercice 2\nOn souhaite étudier la répartition du temps passé devant la télévision par les enquêtés (variable heures.tv). Pour cela, affichez les principaux indicateurs de cette variable : valeur minimale, maximale, moyenne, médiane et écart-type. Représentez ensuite sa distribution par un histogramme en 10 classes.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(df$heures.tv)\nsd(df$heures.tv)\n\nhist(df$heures.tv, breaks = 10)\n\n\n\n\nExercice 3\nOn s’intéresse maintenant à l’importance accordée par les enquêtés à leur travail (variable trav.imp). Faites un tri à plat des effectifs des modalités de cette variable avec la commande table.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntable(df$trav.imp)\n\n\n\n\nFaites un tri à plat affichant à la fois les effectifs et les pourcentages de chaque modalité. Y’a-t-il des valeurs manquantes ?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfreq(df$trav.imp)\n\n\n\n\nReprésentez graphiquement les effectifs des modalités à l’aide d’un graphique en barres.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntab &lt;- sort(table(df$trav.imp))\nbarplot(tab)\n\n\n\n\nUtilisez l’argument col de la fonction barplot pour modifier la couleur du graphique en tomato.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(tab, col = \"tomato\")\n\n\n\n\nTapez colors() dans la console pour afficher l’ensemble des noms de couleurs disponibles dans R. Testez chaque couleur une à une pour trouver votre couleur préférée.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nC’est une blague, hein ! Cela dit moccasin ou palevioletred sont pas mal, si vous voulez essayer :-)",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction à R</span>"
    ]
  },
  {
    "objectID": "02-introR.html#analyse-de-2-variables",
    "href": "02-introR.html#analyse-de-2-variables",
    "title": "\n2  Introduction à R\n",
    "section": "\n2.4 Analyse de 2 variables",
    "text": "2.4 Analyse de 2 variables\nFaire une analyse bivariée, c’est étudier la relation entre deux variables : sont-elles liées ? les valeurs de l’une influencent-elles les valeurs de l’autre ? ou sont-elles au contraire indépendantes ?\nÀ noter qu’on va parler ici d’influence ou de lien, mais pas de relation de cause à effet : les outils présentés permettent de visualiser ou de déterminer une relation, mais des liens de causalité proprement dit sont plus difficiles à mettre en évidence. Il faut en effet vérifier que c’est bien telle variable qui influence telle autre et pas l’inverse, qu’il n’y a pas de “variable cachée”, etc.\nLà encore, le type d’analyse ou de visualisation est déterminé par la nature qualitative ou quantitative des deux variables.\n\n2.4.1 Croisement de deux variables qualitatives\n\n2.4.1.1 Tableaux croisés\nOn va continuer à travailler avec le jeu de données tiré de l’enquête Histoire de vie inclus dans l’extension questionr. On commence donc par charger l’extension, le jeu de données, et à le renommer en un nom plus court pour gagner un peu de temps de saisie au clavier :\n\nlibrary(questionr)\ndata(hdv2003)\nd &lt;- hdv2003\n\nQuand on veut croiser deux variables qualitatives, on fait un tableau croisé. Comme pour un tri à plat ceci s’obtient avec la fonction table de R, mais à laquelle on passe cette fois deux variables en argument. Par exemple, si on veut croiser la catégorie socio-professionnelle et le sexe des enquêtés :\n\ntable(d$qualif, d$sexe)\n\n                          \n                           Homme Femme\n  Ouvrier specialise          96   107\n  Ouvrier qualifie           229    63\n  Technicien                  66    20\n  Profession intermediaire    88    72\n  Cadre                      145   115\n  Employe                     96   498\n  Autre                       21    37\n\n\nPour pouvoir interpréter ce tableau on doit passer du tableau en effectifs au tableau en pourcentages ligne ou colonne. Pour cela, on peut utiliser les fonctions lprop et cprop de l’extension questionr, qu’on applique au tableau croisé précédent.\nPour calculer les pourcentages ligne :\n\ntab &lt;- table(d$qualif, d$sexe)\nlprop(tab)\n\n                          \n                           Homme Femme Total\n  Ouvrier specialise        47.3  52.7 100.0\n  Ouvrier qualifie          78.4  21.6 100.0\n  Technicien                76.7  23.3 100.0\n  Profession intermediaire  55.0  45.0 100.0\n  Cadre                     55.8  44.2 100.0\n  Employe                   16.2  83.8 100.0\n  Autre                     36.2  63.8 100.0\n  All                       44.8  55.2 100.0\n\n\nEt pour les pourcentages colonne :\n\ncprop(tab)\n\n                          \n                           Homme Femme All  \n  Ouvrier specialise        13.0  11.7  12.3\n  Ouvrier qualifie          30.9   6.9  17.7\n  Technicien                 8.9   2.2   5.2\n  Profession intermediaire  11.9   7.9   9.7\n  Cadre                     19.6  12.6  15.7\n  Employe                   13.0  54.6  35.9\n  Autre                      2.8   4.1   3.5\n  Total                    100.0 100.0 100.0\n\n\n\n\n\n\n\n\nNote\n\n\n\nPour savoir si on doit faire des pourcentages ligne ou colonne, on pourra se référer à l’article suivant :\nhttp://alain-leger.lescigales.org/textes/lignecolonne.pdf\nEn résumé, quand on fait un tableau croisé, celui-ci est parfaitement symétrique : on peut inverser les lignes et les colonnes, ça ne change pas son interprétation. Par contre, on a toujours en tête un “sens” de lecture dans le sens où on considère que l’une des variables dépend de l’autre. Par exemple, si on croise sexe et type de profession, on dira que le type de profession dépend du sexe, et non l’inverse : le type de profession est alors la variable dépendante (à expliquer), et le sexe la variable indépendante (explicative).\nPour faciliter la lecture d’un tableau croisé, il est recommandé de faire les pourcentages sur la variable indépendante. Dans notre exemple, la variable indépendante est le sexe, elle est en colonne, on calcule donc les pourcentages colonnes qui permettent de comparer directement, pour chaque sexe, la répartition des catégories socio-professionnelles.\n\n\n\n\n2.4.1.2 Représentation graphique\nIl est possible de faire une représentation graphique d’un tableau croisé, par exemple avec la fonction mosaicplot :\n\nmosaicplot(tab)\n\n\n\nGraphique mosaique\n\n\n\nOn peut améliorer ce graphique en colorant les cases selon les résidus du test du χ² (argument shade = TRUE) et en orientant verticalement les labels de colonnes (argument las = 3) :\n\nmosaicplot(tab, las = 3, shade = TRUE)\n\n\n\nGraphique mosaique modifié\n\n\n\nChaque rectangle de ce graphique représente une case de tableau. Sa largeur correspond au pourcentage des modalités en colonnes (il y’a beaucoup d’employés et d’ouvriers et très peu d’“autres”). Sa hauteur correspond aux pourcentages colonnes : la proportion d’hommes chez les cadres est plus élevée que chez les employés. Enfin, la couleur de la case correspond au résidu du test du χ² correspondant : les cases en rouge sont sous-représentées, les cases en bleu sur-représentées, et les cases blanches sont proches des effectifs attendus sous l’hypothèse d’indépendance.\n\n2.4.2 Croisement d’une variable quantitative et d’une variable qualitative\n\n2.4.2.1 Représentation graphique\nCroiser une variable quantitative et une variable qualitative, c’est essayer de voir si les valeurs de la variable quantitative se répartissent différemment selon la catégorie d’appartenance de la variable qualitative.\nPour cela, l’idéal est de commencer par une représentation graphique de type “boîte à moustache” à l’aide de la fonction boxplot. Par exemple, si on veut visualiser la répartition des âges selon la pratique ou non d’un sport, on va utiliser la syntaxe suivante :\n\nboxplot(age ~ sport, data = d)\n\n\n\n\n\n\n\nNote\n\n\n\nCette syntaxe de boxplot utilise une nouvelle notation de type “formule”. Celle-ci est utilisée notamment pour la spécification des modèles de régression. Ici le ~ peut se lire comme “en fonction de” : on veut représenter le boxplot de l’âge en fonction du sport.\n\n\nCe qui va nous donner le résultat suivant :\n\n\n\n\nGraphique en boites à moustaches\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nL’interprétation d’un boxplot est la suivante : Les bords inférieurs et supérieurs du carré central représentent le premier et le troisième quartile de la variable représentée sur l’axe vertical. On a donc 50% de nos observations dans cet intervalle. Le trait horizontal dans le carré représente la médiane. Enfin, des “moustaches” s’étendent de chaque côté du carré, jusqu’aux valeurs minimales et maximales, avec une exception : si des valeurs sont éloignées du carré de plus de 1,5 fois l’écart interquartile (la hauteur du carré), alors on les représente sous forme de points (symbolisant des valeurs considérées comme “extrêmes”).\n\n\nDans le graphique ci-dessus, on voit que ceux qui ont pratiqué un sport au cours des douze derniers mois ont l’air d’être sensiblement plus jeunes que les autres.\n\n2.4.2.2 Calculs d’indicateurs\nOn peut aussi vouloir comparer certains indicateurs (moyenne, médiane) d’une variable quantitative selon les modalités d’une variable qualitative. Si on reprend l’exemple précédent, on peut calculer la moyenne d’âge pour ceux qui pratiquent un sport et pour ceux qui n’en pratiquent pas.\nUne première méthode pour cela est d’extraire de notre population autant de sous-populations qu’il y a de modalités dans la variable qualitative. On peut le faire notamment avec la fonction subset.\n\nOn applique subset pour créer deux sous-populations, stockées dans deux nouveaux tableaux de données :\n\nd_sport &lt;- subset(d, sport == \"Oui\")\nd_nonsport &lt;- subset(d, sport == \"Non\")\n\nOn peut ensuite utiliser ces deux nouveaux tableaux de données comme on en a l’habitude, et calculer les deux moyennes d’âge :\n\nmean(d_sport$age)\n\n[1] 40.92531\n\n\n\nmean(d_nonsport$age)\n\n[1] 52.25137\n\n\nUne autre possibilité est d’utiliser la fonction tapply, qui prend en paramètre une variable quantitative, une variable qualitative et une fonction, puis applique automatiquement la fonction aux valeurs de la variables quantitative pour chaque niveau de la variable qualitative :\n\ntapply(d$age, d$sport, mean)\n\n     Non      Oui \n52.25137 40.92531 \n\n\n\n\n2.4.3 Croisement de deux variables quantitatives\nLe jeu de données hdv2003 comportant assez peu de variables quantitatives, on va s’intéresser maintenant à un autre jeu de données comportant des informations du recensement de la population de 2012. On le charge avec :\n\ndata(rp2012)\n\nUn nouveau tableau de données rp2012 devrait apparaître dans votre environnement. Celui-ci comprend les 5170 communes de France métropolitaine de plus de 2000 habitants, et une soixantaine de variables telles que le département, la population, le taux de chômage, etc. Pour une description plus complète et une liste des variables, voir section @ref(rp2012).\n\n2.4.3.1 Représentation graphique\nQuand on croise deux variables quantitatives, l’idéal est de faire une représentation graphique sous forme de nuage de points à l’aide de la fonction plot. On va représenter le croisement entre le pourcentage de cadres et le pourcentage de propriétaires dans la commune :\n\nplot(rp2012$cadres, rp2012$proprio)\n\n\n\nGraphique du pourcentage de propriétaire en fonction du pourcentage de cadre\n\n\n\nUne représentation graphique est l’idéal pour visualiser l’existence d’un lien entre les deux variables. Voici quelques exemples d’interprétation :\n\n\n\n\nIllustration des relations bivariées\n\n\n\nDans ce premier graphique généré sur nos données, il semble difficile de mettre en évidence une relation de dépendance. Si par contre on croise le pourcentage de cadres et celui de diplômés du supérieur, on obtient une belle relation de dépendance linéaire.\n\nplot(dipl_sup ~ cadres, data = rp2012)\n\n\n\nRelation entre le nombre de personnes diplomées à l’université et le nombre de cadre\n\n\n\n\n\n2.4.4 Exercices\nExercice 1\nDans le jeu de données hdv2003, faire le tableau croisé entre la catégorie socio-professionnelle (variable qualif) et le fait de croire ou non en l’existence des classes sociales (variable clso). Identifier la variable indépendante et la variable dépendante, et calculer les pourcentages ligne ou colonne. Interpréter le résultat.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(questionr)\ndata(hdv2003)\ntab &lt;- table(hdv2003$qualif, hdv2003$clso)\n\n## Ici la variable indépendante est `qualif`, on calcule donc\n## les pourcentages lignes\nlprop(tab)\n\n\n\n\n\nReprésenter ce tableau croisé sous la forme d’un mosaicplot en colorant les cases selon les résidus du test du χ².\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmosaicplot(tab, shade = TRUE)\n\n\n\n\nExercice 2\nToujours sur le jeu de données hdv2003, faire le boxplot qui croise le nombre d’heures passées devant la télévision (variable heures.tv) avec le statut d’occupation (variable occup).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nboxplot(hdv2003$heures.tv ~ hdv2003$occup)\n\n\n\n\nCalculer la durée moyenne devant la télévision en fonction du statut d’occupation à l’aide de tapply.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntapply(hdv2003$heures.tv, hdv2003$occup, mean, na.rm = TRUE)\n\n\n\n\nExercice 3\nSur le jeu de données rp2012, représenter le nuage de points croisant le pourcentage de personnes sans diplôme (variable dipl_aucun) et le pourcentage de propriétaires (variable proprio).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(questionr)\ndata(rp2012)\nplot(rp2012$dipl_aucun, rp2012$proprio)\n## ou\nplot(proprio ~ dipl_aucun, data = rp2012)",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction à R</span>"
    ]
  },
  {
    "objectID": "02-introR.html#gérer-les-données",
    "href": "02-introR.html#gérer-les-données",
    "title": "\n2  Introduction à R\n",
    "section": "\n2.5 Gérer les données",
    "text": "2.5 Gérer les données\n\n2.5.1 Importer et exporter des données\nIl existe de multiple format four sauvegarder les données, les 2 plus utiles sont .csv et .Rdata. Les fichiers .csv sont utilisés pour stocker des données. Ils sont ouvrables par les éditeurs de texte (e.g. Word, Writer, atom, …) et les tableurs (e.g. MS Excel, LO Calc). Ils sont lus avec la fonction read.csv et créés avec write.csv. Les fichiers .Rdata sont utilisés pour stocker n’importe quel objet R pas uniquement des données. Cependant, ces fichiers ne peuvent être lus et utilisés que par R. Ces fichiers sont lus avec la fonction load et créés avec la fonction save.\nLes données pour les exercices de laboratoire et pour les devoirs vous sont fournies en format .csv.\n\n2.5.1.1 Répertoire de travail\n\n\n\n\n\n\nAvertissement\n\n\n\nUne des erreurs les plus communes lorsque l’on débute avec R est lié au chargement des données et la lecture de fichier externe à R.\n\n\nUn message d’erreur typique est:\nError in file(file, \"rt\") : cannot open the connection\nIn addition: Warning message:\nIn file(file, \"rt\") :\n  cannot open file 'ou_est_mon_fichier.csv': No such file or directory\nL’erreur est du au fait que R ne sache pas où trouver le fichier. Par défaut lorsqu’on ouvre R, R utilise le dossier utilisateur sur l’ordinateur comme dossier de travail. Cela signifie que R va cherhcer à lire les fichiers dans ce dossier et écrire les nouveaux fichiers dans ce dossier. Ceci n’est pas toujours pratique surtout lorsque l’on débute avec R. Pour lire/écrire un fichier dans un endroit particulier sur l’ordinateur, il faut spécifier à R le chemin de cet endroit. Cela peut ce faire de 3 manières différentes:\n\navec la fonction file.choose(). La fonction ouvrira une boîte de dialogue vous permettant d’aller choisir un fichier sur votre ordinateur. Si cette option semble très attirante de part sa simplicité, je ne recommande pas de s’en servir car elle ne permet pas de reproduire l’analyse facilement. En effet, elle nécessite de choisir le document chaque fois que l’on souhaite l’utiliser.\nen spécifiant le chemin complet du fichier dans la commande. Par example \"/home/julien/Documents/cours/BIO4558/labo/data/monfichier.csv\". C’est assez long à taper et surtout cela ne permet pas de facilement utliser le code sur un autre ordinateur.\nen spécifiant un répertoire de travail avec la fonction setwd(). Ceci indique à R de chercher et d’écrire les fichiers dans un dossier en particulier. Le chemin des fichiers est toujours interprété de manière relative au répertoire de travail. Cela à l’avantage de pouvoir facilement utiliser le même code sur plusieurs ordinateur ssi la structure du dossier est la même.\n\nPOur connaitre le répertoire de travail de R il faut utiliser la fonction getwd(). La fonction setwd() permet de spécifier le chemin du dossier à utiliser comme répertoire de travail.\n\n\n\n\n\n\nNote\n\n\n\nSi vous ouvrez RStudio en double-cliquant sur un fichier .R alors Rstudio utlisera le dossier où ce fichier est présent comme répertoire de travail. Plutôt pratique car cela évite d’avoir à utiliser la fonction setwd().\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPour l’ensemble des laboratoire du cours, je suggère de créer un dossier dans lequel seront sauvegardés tous les scripts d’analyses et de sauvegardés tous les fichiers de données dans un sous dossier data. Le code du labo est structuré de cette manière. C’est pourquoi tous les codes de chargement ou d’écriture de données seront du type data/mon_fichier.xxx.\n\n\n\n2.5.1.2 Ouvrir un fichier de données en format .Rdata\n\nPour ouvrir ces fichiers, vous pouvez cliquer dessus et laisser votre système d’exploitation démarrer une nouvelle session de R ou encore, à partir de la console de R, utliser la fonction load avec le nom et le chemin du fichier de données. Par example, pour ouvrir le fichier ErablesGatineau.Rdata qui se situe dans le dossier data du dossier de travail, il faut taper:\n\nload(\"data/ErablesGatineau.Rdata\")\n\n\n2.5.1.3 Ouvrir un fichier de données en format .csv\n\nPour importer ces données en format .csv dans R, il faut utiliser la commande read.csv(). Par exemple, pour créer un objet R erables qui contient les données du fichier ErablesGatineau.csv, il faut utiliser la commande suivant.\n\nerables &lt;- read.csv(\"data/ErablesGatineau.csv\")\n\n\n\n\n\n\n\nAvertissement\n\n\n\nAttention si vous travaillez dans une langue utilisant la virgule au lieu du point décimal. Par défaut, R utilise le point décimal et vous n’obtiendrez pas le résultat escompté. Il existe une version modifiée de read.csv() appelée read.csv2() qui règle ce problème. Googlez-la si vous en avez besoin.\n\n\nPour vérifier si les données ont bel et bien été lues, vous pouvez lister les objets en mémoire avec la fonction ls() ou en obtenir une liste avec une description plus détaillée avec ls.str().\n\n\n\n\n\n\nNote\n\n\n\nJe vous déconseille cependant, la fonction ls.str() car elle peut produire des sorties extrèmementn longue si vous avez beaucoup d’objet dans l’environnement R. Je vous suggère donc d’utliser ls() et ensuite str() sur l’objet qui vous intéresse.\n\n\n\nls()\n\n [1] \"anglais\"        \"chien\"          \"d\"              \"d_nonsport\"    \n [5] \"d_sport\"        \"diplome\"        \"erables\"        \"hdv2003\"       \n [9] \"imc\"            \"maths\"          \"p\"              \"poids\"         \n[13] \"precipitations\" \"reg\"            \"resultat\"       \"rp2012\"        \n[17] \"s\"              \"sport\"          \"tab\"            \"taille1\"       \n[21] \"taille2\"        \"taille3\"        \"taille4\"        \"taille5\"       \n[25] \"tailles\"        \"tailles_m\"      \"temperature\"    \"title\"         \n[29] \"x\"              \"y\"             \n\nstr(erables)\n\n'data.frame':   100 obs. of  3 variables:\n $ station: chr  \"A\" \"A\" \"A\" \"A\" ...\n $ diam   : num  22.4 36.1 44.4 24.6 17.7 ...\n $ biom   : num  732 1171 673 1552 504 ...\n\n\nR confirme avoir en mémoire l’objet erables. erables est un tableau de données rectangulaire (data.frame) contenant 100 observations (lignes) de 3 variables (colonnes): station, une variable de type Facteur avec 2 niveaux, et diam et biom qui sont 2 variables numériques.\n\n2.5.1.4 Entrer des données\nR n’est pas un environnement idéal pour entrer des données. C’est possible, mais la syntaxe est lourde et peut inciter à s’arracher les cheveux. Utilisez votre chiffrier préféré pour faire l’entrée de données. Ce sera plus efficace et moins frustrant.\n\n2.5.1.5 Nettoyer/corriger des données\nUne autre opération qui peut être frustrante en R. Mon conseil : ne le faites pas là. Retournez au fichier original, faites la correction, puis re-exportez les données vers R. Il est finalement plus simple de refaire exécuter les quelques lignes de code par la machine. Vous aurez à la fin une seule version (corrigée) de vos données et un code qui vous permet de refaire votre analyse.\n\n2.5.1.6 Exporter des données à partir de R.\nVous pouvez utiliser la fonction,\n\nwrite.csv(mydata, file = \"outfilename.csv\", row.names = FALSE)\n\noù mydata est le nom du base de données à exporter et outfilename.csv est le nom du fichier à produire. Notez que ce fichier sera créé dans le répertoire de travail (qui peut être changé par le menu à File&gt;Change dir, ou par la commande setwd())\n\n2.5.2 Examen préliminaire des données\nLa première étape de toute analyse est l’examen des données. Elle nous permet de découvrir si on a bien importé les données, si les nombres enregistrés sont possibles, si toutes les données ont bien été lues, etc. L’examen préliminaire des données permet souvent aussi d’identifier des observations suspectes, possiblement dûes à des erreurs d’entrée de donnée. Finalement, l’examen graphique préliminaire permet en général de visualiser les tendances principales qui seront confirmées par l’analyse statistique en tant que telle. Le fichier sturgeon.csv contient les données d’une étude effectuée sur les esturgeons de la rivière Saskatchewan. Ces données ont été récoltées, entre autres, pour examiner comment la taille des esturgeons varie entre les sexes (sex), les sites (location), et les années (year).\n\n\nChargez les données du fichier sturgeon.csv dans un objet sturgeon.\nPour obtenir un aperçu des éléments du fichier qui ont été chargés en mémoire, taper la commande str(sturgeon).\n\n\nsturgeon &lt;- read.csv(\"data/sturgeon.csv\")\nstr(sturgeon)\n\n'data.frame':   186 obs. of  9 variables:\n $ fklngth : num  37 50.2 28.9 50.2 45.6 ...\n $ totlngth: num  40.7 54.1 31.3 53.1 49.5 ...\n $ drlngth : num  23.6 31.5 17.3 32.3 32.1 ...\n $ rdwght  : num  15.95 NA 6.49 NA 29.92 ...\n $ age     : int  11 24 7 23 20 23 20 7 23 19 ...\n $ girth   : num  40.5 53.5 31 52.5 50 54.2 48 28.5 44 39 ...\n $ sex     : chr  \"MALE\" \"FEMALE\" \"MALE\" \"FEMALE\" ...\n $ location: chr  \"THE_PAS\" \"THE_PAS\" \"THE_PAS\" \"THE_PAS\" ...\n $ year    : int  1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 ...\n\n\n\n2.5.2.1 Sommaire statistique\nPour un sommaire du contenu du base de données appelé sturgeon qui est en mémoire, taper la commande\n\nsummary(sturgeon)\n\n    fklngth         totlngth        drlngth          rdwght     \n Min.   :24.96   Min.   :28.15   Min.   :14.33   Min.   : 4.73  \n 1st Qu.:41.00   1st Qu.:43.66   1st Qu.:25.00   1st Qu.:18.09  \n Median :44.06   Median :47.32   Median :27.00   Median :23.10  \n Mean   :44.15   Mean   :47.45   Mean   :27.29   Mean   :24.87  \n 3rd Qu.:48.00   3rd Qu.:51.97   3rd Qu.:29.72   3rd Qu.:30.27  \n Max.   :66.85   Max.   :72.05   Max.   :41.93   Max.   :93.72  \n                 NA's   :85      NA's   :13      NA's   :4      \n      age            girth           sex              location        \n Min.   : 7.00   Min.   :11.50   Length:186         Length:186        \n 1st Qu.:17.00   1st Qu.:40.00   Class :character   Class :character  \n Median :20.00   Median :44.00   Mode  :character   Mode  :character  \n Mean   :20.24   Mean   :44.33                                        \n 3rd Qu.:23.50   3rd Qu.:48.80                                        \n Max.   :55.00   Max.   :73.70                                        \n NA's   :11      NA's   :85                                           \n      year     \n Min.   :1978  \n 1st Qu.:1979  \n Median :1979  \n Mean   :1979  \n 3rd Qu.:1980  \n Max.   :1980  \n               \n\n\nPour chaque variable, R donne le minimum, le maximum, la médiane qui est la valeur au milieu de la liste des observations ordonnées (appelée le 50 ième percentile), ici, la 93 ième valeur des 186 observations, les valeurs au premier (25%) et troisième quartile (75%), et si il y a des valeurs manquantes dans la colonne. Notez que plusieurs des variables ont des observations manquantes (NA). Donc, seules les variables fklngth (longueur à la fourche), sex, location et year ont 186 observations.\n\n\n\n\n\n\nAvertissement\n\n\n\nAttention aux valeurs manquantes Plusieurs fonctions de R y réagissent mal et on doit souvent faire les analyses sur des sous- ensembles sans valeur manquante, par des commandes ou des options dans les commandes. On y reviendra, mais prenez l’habitude de noter mentalement si il y a des données manquantes et de vous en rappeler en faisant l’analyse.\n\n\n\n2.5.2.2 Histogramme, densité de probabilité empirique, boxplot et examen visuel de la normalité\nExaminons maintenant de plus près la distribution de fklngth. La commande hist() permet de tracer un histogramme de la variable fklngth dans le base de données sturgeon.\n\nhist(sturgeon$fklngth)\n\n\n\n\n\n\n\nLes données semblent suivre approximativement une distribution normale.\n\n\n\n\n\n\nNote\n\n\n\nCette syntaxe peut paraître un peu lourde puisqu’on doit ajouter le préfixe sturgeon$ devant chaque nom de variable. On pourrait se faciliter la tâche en utilisant la commande attach() mais cela est fortement déconseillé et jamais utilisé dans ce document.\n\n\nCet histogramme est la représentation classique. Mais les histogrammes ne sont pas parfaits. Leur forme dépend en partie du nombre de catégories utilisées, surtout pour les petits échantillons. On peut faire mieux, particulièrement si on est intéressé à comparer visuellement la distribution des observations à une distribution normale. Mais il faut programmer un peu (ou savoir copier-coller…). Le code suivant est un histogramme fait avec l’extension ggplot2.\n\n\n\n\n\n\nExercice\n\n\n\nCopiez-collez le code suivant dans une nouvelle fenêtre script (File-&gt;New script, ou Ctrl-n dans Windows), puis exécutez le.\n\n\n\n## Chargez l'extension ggplot si besoin\nlibrary(ggplot2)\n##  créer un graphique `mygraph` utilisant les données de \"sturgeon\"\n## et définir l'axe des X comme la longueur `fklngth`\nmygraph &lt;- ggplot(data = sturgeon, aes(x = fklngth))\n\n## ajouter différentes parties au graphique\nmygraph &lt;- mygraph +\n  ## histogramme semi-transparent\n  geom_histogram(aes(y = ..density..), bins = 30, color = \"black\", alpha = 0.3) +\n  ##  line de densité\n  geom_density() +\n  ##  localisation des observations\n  geom_rug() +\n  ##  courbe de distribution normale approximé au données\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(sturgeon$fklngth),\n      sd = sd(sturgeon$fklngth)\n    ),\n    color = \"red\"\n  )\n\n## montrer le graphique\nmygraph\n\n\n\n\n\n\n\nChaque observation est représentée par une barre sous l’axe des x (rug). En rouge est la distribution normale de données avec la même moyenne et écart-type que les observations. Et l’autre ligne est la densité de probabilité empirique, « lissée » à partir des observations. Si vous êtes plus aventureux, vous pouvez examiner la distribution des observations de fklngth par sous-groupes (par exemple sex et year) avec :\n\nmygraph + facet_grid(year ~ sex)\n\n\n\n\n\n\n\nChaque panneau illustre la distribution pour un sexe cette année-là, et la courbe en rouge récurrente représente la distribution normale pour l’ensemble des données. Cette courbe peut servir à mieux évaluer visuellement les différences entre les panneaux. Une autre façon d’évaluer la normalité de données visuellement est de faire un QQ plot avec la paire de commandes qqnorm() et qqline().\n\nqqnorm(sturgeon$fklngth)\nqqline(sturgeon$fklngth)\n\n\n\n\n\n\n\nDes données parfaitement normales suivraient la ligne droite diagonale. Ici, il y a des déviations dans les queues de la distribution, et un peu à droite du centre. Comparez cette représentation à celle des deux graphiques précédents. Vous conviendrez sans doute avec moi qu’il est plus facile de visualiser comment la distribution dévie de la normalité sur les histogrammes et les graphiques de la densité empirique de probabilité que sur les QQ plots. Ceci dit, les QQ plots sont souvent utilisés et vous devriez être capable de les interpréter. De plus, on peut facilement éprouver statistiquement l’hypothèse que les données sont distribuées normalement avec R par la commande shapiro.test() qui calcule une statistique (W) qui est une mesure de la tendance des points d’un QQ plot à former une ligne parfaite. Si oui, alors W=1. Si W s’éloigne de 1 (vers 0), alors les données s’éloignent de la normalité. Ici,\n\nshapiro.test(sturgeon$fklngth)\n\n\n    Shapiro-Wilk normality test\n\ndata:  sturgeon$fklngth\nW = 0.97225, p-value = 0.0009285\n\n\nW n’est pas très loin de 1, mais suffisamment pour que la différence soit significative. L’examen visuel des grands échantillons est souvent compliqué par le fait que plusieurs points se superposent et qu’il devient plus difficile de bien visualiser la tendance centrale. Les boxplots avec “moustaches” (box and whiskers plots) offrent une alternative intéressante. La commande boxplot() peut produire un boxplot de fklngth pour chaque niveau de sex, et ajoute les coches.\n\nboxplot(fklngth ~ sex, data = sturgeon, notch = TRUE)\n\n\n\n\n\n\n\nLa ligne un peu plus épaisse dans la boîte de la Figure indique la médiane. La coche est proportionnelle à l’incertitude quant à la position de la médiane. On peut visuellement interpréter approximativement les différences entre médianes en examinant si il y a chevauchement entre les coches (ici, il n’y a pas chevauchement, et on conclurait provisoirement que la médiane de fklngth pour les femelles est supérieure à celle des mâles). Les boîtes s’étendent du premier au troisième quartile (du 25ième au 75ième percentile si vous préférez), Les barres (moustaches ou whiskers) au-dessus et en dessous des boîtes s’étendent soit de la valeur minimum à la valeur maximum, ou, si il y a des valeurs extrêmes, de la plus petite à la plus grande valeur à l’intérieur de 1.5x la largeur de l’étendue interquartile . Enfin, les observations qui excèdent les limites des moustaches (donc à plus de 1.5x l’étendue interquartile de chaque côté de la médiane) sont indiquées par des symboles.Ce sont des valeurs qui pourraient être considérées comme extrêmes et possiblement aberrantes.\n\n2.5.2.3 Diagrammes de dispersion bivariés\nEn plus des graphiques pour chacune des variables séparément, il est très souvent intéressant de jeter un coup d’oeil aux diagrammes de dispersion . La commande plot(y~x) permet de faire le graphique de y sur l’axe vertical (l’ordonnée) en fonction de x sur l’axe horizontal (l’abscisse).\n\n\n\n\n\n\nExercice\n\n\n\nFaites un graphique de fklngth en fonction de age avec la commande plot.\n\n\nVous devriez obtenir:\n\nplot(fklngth ~ age, data = sturgeon)\n\n\n\n\n\n\n\nR a une fonction qui permet la création des graphiques de dispersion de toutes les paires de variables (pairs()). Une des option de ¬ est l’ajout d’une trace lowess qui indique la tendance de la relation entre les variables. Pour obtenir la matrice de ces graphiques avec la trace lowess pour toutes les variable dans sturgeon, entrer la commande pairs(sturgeon[,1:6], panel=panel.smooth) et vous devriez obtenir\n\npairs(sturgeon[, 1:6], panel = panel.smooth)\n\n\n\n\n\n\n\n\n2.5.3 Créer des sous-ensembles de cas\nIl arrive fréquemment qu’une analyse se concentre sur un sous-ensemble des observations contenues dans un fichier de données. Les cas sont d’habitude sélectionnés selon un critère en particulier. Pour utiliser un sous-ensemble de vos données en créant un graphique ou en performant une analyse, on peut utiliser la commande subset(). Par exemple, pour créer un sous ensemble des données du tableau sturgeon qui ne contient que les femelles capturées en 1978, on peut écrire :\n\nsturgeon_female_1978 &lt;- subset(sturgeon, sex == \"FEMALE\" & year == \"1978\")\nsturgeon_female_1978\n\n     fklngth totlngth  drlngth rdwght age girth    sex   location year\n2   50.19685 54.13386 31.49606     NA  24  53.5 FEMALE    THE_PAS 1978\n4   50.19685 53.14961 32.28346     NA  23  52.5 FEMALE    THE_PAS 1978\n6   49.60630 53.93701 31.10236  35.86  23  54.2 FEMALE    THE_PAS 1978\n7   47.71654 51.37795 33.97638  33.88  20  48.0 FEMALE    THE_PAS 1978\n15  48.89764 53.93701 29.92126  35.86  23  52.5 FEMALE    THE_PAS 1978\n105 46.85039       NA 28.34646  23.90  24    NA FEMALE CUMBERLAND 1978\n106 40.74803       NA 24.80315  17.50  18    NA FEMALE CUMBERLAND 1978\n107 40.35433       NA 25.59055  20.90  21    NA FEMALE CUMBERLAND 1978\n109 43.30709       NA 27.95276  24.10  19    NA FEMALE CUMBERLAND 1978\n113 53.54331       NA 33.85827  48.90  20    NA FEMALE CUMBERLAND 1978\n114 51.77165       NA 31.49606  35.30  26    NA FEMALE CUMBERLAND 1978\n116 45.27559       NA 26.57480  23.70  24    NA FEMALE CUMBERLAND 1978\n118 53.14961       NA 32.67717  45.30  25    NA FEMALE CUMBERLAND 1978\n119 50.19685       NA 32.08661  33.90  26    NA FEMALE CUMBERLAND 1978\n123 49.01575       NA 29.13386  37.50  22    NA FEMALE CUMBERLAND 1978\n\n\n\n\n\n\n\n\nAvertissement\n\n\n\nDans ces comparaisons, il faut toujours utiliser == pour égal à. Dans ce contexte, si vous utilisez = seulement, vous n’obtiendrez pas ce que vous désirez. Dans le tableau qui suit se trouve une liste de commandes communes que vous allez probablement utiliser pour créer des expressions en R.\n\n\n\n\nOperateur\nExplication\nOperateur\nExplication\n\n\n\n==\nÉgal à\n!=\nPas égal à\n\n\n&gt;\nPlus que\n&lt;\nMoins que\n\n\n&gt;=\nPlus que ou égal à\n&lt;=\nMoins que ou égal à\n\n\n&\nEt vectorisé\n|\nOu vectorisé\n\n\n&&\nEt contrôle\n||\nOu contrôle\n\n\n!\nPas\n\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nEn utilisant les commandes subset() et hist(), essayez de faire un histogramme pour le sous-ensemble de cas correspondant aux femelles capturées en 1979 et 1980 (donc sex == \"FEMALE\" & (year == \"1979\" | year == \"1980\"))\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsub_female_7980 &lt;- subset(sturgeon, sex == \"FEMALE\" & (year == \"1979\" | year == \"1980\"))\nhist(sub_female_7980$fklngth)\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 Transformations de données\nIl est très fréquemment nécessaire d’effectuer des transformations mathématiques sur les données brutes pour mieux satisfaire aux conditions d’application de tests statistiques. R étant aussi un langage de programmation complet, il peut donc effectuer les transformations désirées. Les fonctions les plus fréquemment utilisées sont:\n\nlog()\nsqrt()\nifelse()\n\nOn peut employer ces fonctions directement dans les lignes de commandes, ou encore créer de nouvelles variables orphelines ou faisant partie d’un data.frame. Par exemple, pour faire un graphique du logarithme décimal de fklngth en fonction de l’âge, on peut écrire\n\nplot(log(fklngth) ~ age, data = sturgeon)\n\nPour créer une variable orpheline (i.e. non incluse dans le data.frame) appelée logfklngth et contenant le logarithme décimal de fklngth, on peut écrire\n::: {.cell}\nlogfklngth &lt;- log10(sturgeon$fklngth)\n:::\nSi on veut ajouter cette variable transformée à un tableau de données (data.frame), alors, on doit préfixer le nom de la variable par le nom du base de données et du symbole $, par exemple, pour ajouter une variable nommée lfkl contenant le log10 de fklngth au tableau sturgeon, on peut écrire:\n\nsturgeon$logfkl &lt;- log10(sturgeon$fklngth)\n\nN’oubliez pas de sauvegarder ce tableau modifié si vous voulez avoir accès à cette nouvelle variable dans le futur. Pour les transformations conditionnelles, on peut utiliser la fonction ifelse(). Par exemple, pour créer une nouvelle variable appelée dummy qui sera égale à 1 pour les mâles et 0 pour les femelles, on peut écrire:\n\nsturgeon$dummy &lt;- ifelse(sturgeon$sex == \"MALE\", 1, 0)\n\n\n2.5.5 Exercice sur R\nVous trouverez dans le fichier salmonella.csv, des valeurs numériques du ratio d’infection des cellules par la salmonelle dans deux milieux (IN VITRO et IN VIVO) et pour trois souches différentes de salmonelles. Examinez les données pour le ratio et faites des graphiques pour évaluer la normalité de la distribution des ratios pour la souche SAUVAGE dans les 2 milieux combinés et produire un graphique.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n## Charger les données\nsalmonella &lt;- read.csv(\"data/salmonella.csv\")\n\n## creer le graph en utilisant juste la souche sauvage et définir x\nmygraph &lt;- ggplot(subset(salmonella, souche == \"SAUVAGE\"), aes(x = ratio))\n## ajouter des composants graphiques\nmygraph &lt;- mygraph +\n  # line densité\n  geom_density() +\n  # position des observations\n  geom_rug() +\n  # histogramme\n  geom_histogram(aes(y = ..density..),\n    bins = 30,\n    color = \"black\",\n    alpha = 0.3\n  ) +\n  # distribution normal ajustée\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(subset(salmonella, souche == \"SAUVAGE\")$ratio),\n      sd = sd(subset(salmonella, souche == \"SAUVAGE\")$ratio)\n    ),\n    color = \"red\"\n  )\n## faire le graphique\nmygraph\n\n\n\nDistibution des ratios d’infections par la souche sauvage de salmonelle",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction à R</span>"
    ]
  },
  {
    "objectID": "02-introR.html#footnotes",
    "href": "02-introR.html#footnotes",
    "title": "\n2  Introduction à R\n",
    "section": "",
    "text": "On peut ignorer pour le moment la présence du [1] en début de ligne.↩︎\nc est l’abbréviation de combine, son nom est très court car on l’utilise très souvent↩︎\nLa seule limite pour la taille d’un objet étant la mémoire vive (RAM) de la machine sur laquelle tourne la session R.↩︎\nLes différents types de variables seront décrits plus en détail dans le chapitre @ref(vectorfactor) sur les recodages.↩︎\nLes différentes manières de spécifier des couleurs sont indiquées dans l’encadré de la section @ref(scalecolor).↩︎",
    "crumbs": [
      "Données",
      "Utiliser R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction à R</span>"
    ]
  },
  {
    "objectID": "25-power.html",
    "href": "25-power.html",
    "title": "\n3  Analyse de puissance avec R et G*Power\n",
    "section": "",
    "text": "3.1 La théorie",
    "crumbs": [
      "Données",
      "Fundamentals of stats",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-power.html#la-théorie",
    "href": "25-power.html#la-théorie",
    "title": "\n3  Analyse de puissance avec R et G*Power\n",
    "section": "",
    "text": "3.1.1 Qu’est-ce que la puissance?\nLa puissance est la probabilité de rejeter l’hypothèse nulle quand elle est fausse.\n\n3.1.2 Pourquoi faire une analyse de puissance?\nÉvaluer l’évidence\nL’analyse de puissance effectuée après avoir accepté une hypothèse nulle permet de calculer la probabilité que l’hypothèse nulle soit rejetée si elle était fausse et que la taille de l’effet était d’une valeur donnée. Ce type d’analyse a posteriori est très commun.\nPlanifier de meilleures expériences\nL’analyse de puissance effectuée avant de réaliser une expérience (le plus souvent après une expérience préliminaire cependant), permet de déterminer le nombre d’observations nécessaires pour détecter un effet d’une taille donnée à un niveau fixe de probabilité (la puissance). Ce type d’analyse a priori devrait être réalisé plus souvent.\nEstimer la “limite de détection” statistique\nL’effort d’échantillonnage est souvent déterminé à l’avance (par exemple lorsque vous héritez de données récoltées par quelqu’un d’autre), ou très sévèrement limité (lorsque les contraintes logistiques prévalent). Que ce soit a priori ou a posteriori l’analyse de puissance vous permet d’estimer, pour un effort d’échantillonnage donné et un niveau de puissance fixe, quelle est la taille minimale de l’effet qui peut être détecté (comme étant statistiquement significatif).\n\n3.1.3 Facteurs qui affectent la puissance\nIl y a 3 facteurs qui affectent la puissance d’un test statistique.\nLe critère de décision\nLa puissance dépend de \\(\\alpha\\), le seuil de probabilité auquel on rejette l’hypothèse nulle. Si ce seuil est très strict (i.e. si \\(\\alpha\\) est fixé à une valeur très basse, comme 0.1% ou p = 0.001), alors la puissance sera plus faible que si le seuil était moins strict.\nLa taille de l’échantillon\nPlus l’échantillon est grand, plus la puissance est élevée. La capacité d’un test à détecter de petites différences comme étant statistiquement significatives augmente avec une augmentation du nombre d’observations.\nLa taille de l’effet\nPlus la taille de l’effet est grande, plus un test a de puissance. Pour un échantillon de taille fixe, la capacité d’un test à détecter un effet comme étant statistiquement significatif est plus élevée si l’effet est grand que s’il est petit. La taille de l’effet est en fait une mesure du degré de fausseté de l’hypothèse nulle.",
    "crumbs": [
      "Données",
      "Fundamentals of stats",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-power.html#quest-ce-que-gpower",
    "href": "25-power.html#quest-ce-que-gpower",
    "title": "\n3  Analyse de puissance avec R et G*Power\n",
    "section": "\n3.2 Qu’est ce que G*Power?",
    "text": "3.2 Qu’est ce que G*Power?\nG*Power est un programme gratuit, développé par des psychologues de l’Université de Dusseldorf en Allemagne. Le programme existe en version Mac et Windows. Il peut cependant être utilisé sous linux via Wine. G*Power vous permettra d’effectuer une analyse de puissance pour la majorité des tests que nous verrons au cours de la session sans avoir à effectuer des calculs complexes ou farfouiller dans des tableaux ou des figures décrivant des distributions ou des courbes de puissance. Il est possible de faire tous les analyses de G*power avec R, mais cela est nettement plus complexes, car il faut tous coder à la main. Dans les cas les plus simple le code R est aussi fourni. G*power est vraiment un outil très utile que vous devrez maîtriser.\n\n\n\n\n\n\nExercice\n\n\n\nTéléchargez le programme ici et installez-le sur votre ordi et votre station de travail au laboratoire (si ce n’est déjà fait).",
    "crumbs": [
      "Données",
      "Fundamentals of stats",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-power.html#comment-utiliser-gpower",
    "href": "25-power.html#comment-utiliser-gpower",
    "title": "\n3  Analyse de puissance avec R et G*Power\n",
    "section": "\n3.3 Comment utiliser G*Power",
    "text": "3.3 Comment utiliser G*Power\n\n3.3.1 Principe général\nL’utilisation de G*Power implique généralement en trois étapes:\n\nChoisir le test approprié\nChoisir l’un des 5 types d’analyses de puissance disponibles\nInscrire les valeurs des paramètres requis et cliquer sur Calculate\n\n3.3.2 Types d’analyses de puissance disponibles\nA priori\nCalcule l’effectif requis pour une valeur de \\(\\alpha\\), \\(\\beta\\) et de taille d’effet donnée. Ce type d’analyse est utile à l’étape de planification des expériences.\nCompromis\nCalcule \\(\\alpha\\) et \\(\\beta\\) pour un rapport \\(\\beta / \\alpha\\) donné, un effectif fixe, et une taille d’effet donnée. Ce type d’analyse est plus rarement utilisé (je ne l’ai jamais fait), mais peut être utile lorsque le rapport \\(\\beta / \\alpha\\) est d’intérêt, par exemple lorsque le coût d’une erreur de type I et de type II peut être quantifié.\nCritère\nCalcule \\(\\alpha\\) pour \\(\\beta\\), effectif et taille d’effet donné. En pratique, je vois peu d’utilité pour ce type de calcul. Contactez-moi si vous en voyez une!\nPost-hoc\nCalcule la puissance (1 - \\(\\beta\\)) pour \\(\\alpha\\), une taille d’effet et un effectif donné. Très utilisée pour interpréter les résultats d’une analyse statistique non-significative, mais seulement si l’on utilise une taille d’effet biologiquement significative (et non la taille d’effet observée). Peu pertinente lorsque le test est significatif.\nSensitivité\nCalcule la taille d’effet détectable pour une valeur d’\\(\\alpha\\), \\(\\beta\\) et un effectif donné. Très utile également au stade de planification des expériences.\n\n3.3.3 Comment calculer la taille de l’effet G*Power permet de faire une analyse de puissance pour de nombreux tests statistiques\nL’indice de la taille de l’effet qui est utilisé par G*Power pour les calculs dépend du test. Notez que d’autres logiciels peuvent utiliser des indices différents et il est important de vérifier que l’indice que l’on utilise est celui qui convient. G*Power vous facilite la tâche et permet de calculer la taille de l’effet en inscrivant seulement les valeurs pertinentes dans la fenêtre de calcul. Le tableau suivant donne les indices utilisés par G*Power pour les différents tests.\n\n\n\n\n\n\n\nTest\nTaille d’effet\nFormule\n\n\n\ntest de t sur des moyennes\nd\n\\(d = \\frac{|\\mu_1 - \\mu_2|}{\\sqrt{({s_1}^2 + {s_2}^2)/2}}\\)\n\n\ntest de t pour des corrélations\nr\n\n\n\nautres tests de t\nd\n\\(d = \\frac{\\mu}{\\sigma}\\)\n\n\ntest F (ANOVA)\nf\n\\(f = \\frac{\\frac{\\sqrt{\\sum_{i=1}^k (\\mu_i - \\mu)^2}}{k}}{\\sigma}\\)\n\n\nautres test F\n\\(f^2\\)\n\\(f^2 = \\frac{{R_p}^2}{1-{R_p}^2}\\)\n\n\n\n\n\n\\({R_p}\\) est le coefficient de corrélation partiel\n\n\ntest Chi-carré\nw\n\\(w = \\sqrt{ \\sum_{i=1}^m \\frac{(p_{0i} - p_{1i})^2 }{ p_{0i}} }\\)\n\n\n\n\n\n\\(p_{0i}\\) \\(p_{1i}\\) sont les proportions de la catégorie i prédites par l’hypothèse nulle et alternative respectivement",
    "crumbs": [
      "Données",
      "Fundamentals of stats",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-power.html#puissance-pour-un-test-de-t-comparant-deux-moyennes",
    "href": "25-power.html#puissance-pour-un-test-de-t-comparant-deux-moyennes",
    "title": "\n3  Analyse de puissance avec R et G*Power\n",
    "section": "\n3.4 Puissance pour un test de t comparant deux moyennes",
    "text": "3.4 Puissance pour un test de t comparant deux moyennes\n\n\n\n\n\n\nImportant\n\n\n\nL’ensemble des analyses de puissance décrites après peuvent être réalisé avec 2 fonctions dans R.\n\n\npwr.t.test(): lorsque les tailles d’échantillons sont identiques\n\npwr.t2n.test(): lorsque les échantillons ont des tailles différentes\n\n\n\nL’objectif de cette séance de laboratoire est de vous familiariser avec G*Power et de vous aider à comprendre comment les quatre paramètres des analyses de puissance (\\(\\alpha\\), \\(\\beta\\), effectif et taille de l’effet) sont reliés entre eux. On examinera seulement un des nombreux tests, le test de t permettant de comparer deux moyennes indépendantes. C’est le test le plus communément utilisé par les biologistes, vous l’avez tous déjà utilisé, et il conviendra très bien pour les besoins de la cause. Ce que vous apprendrez aujourd’hui s’appliquera à toutes les autres analyses de puissance que vous effectuerez à l’avenir.\nJaynie Stephenson a étudié la productivité des ruisseaux de la région d’Ottawa. Elle a, entre autres, quantifié la biomasse des poissons dans 18 ruisseaux sur le Bouclier Canadien d’une part, et dans 18 autres ruisseaux de la vallée de la rivière des Outaouais et de la rivière Rideau d’autre part. Elle a observé une biomasse plus faible dans les ruisseaux de la vallée (2.64 \\(g/m^2\\), écart-type=3.28) que dans ceux du Bouclier (3.31 \\(g/m^2\\), écart-type=2.79). En faisant un test de t pour éprouver l’hypothèse nulle que la biomasse des poissons est la même dans les deux régions, elle obtient:\nPooled-Variance Two-Sample t-Test\nt = -0.5746, df = 34, p-value = 0.5693\nElle accepte l’hypothèse nulle (puisque p est plus élevé que 0.05) conclue donc que la biomasse moyenne des poissons est la même dans ces deux régions.\n\n3.4.1 Analyse post-hoc\nCompte tenu des valeurs des moyennes observées et de leur écart- type, on peut utiliser G*Power pour calculer la puissance du test de t bilatéral pour deux moyennes indépendantes et pour la taille d’effet (i.e. la différence entre la biomasse entre les deux régions, pondérée par les écarts-type) à \\(\\alpha\\) = 0.05.\nDémarrer G*Power.\n\nÀ Test family, choisir: t tests\nÀ Statistical test, choisir: Means: Difference between two inde- pendent means (two groups)\nÀ Type of power analysis, choisir: Post hoc: Compute achieved power - given \\(\\alpha\\), sample size, and effect size\nDans Input Parameters,\n\n\nà la boîte Tail(s), choisir: Two,\nvérifier que \\(\\alpha\\) err prob est égal à 0.05\ninscrire 18 pour Sample size group 1 et 2\npour calculer la taille d’effet (Effect size d), cliquer sur le bouton Determine =&gt;\n\n\n\nDans la fenêtre qui s’ouvre à droite, sélectionner n1 = n2\n\n\n\nentrer les moyennes (Mean group 1 et 2)\nentrer les écarts types (SDs group 1 et 2)\ncliquer sur le bouton Calculate and transfer to main window\n\n\n\nCliquer sur le bouton Calculate dans la fenêtre principale et vous devriez obtenir ceci:\n\n\n\n\n\nAnalyse post-hoc avec la taille d’effet estimée\n\n\n\nLa même analyse peut être faites en utlisant R. Pour l’analyse avec R, il faut définir d’abord calculer la taille d’effet d pour un test de t comparant deux moyennes, puis utiliser la fonction pwr.t.test de l’extension pwr. Le plus simple comme nous allons estimer la taille d’effet d plusieurs fois et de créer une petite fonction qui estime d basé sur les paramètres nécessaires.\n\n# charger l'extension pwr\nlibrary(pwr)\n# définir une fonction pour d\nd &lt;- function(u1, u2, sd1, sd2) {\n  abs(u1 - u2) / sqrt((sd1^2 + sd2^2) / 2)\n}\n\n# analyse de puissance\npwr.t.test(n = 18, d = d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79), sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 18\n              d = 0.220042\n      sig.level = 0.05\n          power = 0.09833902\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n# graphique comme g*Power\nx &lt;- seq(-4, 4, length = 200)\nplot(x, dnorm(x), type = \"l\", col = \"red\", lwd = 2)\nqc &lt;- qt(0.025, 16)\nabline(v = qc, col = \"green\")\nabline(v = -qc, col = \"green\")\nlines(x, dnorm(x, mean = (3.31 - 2.64)), type = \"l\", col = \"blue\", lwd = 2)\n\n# power corresponds to the shaded area\ny &lt;- dnorm(x, mean = (3.31 - 2.64))\npolygon(c(x[x &lt;= -qc], -qc), c(y[x &lt;= -qc], 0), col = rgb(red = 0, green = 0.2, blue = 1, alpha = 0.5))\n\n\n\nGraphique de l’analyse de puissance dans R\n\n\n\nÉtudions un peu la figure @ref(fig:gpower-1).\n\nLa courbe de gauche, en rouge, correspond à la distribution de la statistique t si \\(H_0\\) est vraie (i.e si les deux moyennes étaient égales) compte tenu de l’effectif (18 dans chaque région) et des écarts- types observés.\nLes lignes verticales vertes correspondent aux valeurs critiques de t pour une valeur \\(\\alpha = 0.05\\) et un effectif total de 36 (2x18).\nLes régions ombrées en rose correspondent aux zones de rejet de \\(H_0\\). Si Jaynie avait obtenu une valeur de t en dehors de l’intervalle délimité par les valeurs critiques allant de -2.03224 à 2.03224, alors elle aurait rejeté \\(H_0\\), l’hypothèse nulle d’égalité des deux moyennes. En fait, elle a obtenu une valeur de t égale à -0.5746 et conclu que la biomasse est la même dans les deux régions.\nLa courbe de droite, en bleu, correspond à la distribution de la sta- tistique t si \\(H_1\\) est vraie (ici \\(H_1\\) correspond à une différence de biomasse entre les deux régions de \\(3.33-2.64=0.69g/m^2\\), compte tenu des écarts-types observés). Cette distribution correspond à ce qu’on devrait s’attendre à observer si \\(H_1\\) était vraie et que l’on répétait un grand nombre de fois les mesures dans des échantillons aléatoires de 18 ruisseaux des deux régions en calculant la statistique t à chaque fois. En moyenne, on observerait une valeur de t d’environ 0.6.\nNotez que la distribution de droite chevauche considérablement celle de gauche, et une bonne partie de la surface sous la courbe de droite se retrouve à l’intérieur de l’intervalle d’acceptation de \\(H_0\\), délimité par les deux lignes vertes et allant de -2.03224 à 2.03224. Cette proportion, correspondant à la partie ombrée en bleu sous la courbe de droite et dénoté par \\(\\beta\\) correspond au risque d’erreur de type II qui est d’accepter \\(H_0\\) quand \\(H_1\\) est vraie.\nLa puissance est simplement \\(1-\\beta\\), et est ici de 0.098339. Donc, si la biomasse différait de 0.69\\(g/m^2\\) entre les deux régions, Jaynie n’avait que 9.8% des chances d’être capable de détecter une différence statistiquement significative à \\(\\alpha=5%\\) en échantillonnant 18 ruisseaux de chaque région.\n\nRécapitulons: La différence de biomasse entre les deux régions n’est pas statistiquement significative d’après le test de t. C’est donc que cette différence est relativement petite compte tenu de la précision des mesures. Il n’est donc pas très surprenant que la puissance, i.e. la probabilité de détecter une différence significative, soit faible. Toute cette analyse ne nous informe pas beaucoup.\nUne analyse de puissance post hoc avec la taille de l’effet observé n’est pas très utile. On la fera plutôt pour une taille d’effet autre que celle observée quand H 0 est acceptée. Quelle taille d’effet utiliser? C’est la biologie du système étudié qui peut nous guider. Par exemple, en ce qui concerne la biomasse des poissons, on pourrait s’attendre à ce qu’une différence de biomasse du simple au double (disons de 2.64 à 5.28 \\(g/m^2\\)) ait des conséquences écologiques. On voudrait s’assurer que Jaynie avait de bonnes chances de détecter une différence aussi grande que celle-là avant d’accepter ses conclusions que la biomasse est la même entre les deux régions. Quelles étaient les chances de Jaynie de détecter une différence de 2.64 \\(g/m^2\\) entre les deux régions? G*Power peut nous le dire.\n\n\n\n\n\n\nExercice\n\n\n\nChanger la moyenne du groupe 2 à 5.28, recalculer la taille d’effet, et cliquer sur Calculate pour obtenir (@ref(fig:gpower-2)).\n\n\n\n\n\n\nAnalyse post-hoc avec une taille d’effet différente\n\n\n\n\npwr.t.test(n = 18, d = d(u1 = 2.64, sd1 = 3.28, u2 = 5.28, sd2 = 2.79), sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 18\n              d = 0.8670313\n      sig.level = 0.05\n          power = 0.7146763\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nLa puissance est de 0.71, donc Jaynie avait une chance raisonnable de détecter une différence du simple au double avec 18 ruisseaux dans chaque région.\nNotez que cette analyse de puissance post hoc pour une taille d’effet jugée biologiquement significative est bien plus informative que l’analyse précédente pour la taille d’effet observée (qui est celle effectuée par défaut par bien des néophytes et de trop nombreux logiciels qui essaient de penser pour nous). En effet, Jaynie n’a pu détecter de différences significatives entre les deux régions. Cela pourrait être pour deux raisons: soit qu’il n’y a pas de différences entre les régions, ou soit parce que la précision des mesures est si faible et l’effort d’échantillonnage était si limité qu’il était très peu probable de détecter même d’énormes différences. La deuxième analyse de puissance permet d’éliminer cette seconde possibilité puisque Jaynie avait 71% des chances de détecter une différence du simple au double.\n\n3.4.2 Analyse à priori\nSupposons qu’on puisse défendre la position qu’une différence de biomasse observée par Jaynie entre les deux régions, \\(3.31- 2.64=0.67g/m^2\\), soit écologiquement signifiante. On devrait donc planifier la prochaine saison d’échantillonnage de manière à avoir de bonnes chances de détecter une différence de cette taille. Combien de ruisseaux Jaynie devrait-elle échantillonner pour avoir 80% des chances de la détecter (compte tenu de la variabilité observée)?\n\n\n\n\n\n\nExercice\n\n\n\nChanger le type d’analyse de puissance dans G*Power à A priori: Compute sample size - given \\(\\alpha\\), power, and effect size. Assurez-vous que les valeurs pour les moyennes et les écarts-type soient celles qu’a obtenu Jaynie, recalculez la taille de l’effet, et inscrivez 0.8 pour la puissance.\n\n\n\n\n\n\nAnalyse à priori\n\n\n\n\npwr.t.test(power = 0.8, d = d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79), sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 325.1723\n              d = 0.220042\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nOuch! Il faudrait échantillonner 326 ruisseaux dans chaque région! Cela coûterait une fortune et exigerait de nombreuses équipes de travail. Sans cela, on ne pourrait échantillonner que quelques dizaines de ruisseaux, et il serait peu probable que l’on puisse détecter une si faible différence de biomasse entre les deux régions. Ce serait vraisemblablement en vain et pourrait être considéré comme une perte de temps: pourquoi tant d’efforts et de dépenses si les chances de succès sont si faibles.\nSi on refait le même calcul pour une puissance de 95%, on obtient 538 ruisseaux par région. Augmenter la puissance ça demande plus d’effort.\n\npwr.t.test(power = 0.95, d = d(u1 = 2.64, sd1 = 3.28, u2 = 3.31, sd2 = 2.79), sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 537.7286\n              d = 0.220042\n      sig.level = 0.05\n          power = 0.95\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\n3.4.3 Analyse de sensitivité - Calculer la taille d’effet détectable\nCompte tenu de la variabilité observée, d’un effort d’échantillonnage de 18 ruisseaux par région, et en conservant \\(\\alpha=0.05\\), quelle est la taille d’effet que Jaynie pouvait détecter avec 80% de chances \\(\\beta=0.2\\))?\n\n\n\n\n\n\nExercice\n\n\n\nChangez le type d’analyse dans G*Power à Sensitivity: Compute required effect size - given \\(\\alpha\\), power, and sample size et assurez-vous que la taille des échantillons est de 18 dans chaque région.\n\n\n\n\n\n\nAnalyse de sensitivité\n\n\n\n\npwr.t.test(power = 0.8, n = 18, sig.level = 0.05, type = \"two.sample\")\n\n\n     Two-sample t test power calculation \n\n              n = 18\n              d = 0.9612854\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nLa taille d’effet détectable pour cette taille d’échantillon, \\(\\alpha=0.05\\) et \\(\\beta=0.2\\) (ou une puissance de 80%) est de 0.961296.\n\n\n\n\n\n\nAvertissement\n\n\n\ncette valeur est l’indice d de la taille de l’effet et est pondérée par la variabilité des mesures.\n\n\nDans ce cas ci, d est approximativement égal à \\[ d = \\frac{| \\bar{X_1} \\bar{X_2} |} {\\sqrt{\\frac{{s_1}^2 +{s_2}^2}{2}}}\\] Pour convertir cette valeur de d sans unités en une valeur de différence de biomasse détectable (i.e \\(| \\bar{X_1} \\bar{X_2} |\\)), il suffit de multiplier d par le dénominateur de l’équation. \\[\n| \\bar{X_1} \\bar{X_2} | = d * \\sqrt{\\frac{{s_1}^2 +{s_2}^2}{2}}\n\\] Dans R, on peut estimer cela avec:\n\npwr.t.test(power = 0.8, n = 18, sig.level = 0.05, type = \"two.sample\")$d * sqrt((3.28^2 + 2.79^2) / 2)\n\n[1] 2.926992\n\n\nDonc, avec 18 ruisseaux dans chaque région, pour \\(\\alpha=0.05\\) et \\(\\beta=0.2\\) (une puissance de 80%), Jaynie pouvait détecter une différence de biomasse de 2.93\\(g/m^2\\) entre les régions, un peu plus que du simple au double.",
    "crumbs": [
      "Données",
      "Fundamentals of stats",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "25-power.html#points-à-retenir",
    "href": "25-power.html#points-à-retenir",
    "title": "\n3  Analyse de puissance avec R et G*Power\n",
    "section": "\n3.5 Points à retenir",
    "text": "3.5 Points à retenir\n\nL’analyse de puissance post hoc n’est pertinente que lorsque l’on a accepté l’hypothèse nulle. Il est en effet impossible de faire une erreur de type II quand on rejette \\(H_0\\).\nAvec de très grands échantillons, on a une puissance quasi infinie et on peut détecter statistiquement de très petites différences qui ne sont pas nécessairement biologiquement significatives.\nEn utilisant un critère de signification plus strict (\\(\\alpha\\)&lt;0.05) on diminue notre puissance.\nEn voulant maximiser la puissance, on augmente l’effort requis, à moins d’utiliser une valeur critique plus libérale (\\(\\alpha&gt;0.05\\))\nLe choix de \\(\\beta\\) est quelque peu arbitraire. On considère que \\(\\beta=0.2\\) (puissance de 80%) est relativement élevé.",
    "crumbs": [
      "Données",
      "Fundamentals of stats",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Analyse de puissance avec R et G\\*Power</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html",
    "href": "31-reg_lin.html",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "",
    "text": "4.1 Extensions R et données requises pour le labo\nCe laboratoire nécessite:\nIl ne faut pas oublier de charge les extensions avec library() et de les installer au besoin avec install.packages() Pour les données, il faut les lire et les assigner à un objet R.\nlibrary(car)\nlibrary(lmtest)\nlibrary(boot)\nlibrary(ggplot2)\nlibrary(pwr)\nlibrary(performance)\n\nsturgeon &lt;- read.csv(\"data/sturgeon.csv\")",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#set-lm",
    "href": "31-reg_lin.html#set-lm",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "",
    "text": "les paquets R:\n\ncar\nlmtest\nboot\npwr\nggplot\n\n\nles fichiers de données\n\nsturgeon.csv\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotez que la ligne de code pour lire les données considère que le fichier de données se trouve dans un dossier data au sein de votre répertoire de travail. Si ce n’est pas le cas veuillez ajuster la ligne de commande.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#diagrammes-de-dispersion",
    "href": "31-reg_lin.html#diagrammes-de-dispersion",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "\n4.2 Diagrammes de dispersion",
    "text": "4.2 Diagrammes de dispersion\nLes analyses de corrélation et de régression devraient toujours commencer par un examen des données.C’est une étape critique qui sert à évaluer si ce type d’analyse est approprié pour un ensemble de données. Supposons que nous sommes intéressés à évaluer si la longueur d’esturgeons mâles dans la région de The Pas covarie avec leur poids. Pour répondre à cette question, regardons d’abord la corrélation entre la longueur et le poids. Souvenez-vous qu’une des conditions d’application de l’analyse de corrélation est que la relation entre les deux variables est linéaire. Pour évaluer cela, commençons par faire un diagramme de dispersion.\n\nLes données sur les esturgeons son disponibles dans le fichier sturgeon.csv. Après avoir chargé les données dnas un objet sturgeon, faites un diagramme de dispersion avec une droite de régression et une courbe LOWESS de la longueur en fonction du poids.\n\n\nsturgeon &lt;- read.csv(\"data/sturgeon.csv\")\nstr(sturgeon)\n\n'data.frame':   186 obs. of  9 variables:\n $ fklngth : num  37 50.2 28.9 50.2 45.6 ...\n $ totlngth: num  40.7 54.1 31.3 53.1 49.5 ...\n $ drlngth : num  23.6 31.5 17.3 32.3 32.1 ...\n $ rdwght  : num  15.95 NA 6.49 NA 29.92 ...\n $ age     : int  11 24 7 23 20 23 20 7 23 19 ...\n $ girth   : num  40.5 53.5 31 52.5 50 54.2 48 28.5 44 39 ...\n $ sex     : chr  \"MALE\" \"FEMALE\" \"MALE\" \"FEMALE\" ...\n $ location: chr  \"THE_PAS\" \"THE_PAS\" \"THE_PAS\" \"THE_PAS\" ...\n $ year    : int  1978 1978 1978 1978 1978 1978 1978 1978 1978 1978 ...\n\n\n\nmygraph &lt;- ggplot(\n  data = sturgeon[!is.na(sturgeon$rdwght), ], # source of data\n  aes(x = fklngth, y = rdwght)\n)\n# plot data points, regression, loess trace\nmygraph &lt;- mygraph +\n  stat_smooth(method = lm, se = FALSE, color = \"green\") + # add linear regression, but no SE shading\n  stat_smooth(color = \"red\", se = FALSE) + # add loess\n  geom_point() # add data points\n\nmygraph # display graph\n\n\n\nGraphique du poids en fonction de la longueur des esturgeons\n\n\n\n\n\nEst-ce que la dispersion des points suggère une bonne corrélation entre les deux variables? Est-ce que la relation semble linéaire?\n\nCe graphique suggère une tendance plus curvilinéaire que linéaire. Malgré tout, il semble y avoir une forte corrélation entre les deux variables.\n\nRefaites le diagramme de dispersion avec des axes logarithmiques.\n\n\n# apply log transformation on defined graph\nmygraph + scale_x_log10() + scale_y_log10()\n\n\n\nGraphique poids-longueur avec une échelle log\n\n\n\nComparez les diagrammes de dispersion avant et après transformation (Figures @ref(fig:stur-2) et @ref(fig:stur-log)). Comme l’analyse de corrélation présuppose une relation linéaire entre les variables, on devrait donc privilégier l’analyse sur les données log-transformées.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#transformations-et-le-coefficient-de-corrélation",
    "href": "31-reg_lin.html#transformations-et-le-coefficient-de-corrélation",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "\n4.3 Transformations et le coefficient de corrélation",
    "text": "4.3 Transformations et le coefficient de corrélation\nUne autre condition préalable à l’analyse de corrélation est que les deux variables concernées suivent une distribution normale bidimensionnelle. On peut aisément vérifier la normalité de chacune des 2 variables séparément tel que décrit dans le laboratoire précédent. Si les deux variables sont normalement distribuées, on présume généralement qu’elles suivent une distribution normale bidimensionnelle lorsqu’analysées simultanément (notez que ce n’est pas toujours le cas cependant).\n\nExaminez la distribution des quatre variables (les deux variables originales et les variables transformées). Que concluez-vous de l’inspection visuelle de ces graphiques ?\n\nLes figures ci-dessous sont les diagrammes de probabilité (qqplot()). Le code pour produire des graphiques multiples sur une page, comme on voit ci-dessous, est:\n\npar(mfrow = c(2, 2)) # divise le graphique en 4 sections\nqqnorm(sturgeon$fklngth, ylab = \"fklngth\")\nqqline(sturgeon$fklngth)\nqqnorm(log10(sturgeon$fklngth), ylab = \"log10(fklngth)\")\nqqline(log10(sturgeon$fklngth))\nqqnorm(sturgeon$rdwght, ylab = \"rdwght\")\nqqline(sturgeon$rdwght)\nqqnorm(log10(sturgeon$rdwght), ylab = \"log10(rdwgth)\")\nqqline(log10(sturgeon$rdwght))\n\n\n\n\n\n\npar(mfrow = c(1, 1)) # redéfinie la zone de graphique par défaut\n\nIl n’y a pas grand-chose à redire: aucune des distributions n’est parfaitement normale, mais les déviations semblent mineures.\n\nGénérez une matrice de graphiques de dispersion améliorés en utilisant la commande scatterplotMatrix de la librairie car.\n\n\nscatterplotMatrix(\n  ~ fklngth + log10(fklngth) + rdwght + log10(rdwght),\n  data = sturgeon,\n  smooth = TRUE, diagonal = \"density\"\n)\n\nWarning in applyDefaults(diagonal, defaults = list(method = \"adaptiveDensity\"),\n: unnamed diag arguments, will be ignored\n\n\n\n\n\n\n\n\n\nEnsuite, calculez le coefficient de corrélation de Pearson entre chaque paire (variables originales et logtransformées) en utilisant la commande cor(). Avant de commencer, on va cependant ajouter les variables transformées au tableau de données sturgeon:\n\n\nsturgeon$lfklngth &lt;- with(sturgeon, log10(fklngth))\nsturgeon$lrdwght &lt;- log10(sturgeon$rdwght)\n\nVous pouvez ensuite obtenir la matrice de corrélation par:\n\ncor(sturgeon[, c(\"fklngth\", \"lfklngth\", \"lrdwght\", \"rdwght\")], use = \"complete.obs\")\n\nFréquemment, il y a des données manquantes dans un échantillon. En choisissant use=\"complete.obs\", toutes les lignes du fichier pour lesquelles les variables ne sont pas toutes mesurées sont éliminées. Dans ce cas, toutes les corrélations seront calculées avec le même nombre de cas. Par contre, en utilisant use=\"pairwise.complete.obs\", R élimine une observation que lorsqu’un des deux membres de la paire a une valeur manquante. Dans ce cas, si les données manquantes pour différentes variables se retrouvent dans un groupe différent d’observation, les corrélations ne seront pas nécessairement calculées sur le même nombre de cas ni sur le même sous-ensemble de cas. En général, vous devriez utiliser l’option use=\"complete.obs\" à moins que vous ayez un très grand nombre de données manquantes et que cette façon de procéder élimine la plus grande partie de vos observations.\nPourquoi la corrélation entre les variables originales est-elle la plus faible des trois ?\n\ncor(sturgeon[, c(\"fklngth\", \"lfklngth\", \"lrdwght\", \"rdwght\")], use = \"complete.obs\")\n\n           fklngth  lfklngth   lrdwght    rdwght\nfklngth  1.0000000 0.9921435 0.9645108 0.9175435\nlfklngth 0.9921435 1.0000000 0.9670139 0.8756203\nlrdwght  0.9645108 0.9670139 1.0000000 0.9265513\nrdwght   0.9175435 0.8756203 0.9265513 1.0000000\n\n\nIl y a plusieurs choses à noter ici.\n\nPremièrement, la corrélation entre la longueur à la fourche et le poids rond est élevée, peu importe la transformation: les poissons lourds ont tendance à être longs.\nDeuxièmement, la corrélation est plus forte pour les données transformées que pour les données brutes.\n\nPourquoi? Parce que le coefficient de corrélation est inversement proportionnel au bruit autour de la relation linéaire. Si la relation est curvilinéaire (comme dans le cas des données non transformées), le bruit est plus grand que si la relation est parfaitement linéaire. Par conséquent, la corrélation est plus faible.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#matrices-de-corrélations-et-correction-de-bonferroni",
    "href": "31-reg_lin.html#matrices-de-corrélations-et-correction-de-bonferroni",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "\n4.4 Matrices de corrélations et correction de Bonferroni",
    "text": "4.4 Matrices de corrélations et correction de Bonferroni\nUne pratique courante est d’examiner une matrice de corrélation à la recherche des associations significatives. Comme exemple, essayons de tester si la corrélation entre lfklngth et rdwght est significative (c’est le plus faible coefficient de corrélation de cette matrice).\n\nEstimer la correlation entre la longueur (fklngth) et le poids (rdwght) des esturgeons:\n\n\ncor.test(\n  sturgeon$lfklngth, sturgeon$rdwght,\n  alternative = \"two.sided\",\n  method = \"pearson\"\n)\n\n\n    Pearson's product-moment correlation\n\ndata:  sturgeon$lfklngth and sturgeon$rdwght\nt = 24.322, df = 180, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8367345 0.9057199\nsample estimates:\n      cor \n0.8756203 \n\n\nOn voit ici que la corrélation est hautement significative (\\(p &lt; 2.2e-16\\)),ce qui n’est pas surprenant étant donné la valeur du coefficient de corrélation (0.8756). Il est important de réaliser que si une matrice contient un grand nombre de corrélations, il n’est pas surprenant d’en trouver au moins une qui soit “significative”. En effet, on s’attend à en trouver 5% en moyenne lorsqu’il n’y a en fait aucune corrélation entre les paires de moyennes. Une façon de corriger pour cette tendance est d’ajuster le niveau \\(\\alpha\\) critique auquel on attribue une signification statistique en divisant \\(\\alpha\\) par le nombre \\(k\\) de corrélations qui sont examinées : \\(\\alpha' =\n\\alpha / k\\) (ajustement de Bonferroni). Si initialement \\(\\alpha = 0.05\\) et qu’il y a 10 corrélations qui sont examinées, alors \\(\\alpha'= 0.005\\). Donc, afin de rejeter l’hypothèse nulle, la valeur de p devra être plus petite que \\(\\alpha'\\), en l’occurrence 0.005. Dans l’exemple qui précède, on devrait donc ajuster \\(\\alpha\\) critique en divisant par le nombre total de corrélations dans la matrice (6 dans ce cas, donc \\(\\alpha'=0.00833\\)). Cette correction modifie-t-elle votre conclusion quant à la corrélation entre lkfl et rdwght?",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#corrélations-non-paramétriques-r-de-spearman-et-tau-de-kendall",
    "href": "31-reg_lin.html#corrélations-non-paramétriques-r-de-spearman-et-tau-de-kendall",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "\n4.5 Corrélations non paramétriques: r de Spearman et \\(\\tau\\) de Kendall",
    "text": "4.5 Corrélations non paramétriques: r de Spearman et \\(\\tau\\) de Kendall\nL’analyse faite à la section précédente avec les esturgeons suggère que l’une des conditions préalables à l’analyse de corrélation, soit la distribution normale bidimensionnelle de données, pourrait ne pas être remplie pour fklngth et rdwght, ni pour les paires de variables transformées. La recherche d’une transformation appropriée peut parfois être difficile. Pire encore, pour certaines distributions il n’existe pas de transformation qui va normaliser les données. Dans ces cas-là, la meilleure option est de faire une analyse non paramétrique qui ne présume ni de la normalité ni de la linéarité. Ces analyses sont basées sur les rangs. Les deux plus communes sont le coefficient de rang de Spearman et le \\(\\tau\\) (tau) de Kendall.\n\nDans R, testez la corrélation entre fklngth et rdwght en utilisant Spearman et Kendall’s .\n\n\ncor.test(\n  sturgeon$lfklngth, sturgeon$rdwght,\n  alternative = \"two.sided\",\n  method = \"spearman\"\n)\n\nWarning in cor.test.default(sturgeon$lfklngth, sturgeon$rdwght, alternative =\n\"two.sided\", : Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  sturgeon$lfklngth and sturgeon$rdwght\nS = 47971, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.9522546 \n\n\n\ncor.test(\n  sturgeon$lfklngth, sturgeon$rdwght,\n  alternative = \"two.sided\",\n  method = \"kendall\"\n)\n\n\n    Kendall's rank correlation tau\n\ndata:  sturgeon$lfklngth and sturgeon$rdwght\nz = 16.358, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.8208065 \n\n\nComparer les résultats de cette analyse à l’analyse paramétrique. Pourquoi y-a-t’il une différence ?\nCalculez les corrélations non paramétriques sur les paires de variables transformées. Vous devriez voir tout de suite que les corrélations des données transformées et non transformées sont identiques puisque dans les deux cas la corrélation est calculée à partir des rangs qui ne sont pas affectés par la transformation.\nNotez que les corrélations obtenues avec le tau de Kendall (0.820) sont plus faibles que celles du coefficient de Spearman (0.952). Le tau de Kendall pondère un peu plus les grandes différences entre les rangs alors que le coefficient de Spearman donne le même poids à chaque paire d’observations. En général, on préfère le tau de Kendall lorsqu’il y a plus d’incertitude quant aux rangs qui sont près les uns des autres.\nLes esturgeons de cet échantillon ont été capturés à l’aide de filets et d’hameçons d’une taille fixe. Quel impact cette méthode de capture peut-elle avoir eu sur la forme de la distribution de fklngth et rdwght? Compte tenu de ces circonstances, l’analyse de corrélation est-elle appropriée ?\nRappelez-vous que l’analyse de corrélation présume aussi que chaque variable est échantillonnée aléatoirement. Dans le cas de nos esturgeons, ce n’est pas le cas: les hameçons appâtés et les filets ne capturent pas de petits esturgeons (et c’est pourquoi il n’y en a pas dans l’échantillon). Il faut donc réaliser que les coefficients de corrélation obtenus dans cette analyse ne reflètent pas nécessairement ceux de la population totale des esturgeons.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#régression-linéaire-simple",
    "href": "31-reg_lin.html#régression-linéaire-simple",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "\n4.6 Régression linéaire simple",
    "text": "4.6 Régression linéaire simple\nL’analyse de corrélation vise à décrire comment deux variables covarient. L’analyse de régression vise plutôt à produire un modèle permettant de prédire une variable (la variable dépendante) par l’autre (la variable indépendante).\nComme pour l’analyse de corrélation, on devrait commencer en examinant des graphiques. Puisque l’on est intéressé à quantifier la relation entre deux variables, un graphique de la variable dépendante (Y) en fonction de la variable indépendante (X) est tout à fait approprié.\n\nLe fichier sturgeon.csv contient des données d’un inventaire des esturgeons récoltés en 1978-1980 à Cumberland House en Saskatchewan et à The Pas au Manitoba. Faites un diagramme de dispersion de fklngth (la variable dépendante) en fonction de age (la variable indépendante) pour les esturgeons mâles unqiuement et ajoutez-y une régression linéaire et une trace lowess. Que concluez-vous de ce diagramme de dispersion ?\n\n\nsturgeon.male &lt;- subset(sturgeon, subset = sex == \"MALE\")\nmygraph &lt;- ggplot(\n  data = sturgeon.male, # source of data\n  aes(x = age, y = fklngth)\n) # aesthetics: y=fklngth, x=rdwght\n# plot data points, regression, loess trace\nmygraph &lt;- mygraph +\n  stat_smooth(method = lm, se = FALSE, color = \"green\") + # add linear regression, but no SE shading\n  stat_smooth(color = \"red\") + # add loess\n  geom_point() # add data points\nmygraph # display graph\n\n\n\n\n\n\n\nCe graphique suggère que la relation n’est pas linéaire.\nSupposons que nous désirions estimer le taux de croissance des esturgeons mâles. Un estimé (peut-être pas terrible…) du taux de croissance peut être obtenu en calculant la pente de la régression de la longueur à la fourche sur l’âge.\nAjustons d’abord une régression avec la commande lm() et sauvons ces résultats dans un objet appelé RegModel.1.\n\nRegModel.1 &lt;- lm(fklngth ~ age, data = sturgeon.male)\n\nRien n’apparait à l’écran, c’est normal ne vous inquiétez pas, tout a été sauvegardé en mémoire. Pour voir les résultats, tapez:\n\nsummary(RegModel.1)\n\n\nCall:\nlm(formula = fklngth ~ age, data = sturgeon.male)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.4936 -2.2263  0.1849  1.7526 10.8234 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 28.50359    1.16873   24.39   &lt;2e-16 ***\nage          0.70724    0.05888   12.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.307 on 73 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.664, Adjusted R-squared:  0.6594 \nF-statistic: 144.3 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\nla sortie R donne:\n\n\nCall: Le modèle qui a été ajusté et les données utilisées.\n\nResiduals: Un sommaire statistique des résidus autour du modèle estimé.\n\nCoefficients: Valeurs estimées des paramètres du modèle, erreurs-types, statistiques t et probabilités associées.\n\nResidual standard error: Racine carrée de la variance résiduelle.\n\nMultiple R-squared: Coefficient de détermination. Il correspond à la proportion de la variabilité de la variable dépendante qui peut être expliquée par la régression.\n\nAdjusted R-squared: Le R-carré ajusté tient compte du nombre de paramètres du modèle. Si vous voulez comparer différents modèles qui n’ont pas le même nombre de paramètres, c’est ce qu’il faut utiliser.\n\nF-statistic: C’est le test de signification omnibus du modèle. Dans le cas de la régression simple, il est équivalent au test sur la pente de la régression.\n\nLa régression estimée est donc:\n\\[ Fklngth = 28.50359 + 0.70724 * age\\]\nÉtant donné la valeur significative du test de F ainsi que pour le test de t pour la pente de la droite, on rejette l’hypothèse nulle qu’il n’y a pas de relation entre la taille et l’âge.\n\n4.6.1 Vérifier les conditions d’application de la régression\nLa régression simple de type I a quatre conditions préalables :\n\nil n’y a pas d’erreur de mesure sur la variable indépendante (X)\nla relation entre Y et X est linéaire\nles résidus sont normalement distribués\nla variance des résidus est constante pour toutes les valeurs de la variable indépendante\n\nProcédons maintenant à l’examen post-mortem. La première condition est rarement remplie avec des données biologiques ; il y presque toujours de l’erreur sur X et sur Y. Cela veut dire qu’en général les pentes estimées sont biaisées, mais que les valeurs prédites ne le sont pas. Toutefois, si l’erreur de mesure sur X est petite par rapport à l’étendue des valeurs de X, le résultat de l’analyse n’est pas dramatiquement influencé. Par contre, si l’erreur de mesure est relativement grande (toujours par rapport à l’étendue des valeurs de X), la droite de régression obtenue par la régression de modèle I est un piètre estimé de la relation fonctionnelle entre X et Y. Dans ce cas, il est préférable de passer à la régression de modèle II, malheureusement au-delà du contenu de ce cours. Les autres conditions préalables à l’analyse de régression de modèle I peuvent cependant être vérifiées, ou du moins évaluées visuellement. La commande plot() permet de visualiser des graphiques diagnostiques pour des modèles linéaires.\n\npar(mfrow = c(2, 2), las = 1)\nplot(RegModel.1)\n\nLa commande par() est utilisée pour dire à R de tracer 2 rangées et 2 colonnes de graphiques par page (il y a quatre graphiques diagnostiques qui sont générés automatiquement pour les modèles linéaires), et la commande las indique à R d’effectuer une rotation des légendes de l’axe des Y pour qu’elles soient perpendiculaires à l’axe (oui. Je sais. Rien de tout ça n’est évident.)\nVous obtiendrez:\n\n\n\n\n\n\n\n\n\nEn haut à gauche, permet d’évaluer la linéarité, la normalité, et l’homoscédasticité des résidus. Il illustre les déviations autour de la régression en fonction des valeurs prédites. Rappllez-vous que le graphique de fklngth vs age suggère que la relation entre la longueur à la fourche et l’âge n’est pas linéaire. Les très jeunes et très vieux esturgeons sont sous la droite en général, alors que les esturgeons d’âge moyen sont retrouvés généralement au-dessus de la droite de régression. C’est exactement ce que le graphique des résidus en fonction des valeurs prédites illustre. La ligne en rouge est une trace lowess au travers de ce nuage de points. Si la relation était linéaire, la trace lowess serait presque plate et près de 0. La dispersion des résidus permet d’évaluer visuellement leur normalité et hétéroscédasticité; mais ce graphique n’est pas optimal pour évaluer ces propriétés. Les deux graphiques suivants sont supérieurs au premier pour cela.\nEn haut à droite permet d’évaluer la normalité des résidus. C’est un graphique QQ des résidus (QQ plot). Des résidus distribués normalement tomberaient exactement sur la diagonale. Ici, on voit que c’est presque le cas, sauf dans les queues de la distribution.\nEn bas à gauche, intitulé Scale-Location, permet d’évaluer l’homoscedasticité. On y retrouve sur l’ordonnée (l’axe des y) la racine carrée de la valeur absolue des résidus standardisés (résidus divisés par l’écart-type des résidus) en fonction des valeurs prédites. Le graphique aide à déterminer si la variation des résidus est constante ou non. Si les résidus sont homoscédastiques, la valeur moyenne sur l’axe des y ne va pas changer en fonction de la valeur prédite. Ici, il y a une certaine tendance, mais pas une tendance monotone puisqu’ il y a d’abord une baisse puis une hausse..; bref, rien qui soit une forte évidence contre la supposition d’homoscédasticité.\nEn bas à droite, montre les résidus en fonction du “leverage” et permet de détecter certaines valeurs extrêmes qui ont une grande influence sur la régression. Le leverage d’un point mesure sa distance des autres points, mais seulement en ce qui concerne les variables indépendantes. Dans le cas d’une régression simple, cela revient à la distance entre le point sur l’axe des x et la moyenne de tous les points sur cet axe. Vous devriez porter une attention particulière aux observations qui ont un leverage plus grand que \\(2(k+1)/n\\), où k est le nombre de variables indépendantes (ici, 1) et n est le nombre d’observations. Dans cet exemple, il y a 75 observations et une variable indépendante et donc les points ayant un leverage plus grand que \\(4 / 75 =  0.053\\) devrait être considérés avec attention. Le graphique indique également comment la régression changerait si on enlevait un point. Ce changement est mesuré par la distance de Cook, illustrée par les bandes en rouge sur le graphique. Un point ayant une distance de Cook supérieure à 1 a une grande influence.\n\n\n\n\n\n\n\nAvertissement\n\n\n\nNotez que R identifie automatiquement les cas les plus extrèmes sur chacun de ces 4 graphiques. Le fait qu’un point soit identifié ne signifie pas nécessairement que c’est une valeur réellement extrème, ou que vous devez vous en préoccuper. Dans tous les ensembles de données il y aura toujours un résidu plus grand que les autres…\n\n\nIl est possible d’obtenir des graphiques d’évalutions des conditions d’applications, qui sont plus simple à interpréter et plus joli (avec des couleurs). Il faut utiliser la fonction model_check() dans le 📦 performance.\n\ncheck_model(RegModel.1)\n\n\n\n\n\n\n\nFinalement, quel est le verdict concernant la régression linéaire entre fklngth et age ? Elle viole la condition de linéarité, possiblement celle de normalité, remplit la condition d’homoscédasticité, et ne semble pas influencée outre mesure par des valeurs bizarres ou extrêmes.\n\n4.6.2 Tests formels des conditions d’application pour la régression\nPersonnellement, je n’utilise jamais les tests formels des conditions d’application de la régression et me contente des graphiques des résidus pour guider mes décisions. C’est ce que la plupart des praticiens font. Cependant, lors de mes premières analyses, je n’étais pas toujours certain de bien interpréter les graphiques et j’aurais aimé un indice plus formel ou un test permettant de détecter les violations des conditions d’application de la régression.\nLe package lmtest, qui ne fait pas partie de l’installation de base, mais qui est disponible sur CRAN, permet de faire plusieurs tests de linéarité et d’homoscédasticité. Et on peut tester la normalité avec le test Shapiro-Wilk test vu précédemment.\nCharger le package lmtest de CRAN (et installer le si besoin).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(lmtest)\n\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nExécutez les commandes suivantes\n\n\n\nbptest(RegModel.1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  RegModel.1\nBP = 1.1765, df = 1, p-value = 0.2781\n\n\nLe test Breusch-Pagan test examine si la variabilité des résidus est constantes lorsque les valeurs prédites changent. Une faible valeur de p suggère de l’hétéroscédasticité. Ici, la valeur p est élevée et suggère que la condition d’application d’homoscédasticité est remplie avec ces données.\n\ndwtest(RegModel.1)\n\n\n    Durbin-Watson test\n\ndata:  RegModel.1\nDW = 2.242, p-value = 0.8489\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nLe test Durbin-Watson permet de détecter l’autocorrélation sérielle des résidus. En l’absence d’autocorrélation (i.e. d’indépendance des résidus) la valeur attendue de la statistique D est 2. Ce test permet d’éprouver l’hypothèse d’indépendance des résidus, mais ne permet de détecter qu’un type particulier de dépendance. Ici, le test ne permet pas de rejeter l’hypothèse d’indépendance.\n\nresettest(RegModel.1)\n\n\n    RESET test\n\ndata:  RegModel.1\nRESET = 14.544, df1 = 2, df2 = 71, p-value = 5.082e-06\n\n\nLe test RESET permet d’éprouver la linéarité. Si la relation est linéaire, alors la statistique RESET sera d’environ 1. Ici, la statistique est beaucoup plus élevée (14.54) et hautement significative. Le test confirme la tendance que nous avons détectée visuellement plus haut: la relation n’est pas linéaire.\n\nshapiro.test(residuals(RegModel.1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(RegModel.1)\nW = 0.98037, p-value = 0.2961\n\n\nLe test de normalité Shapiro-Wilk sur les résidus confirme que la déviation par rapport à une distribution normale des résidus n’est pas grande.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#transformation-des-données-en-régression",
    "href": "31-reg_lin.html#transformation-des-données-en-régression",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "\n4.7 Transformation des données en régression",
    "text": "4.7 Transformation des données en régression\nLa relation entre fklngth et age n’étant pas linéaire, on devrait donc essayer de transformer les données pour tenter de les linéariser :\n\nVoyons ce qu’une transformation log donne:\n\n\npar(mfrow = c(1, 1), las = 1)\nggplot(\n  data = sturgeon.male,\n  aes(x = log10(age), y = log10(fklngth))\n) +\n  geom_smooth(color = \"red\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"green\") +\n  geom_point()\n\n\n\n\n\n\n\nAjustons maintenant une régression simple sur ces données transformées.\n\nRegModel.2 &lt;- lm(log10(fklngth) ~ log10(age), data = sturgeon.male)\nsummary(RegModel.2)\n\n\nCall:\nlm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.082794 -0.016837 -0.000719  0.021102  0.087446 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19199    0.02723   43.77   &lt;2e-16 ***\nlog10(age)   0.34086    0.02168   15.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03015 on 73 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.772, Adjusted R-squared:  0.7688 \nF-statistic: 247.1 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\nExaminons maintenant les graphiques diagnostiques:\n\npar(mfrow = c(2, 2), las = 1)\nplot(RegModel.2)\n\n\n\n\n\n\n\nIl y a une certaine amélioration, mais ce n’est pas encore parfait (la perfection n’est pas de ce monde….). Le graphique des résidus en fonction des valeurs prédites suggère encore une certaine non linéarité. Sur le graphique Q-Q les points se retrouvent plus près de la droite diagonale qu’avant, indiquant que les résidus sont encore plus près de la normalité après la transformation log-log. Il n’y a pas d’indice d’hétéroscédasticité. Finalement, même si il reste quelques points avec plus d’influence (leverage) que les autres, aucun n’a une distance de Cook au-delà de 0.5. En résumé, la transformation log a amélioré les choses: relation est plus linéaire, les résidus sont plus normaux, et il y a moins de points avec une influence relativement élevée.Est-ce que les tests formels supportent cette évaluation?\n\nbptest(RegModel.2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  RegModel.2\nBP = 0.14282, df = 1, p-value = 0.7055\n\ndwtest(RegModel.2)\n\n\n    Durbin-Watson test\n\ndata:  RegModel.2\nDW = 2.1777, p-value = 0.6134\nalternative hypothesis: true autocorrelation is greater than 0\n\nresettest(RegModel.2)\n\n\n    RESET test\n\ndata:  RegModel.2\nRESET = 4.4413, df1 = 2, df2 = 71, p-value = 0.01523\n\nshapiro.test(residuals(RegModel.2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(RegModel.2)\nW = 0.98998, p-value = 0.8246\n\n\nOui, les conclusions sont les mêmes: les résidus sont encore homoscédastiques (test Breusch-Pagan), ne sont pas autocorrélés (test Durbin-Watson), sont normaux (test Shapiro-Wilk), et sont plus linéaires (la valeur de P du test RESET est maintenant 0.015, au lieu de 0.000005). Donc la linéarité a augmenté, mais cette condition d’application semble encore légèrement violée.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#traitement-des-valeurs-extrèmes",
    "href": "31-reg_lin.html#traitement-des-valeurs-extrèmes",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "\n4.8 Traitement des valeurs extrèmes",
    "text": "4.8 Traitement des valeurs extrèmes\nDans cet exemple, il n’y a pas de valeur vraiment extrème. Oui, je sais, R a quand même identifié les observations 8, 24, et 112 dans le dernier graphique diagnostique. Mais ces valeurs sont encore dans la fourchette de valeurs que je juge “acceptables”. Mais comment déterminer objectivement ce qui est acceptable? À quel moment juge t’on qu’une valeur extrême est vraiment trop invraisemblable pour ne pas l’exclure? Il n’y a malheureusement pas de règle absolue là-dessus. Les opinions varient, mais je penche vers le conservatisme sur cette question.\nMa position est que, à moins que la valeur soit biologiquement impossible ou clairement une erreur d’entrée de données, je n’élimine pas les valeurs extrêmes et j’utilise toutes mes données dans leur analyse. Pourquoi?\nParce que je veux que mes données reflètent bien la variabilité naturelle ou réelle. C’est d’ailleurs parfois cette variabilité qui est intéressante.\nL’approche conservatrice qui consiste à conserver toutes les valeurs extrêmes possibles est possiblement la plus honnête, mais elle peut causer certains problèmes. Ces valeurs extrêmes sont souvent la cause des violations des conditions d’application des tests statistiques. La solution suggérée à ce dilemme est de faire l’analyse avec et sans les valeurs extrêmes et de comparer les conclusions. Dans bien des cas, les conclusions seront qualitativement les mêmes et les tailles d’effet ne seront pas très différentes. Toutefois, dans certains cas, la présence des valeurs extrêmes change complètement les conclusions. Dans ces cas, il faut simplement accepter que les conclusions dépendent entièrement de la présence des valeurs extrêmes et sont donc peu concluantes.\nSuivant cette approche comparative, refaisons donc l’analyse après avoir enlevé les observations 8, 24, et 112.\n\nRegModel.3 &lt;- lm(log10(fklngth) ~ log10(age), data = sturgeon.male, subset = !(rownames(sturgeon.male) %in% c(\"8\", \"24\", \"112\")))\nsummary(RegModel.3)\n\n\nCall:\nlm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male, \n    subset = !(rownames(sturgeon.male) %in% c(\"8\", \"24\", \"112\")))\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.069163 -0.017390  0.000986  0.018590  0.047647 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.22676    0.02431   50.46   &lt;2e-16 ***\nlog10(age)   0.31219    0.01932   16.16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02554 on 70 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.7885,    Adjusted R-squared:  0.7855 \nF-statistic:   261 on 1 and 70 DF,  p-value: &lt; 2.2e-16\n\n\nL’ordonnée à l’origine (Intercept), la pente, et le R carré sont presque les mêmes, et la valeur de p est encore astronomiquement petite. Enlever les valeurs extrêmes a peu d’effet dans ce cas.\nLes graphiques diagnostiques des résidus et les tests formels des conditions d’application sur ce sous-ensemble de données donnent:\n\npar(mfrow = c(2, 2))\nplot(RegModel.3)\n\n\n\n\n\n\nsturgeon.male.subset &lt;- subset(sturgeon, subset = !(rownames(sturgeon.male) %in% c(\"8\", \"24\", \"112\")))\nbptest(RegModel.3)\n\n\n    studentized Breusch-Pagan test\n\ndata:  RegModel.3\nBP = 0.3001, df = 1, p-value = 0.5838\n\ndwtest(RegModel.3)\n\n\n    Durbin-Watson test\n\ndata:  RegModel.3\nDW = 2.0171, p-value = 0.5074\nalternative hypothesis: true autocorrelation is greater than 0\n\nresettest(RegModel.3)\n\n\n    RESET test\n\ndata:  RegModel.3\nRESET = 3.407, df1 = 2, df2 = 68, p-value = 0.0389\n\nshapiro.test(residuals(RegModel.3))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(RegModel.3)\nW = 0.98318, p-value = 0.4502\n\n\nIl n’y a pas vraiment de différence ici non plus avec l’analyse des données en entier. Bref, tout pointe vers la conclusion que les valeurs les plus extrêmes de cet ensemble de donnée n’influencent pas indûment les résultats statistiques.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#quantifier-la-taille-deffet-et-analyse-de-puissance-en-régression",
    "href": "31-reg_lin.html#quantifier-la-taille-deffet-et-analyse-de-puissance-en-régression",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "\n4.9 Quantifier la taille d’effet et analyse de puissance en régression",
    "text": "4.9 Quantifier la taille d’effet et analyse de puissance en régression\nL’interprétation biologique des résultats n’est pas la même chose que l’interprétation statistique. Dans l’analyse qui précède, on conclue statistiquement que la taille augmente avec l’âge (puisque la pente est positive et et p&lt;0.05). Mais cette augmentation “statistique” de la taille avec l’âge ne donne pas d’information sur la différence de taille entre les jeunes et vieux individus. La pente et un graphique sont plus informatifs à ce sujet que la valeur p. La pente (dans l’espace log-log) est 0.34. Cela veut dire que pour chaque unité d’accroissement de X (log10(age)), il y a une augmentation de 0.34 unités de log10(fklngth). En d’autres mots, quand l’âge est multiplié par 10, la longueur à la fourche est multipliée environ par 2 (100.34). Donc la longueur des esturgeons augmente plus lentement que leur âge (contrairement à mon tour de taille, semble-t-il….). La valeur de la pente (0.34) est un estimé de la taille de l’effet de l’âge sur la longueur.\nIl est aussi importnat d’estimer l’intervalle de confiance sur la pente pour pouvoir estimer si l’intervalle inclus ou non que des valeurs biologiquement importantes. Cela peut être fait simplement avec la fonction confint().\n\nconfint(RegModel.2)\n\n                2.5 %   97.5 %\n(Intercept) 1.1377151 1.246270\nlog10(age)  0.2976433 0.384068\n\n\nL’intervalle de confiance à 95% de la pente est 0.29-0.38. L,intervalle de confiance est assez faible et éloigné de zéro.\n\n4.9.1 Puissance de détecter une pente donnée\nPour les calculs de puissance avec G*Power vous devrez cependant utiliser une autre métrique de la taille de l’effet, calculée à partir de la pente, de son erreur-type, et de la taille de l’échantillon (ce qui facilite les calculs pour G*Power, mais malheureusement pas pour vous ;-) La métrique (d) est calculée comme: \\[ d = \\frac{b}{s_b\\sqrt{n-k-1}} \\] où \\(b\\) est l’estimé de la pente, \\(s_b\\) est l’erreur type de la pente, \\(n\\) est le nombre d’observations, et \\(k\\) est le nombre de variables indépendantes (1 pour la régression linéaire simple).\nVous pouvez calculer approximativement la puissance avec G*Power pour une valeur de pente que vous jugez assez grande pour mériter d’être détectée. Allez à Tests: Means: One group: difference from constant, là, vous devrez remplacer la valeur de \\(b\\) dans l’équation pour la taille d’effet (d) par la pente que vous voudriez détecter, mais utiliser l’erreur type calculée à partir de vos données.\nPar exemple, supposons que les ichthyologues considèrent qu’une pente de 0.1 pour la relation entre log10(fklngth) et log10(age) est signifiante biologiquement, et qu’ils désirent estimer la puissance de détecter une telle pente à partir d’un échantillon de 20 esturgeons. Les résultats de la régression log-log nous fournissent ce dont on a besoin:\n\nsummary(RegModel.2)\n\n\nCall:\nlm(formula = log10(fklngth) ~ log10(age), data = sturgeon.male)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.082794 -0.016837 -0.000719  0.021102  0.087446 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.19199    0.02723   43.77   &lt;2e-16 ***\nlog10(age)   0.34086    0.02168   15.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03015 on 73 degrees of freedom\n  (5 observations deleted due to missingness)\nMultiple R-squared:  0.772, Adjusted R-squared:  0.7688 \nF-statistic: 247.1 on 1 and 73 DF,  p-value: &lt; 2.2e-16\n\n\nL’erreur-type de la pente est 0.02168. Il y avait 75 poissons (n=75) dans l’échantillon de départ. On peut donc calculer la métrique de taille d’effet pour G*Power \\[ d = \\frac{b}{s_b\\sqrt{n-k-1}} = \\frac{0.1}{0.02168\\sqrt{74-1-1}}=0.54\\]\nArmés de cette taille d’effet (une pente présumée de 0.1 et une variabilité autour de la régression similaire à la régression de fklngth vs age), aller à Tests: Means: One group: difference from constant, et entrez la valeur calculée de d, alpha, et l’effectif de l’échantillon pour calculer la puissance.\n\n\n\n\nAnalyse de puissance pour N = 20 et pente = 0.1\n\n\n\nDans R, il est possible de faire cette analyse avec le code suivant:\n\nlibrary(pwr)\n\n# analyse de puissance\npwr.t.test(n = 20, d = 0.54, sig.level = 0.05, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 20\n              d = 0.54\n      sig.level = 0.05\n          power = 0.6299804\n    alternative = two.sided\n\n\nLa puissance de détecter une pente comme étant statistiquement significative (au niveau alpha), si la pente est 0.1, que la variabilité résiduelle autour de la régression est semblable à celle de notre échantillon (ce qui revient à une taille d’effet de 0.54, pour un échantillon de 20 esturgeons et alpha=0.05) est de 0.629. Seulement environ 2/3 des échantillons de cette taille détecteraient un effet significatif de l’âge sur fklngth.\n\n4.9.2 Effectif requis pour atteindre une puissance désirée (test A-priori)\nPour estimer la taille d’échantillon (effectif) requis pour avoir une puissance de 99% de détecter un effet de l’âge si la pente est 0.1 (sur une échelle log-log), avec alpha=0.05, on utilise la même valeur de d (0.54):\n\n\n\n\nAnalyse à priori pour déterminer la taille d’échantillon pour une puissance de 0.99\n\n\n\nDans R, il est possible de faire cette analyse avec le code suivant:\n\nlibrary(pwr)\n\n# analyse de puissance\npwr.t.test(n = 65, d = 0.54, sig.level = 0.05, type = \"one.sample\")\n\n\n     One-sample t test power calculation \n\n              n = 65\n              d = 0.54\n      sig.level = 0.05\n          power = 0.9900297\n    alternative = two.sided\n\n\nEn augmentant la taille de l’échantillon à 65, selon le même scénario que précédemment, la puissance augmente à 99%.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "31-reg_lin.html#bootstrap-en-régression-simple-avec-r",
    "href": "31-reg_lin.html#bootstrap-en-régression-simple-avec-r",
    "title": "\n4  Corrélation et régression linéaire simple\n",
    "section": "\n4.10 Bootstrap en régression simple avec R",
    "text": "4.10 Bootstrap en régression simple avec R\nUn test non paramétrique pour l’ordonnée à l’origine et la pente d’une régression simple peut être effectué par bootstrap.\n\n# charger le paquet boot\nlibrary(boot)\n# obtenir les poids de régression\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ] # allows boot to select sample\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n# bootstrapping with 1000 replications\nresults &lt;- boot(\n  data = sturgeon.male,\n  statistic = bs,\n  R = 1000, formula = log10(fklngth) ~ log10(age)\n)\n# view results\nresults\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = sturgeon.male, statistic = bs, R = 1000, formula = log10(fklngth) ~ \n    log10(age))\n\n\nBootstrap Statistics :\n     original        bias    std. error\nt1* 1.1919926  0.0011567837  0.03387942\nt2* 0.3408557 -0.0007755788  0.02683725\n\n\nPour chaque paramètre du modèle (ici l’ordonnée à l’origine est appelée t1* et la pente de la régression t2*), R imprime :\n\n\noriginal la valeur estimée sur tout l’échantillon\n\nbias la différence entre la valeur moyenne des estimés par bootstrap et la valeur originale sur tout l’échantillon\n\nstd. error l’erreur-type de l’estimé bootstrap\n\n\npar(mfrow = c(2, 2))\nplot(results, index = 1) # intercept\n\n\n\n\n\n\nplot(results, index = 2) # log10(age)\n\n\n\n\n\n\n\nLa distribution des estimés obtenus par bootstrap est assez normale dans cet exemple, avec de petites déviations dans les queuee de la distribution (là où ça compte pour les intervalles de confiance…). On pourrait utiliser l’erreur-type des estimés bootstrap pour calculer un intervalle de confiance symétrique (moyenne +- t ET). Cependant, comme R peut facilement calculer des intervalles de confiance qui corrigent pour le biais (BCa) ou encore des intervalle empiriques à partir des distributions simulées (méthode Percentile) il peut être aussi simple de les calculer selon les 3 méthodes:\n\n# interval de confiance pour l'ordonnée à l'origine\nboot.ci(results, type = \"all\", index = 1)\n\nWarning in boot.ci(results, type = \"all\", index = 1): bootstrap variances\nneeded for studentized intervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"all\", index = 1)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 1.124,  1.257 )   ( 1.120,  1.258 )  \n\nLevel     Percentile            BCa          \n95%   ( 1.125,  1.264 )   ( 1.114,  1.251 )  \nCalculations and Intervals on Original Scale\n\n\n\n# intervalle de confiance pour la pente\nboot.ci(results, type = \"all\", index = 2)\n\nWarning in boot.ci(results, type = \"all\", index = 2): bootstrap variances\nneeded for studentized intervals\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"all\", index = 2)\n\nIntervals : \nLevel      Normal              Basic         \n95%   ( 0.2890,  0.3942 )   ( 0.2894,  0.3968 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.2849,  0.3923 )   ( 0.2937,  0.4029 )  \nCalculations and Intervals on Original Scale\n\n\nIci, les 4 types d’intervalles de confiance que R a calculé sont essentiellement semblables. Si les données avaient violé plus sévèrement les conditions d’application de la régression (normalité, homoscedasticité), alors les différentes méthodes (Normal, Basic, Percentile, et BCa) auraient divergé un peu plus. Lequel choisir alors? BCa est celui qui est préféré de la majorité des praticiens, présentement.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Corrélation et régression linéaire simple</span>"
    ]
  },
  {
    "objectID": "32-t_test.html",
    "href": "32-t_test.html",
    "title": "\n5  Comparaison de deux échantillons\n",
    "section": "",
    "text": "5.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:\nlibrary(car)\nlibrary(lmtest)\nlibrary(boot)\nlibrary(lmPerm)\nlibrary(ggplot2)\nsturgeon &lt;- read.csv(\"data/sturgeon.csv\")\nskull &lt;- read.csv(\"data/skulldat_2020.csv\")",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#set-t",
    "href": "32-t_test.html#set-t",
    "title": "\n5  Comparaison de deux échantillons\n",
    "section": "",
    "text": "les paquets R:\n\ncar\nlmtest\nboot\nlmPerm\n\n\nles fichiers de données\n\nsturgeon.csv\nskulldat_2020.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#examen-visuel-des-données",
    "href": "32-t_test.html#examen-visuel-des-données",
    "title": "\n5  Comparaison de deux échantillons\n",
    "section": "\n5.2 Examen visuel des données",
    "text": "5.2 Examen visuel des données\nUne des premières étapes dans toute analyse de données est l’examen visuel des données par des graphiques et statistiques sommaires pour détecter les distributions sous-jacentes, les valeurs extrêmes et les tendances dans vos données. Cela commence souvent avec des graphiques de vos données (histogrammes, diagrammes de probabilité, Box plots, etc.) qui vous permettent d’évaluer si vos données sont normales, si elles sont corrélées les unes aux autres, ou s’il y a des valeurs suspectes dans le fichier.\nSupposons que l’on veuille comparer la distribution en taille des esturgeons de The Pas et Cumberland House. La variable fklngth dans le fichier sturgeon.csv représente la longueur (en cm) à la fourche de chaque poisson mesurée de l’extrémité de la tête à la base de la fourche de la nageoire caudale. Pour commencer, examinons si cette variable est normalement distribuée. On ne va pas tester pour la normalité à ce stade-ci; la présomption de normalité dans les analyses paramétriques s’applique aux résidus et non aux données brutes. Cependant, si les données brutes ne sont pas normales, vous avez d’habitude une très bonne raison de soupçonner que les résidus vont aussi ne pas avoir une distribution normale.\nUne excellente façon de comparer visuellement une distribution à la distribution normale est de superposer un histogramme des données observées à une courbe normale. Pour ce faire, il faut procéder en deux étapes :\n\nindiquer à R que nous voulons créer un histogramme superposé à une courbe normale\nspécifier qu’on veut que les graphiques soient faits pour les deux sites\n\n\nEn utilisant les données du fichier sturgeon.csv, générez les histogrammes et les approximations des distributions normales ajustées aux données de fklngth à The Pas et Cumberland House.\n\n\n# use \"sturgeon\" dataframe to make plot called mygraph\n# and define x axis as representing fklngth\nmygraph &lt;- ggplot(\n  data = sturgeon,\n  aes(x = fklngth)\n) +\n  xlab(\"Fork length (cm)\")\n# add data to the mygraph ggplot\nmygraph &lt;- mygraph +\n  geom_density() + # add data density smooth\n  geom_rug() + # add rug (bars at the bottom of the plot)\n  geom_histogram( # add black semitransparent histogram\n    aes(y = ..density..),\n    color = \"black\", alpha = 0.3\n  ) +\n  # add normal curve in red, with mean and sd from fklength\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(sturgeon$fklngth),\n      sd = sd(sturgeon$fklngth)\n    ),\n    color = \"red\"\n  )\n# display graph, by location\nmygraph + facet_grid(. ~ location)\n\n\n\nDistribution de la longueur des esturgeons\n\n\n\nExaminez ce graphique et essayez de déterminer si ces deux échantillons sont normalement distribués. À mon avis, cette variable est approximativement normalement distribuée dans les deux échantillons. Puisque ce qui nous intéresse est de comparer la taille des poissons de deux sites différents, c’est probablement une bonne idée de créer un graphique qui compare les deux groupes de données. Un Box plot convient très bien pour cette tâche.\n\nTracez un boxplot de fklngth groupé par location. Que concluez-vous quant à la différence entre les deux sites?\n\n\nggplot(data = sturgeon, aes(\n  x = location,\n  y = fklngth\n)) +\n  geom_boxplot(notch = TRUE)\n\n\n\nBoxplot de la longueur des esturgeons\n\n\n\nIl n’y a pas de grande différence de taille entre les deux sites, mais la taille des poissons à The Pas est plus variable ayant une plus large étendue de taille et des valeurs extrêmes (définies par les valeurs qui sont &gt; 1.5*l’étendue interquartile) à chaque bout de la distribution.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#comparer-les-moyennes-de-deux-échantillons-indépendants",
    "href": "32-t_test.html#comparer-les-moyennes-de-deux-échantillons-indépendants",
    "title": "\n5  Comparaison de deux échantillons\n",
    "section": "\n5.3 Comparer les moyennes de deux échantillons indépendants",
    "text": "5.3 Comparer les moyennes de deux échantillons indépendants\nÉprouvez l’hypothèse nulle d’égalité de la longueur à la fourche à The Pas et Cumberland House de 3 manières différentes:\n\nparamétriques supposant des variances égales\nparamétriques supposant des variances différentes\nnon-paramétrique (pas de conditions d’aplications sur la distribution et la variance)\n\nQue concluez-vous?\n\n# t-test assuming equal variances\nt.test(\n  fklngth ~ location,\n  data = sturgeon,\n  alternative = \"two.sided\",\n  var.equal = TRUE\n)\n\n\n    Two Sample t-test\n\ndata:  fklngth by location\nt = 2.1359, df = 184, p-value = 0.03401\nalternative hypothesis: true difference in means between group CUMBERLAND and group THE_PAS is not equal to 0\n95 percent confidence interval:\n 0.1308307 3.2982615\nsample estimates:\nmean in group CUMBERLAND    mean in group THE_PAS \n                45.08439                 43.36984 \n\n\n\n# t-test assuming unequal variances\nt.test(\n  fklngth ~ location,\n  data = sturgeon,\n  alternative = \"two.sided\",\n  var.equal = FALSE\n)\n\n\n    Welch Two Sample t-test\n\ndata:  fklngth by location\nt = 2.2201, df = 169.8, p-value = 0.02774\nalternative hypothesis: true difference in means between group CUMBERLAND and group THE_PAS is not equal to 0\n95 percent confidence interval:\n 0.1900117 3.2390804\nsample estimates:\nmean in group CUMBERLAND    mean in group THE_PAS \n                45.08439                 43.36984 \n\n\n\n# test non paramétrique\nwilcox.test(\n  fklngth ~ location,\n  data = sturgeon,\n  alternative = \"two.sided\"\n)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  fklngth by location\nW = 4973, p-value = 0.06296\nalternative hypothesis: true location shift is not equal to 0\n\n\nEn se fiant au test de t, on rejette donc l’hypothèse nulle. Il y a une différence significative entre les deux moyennes des longueurs à la fourche.\nNotez que si l’on se fie au test de Wilcoxon, il faut accepter l’hypothèse nulle. Les deux tests mènent donc à des conclusions contradictoires. La différence significative obtenue par le test de t peut provenir en partie d’une violation des conditions d’application du test (normalité et homoscédasticité). D’un autre côté, l’absence de différence significative selon le test de Wilcoxon pourrait être due au fait que, pour un effectif donné, la puissance du test non paramétrique est inférieure à celle du test paramétrique correspondant. Compte tenu 1) des valeurs de p obtenues pour les deux tests, et 2) le fait que pour des grands échantillons (des effectifs de 84 et 101 sont considérés grands) le test de t est considéré robuste, il est raisonnable de rejeter l’hypothèse nulle.\nAvant d’accepter les résultats du test de t et de rejeter l’hypothèse nulle qu’il n’y a pas de différences de taille entre les deux sites, il est important de déterminer si les données remplissent les conditions de normalité des résidus et d’égalité des variances. L’examen préliminaire suggérait que les données sont à peu près normales mais qu’il y avait peut-être des problèmes avec les variances (puisque l’étendue des données pour The Pas était beaucoup plus grande que celle pour Cumberland). On peut examiner ces conditions d’application plus en détail en examinant les résidus d’un modèle linéaire et en utilisant les graphiques diagnostiques:\n\nm1 &lt;- lm(fklngth ~ location, data = sturgeon)\npar(mfrow = c(2, 2))\nplot(m1)\n\n\n\nCondition d’application du modèle linéaire\n\n\n\nLe premier graphique ci-dessus montre comment les résidus se distribuent autour des valeurs prédites (les moyennes) pour chaque site et permette de juger si il semble y avoir un problème de normalité ou d’homoscédasticité. Si les variances étaient égales dans les deux sites, l’étendue verticale des résidus tendrait à être la même. Sur le graphique, on voit que l’étendue des résidus est plus grande à gauche (le site où la taille moyenne est la plus faible), ce qui suggère un possible problème d’homogénéité des variances. On peut tester cela plus formellement en comparant la moyenne de la valeur absolue des résidus.(on y reviendra; c’est le test de Levene).\nLe second graphique est un graphique de probabilité (graphique Q-Q) des résidus. Comme ici, les points tombent près de la diagonale, il ne semble pas y avoir de problème important avec la normalité. On peut faire un test formel de la condition de normalité par le test de Shapiro- Wilk:\n\nshapiro.test(residuals(m1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(m1)\nW = 0.97469, p-value = 0.001857\n\n\nHummm. Ce test indique que les résidus ne sont pas normaux, ce qui contredit notre évaluation visuelle. Cependant, puisque (a) la distribution des résidus ne s’éloigne pas beaucoup de la normalité et (b) le nombre d’observations à chaque site est raisonnablement grand (i.e. &gt;30), on n’a pas à être trop inquiet quant à l’impact de cette violation de normalité sur la fiabilité du test.\nQu’en est-il de l’égalité des variances?\n\nlibrary(car)\nleveneTest(m1)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   1  11.514 0.0008456 ***\n      184                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nbptest(m1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  m1\nBP = 8.8015, df = 1, p-value = 0.00301\n\n\nLes résultats qui précédents proviennent de 3 des tests disponibles en R (dans les package car et lmtest) qui éprouvent l’hypothèse de l’égalité des variances dans des tests de t ou des modèles linéaires ayant uniquement des variables indépendantes discontinues ou catégoriques. Il est redondant de faire les 2 tests. Si ils sont présentés ici, c’est que ces 2 tests sont usuels et qu’il n’y a pas consensus quant au meilleur des deux. Le test de Levene est le plus connu et utilisé. Il compare la moyenne des valeurs absolues des résidus dans les deux groupes. Le test Breusch-Pagan a l’avantage d’être applicable à une plus large gamme de modèles linéaires (il peut être utilisé également avec des variables indépendantes continues, comme en régression). Ici, les deux tests mènent à la même conclusion: la variance diffère entre les deux sites.\nSur la base de ces résultats, on peut conclure qu’il y a évidence (mais faible) pour rejeter l’hypothèse nulle qu’il n’y a pas de différence dans la taille de poissons entre les deux sites. On a utilisé une modification du test de t pour tenir compte du fait que les variances ne sont pas égales et nous sommes satisfaits que la condition de normalité des résidus a été remplie. Alors, fklngth à Cumberland est plus grande que fklngth à The Pas.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#bootstrap-et-tests-de-permutation-pour-comparer-deux-moyennes",
    "href": "32-t_test.html#bootstrap-et-tests-de-permutation-pour-comparer-deux-moyennes",
    "title": "\n5  Comparaison de deux échantillons\n",
    "section": "\n5.4 Bootstrap et tests de permutation pour comparer deux moyennes",
    "text": "5.4 Bootstrap et tests de permutation pour comparer deux moyennes\n\n5.4.1 Bootstrap\nLe bootstrap et les tests de permutation peuvent être utilisés pour comparer les moyennes (ou d’autres statistiques). Le principe général est simple et peut être effectué de diverses façons. Ici j’utilise certains des outils disponibles et le fait qu’une comparaison de moyenne peut être représentée par un modèle linéaire. On pourra utiliser un programme similaire plus tard quand on ajustera des modèles plus complexes.\nlibrary(boot)\nLa première section sert à définir une fonction (ici appelée bs) qui extraie les coefficients d’un modèle ajusté :\n\n# function to obtain model coefficients for each iteration\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ]\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n\nLa deuxième section avec la commande boot() fait le gros du travail: on prend les données dans sturgeon, on les bootstrap \\(R = 1000\\) fois, et chaque fois on ajuste le modèle fklngth vs location et on garde les valeurs calculées par la fonction bs.\n\n# bootstrapping with 1000 replications\nresults &lt;- boot(\n  data = sturgeon, statistic = bs, R = 1000,\n  formula = fklngth ~ location\n)\n# view results\nresults\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = sturgeon, statistic = bs, R = 1000, formula = fklngth ~ \n    location)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 45.084391 -0.001144302   0.4317398\nt2* -1.714546  0.017195029   0.7498871\n\n\nOn obtient les estimés originaux pour les deux coefficients du modèle: la moyenne pour le premier (alphabétiquement) site soit Cumberland, et la différence entre les deux moyennes à Cumberland et The Pas. C’est ce second paramètre, la différence entre les moyennes, qui nous intéresse.\n\nplot(results, index = 2)\n\n\n\nNormalité des estimés de la différence des moyennes par bootstrap\n\n\n\n\n# get 95% confidence intervals\nboot.ci(results, type = \"bca\", index = 2)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"bca\", index = 2)\n\nIntervals : \nLevel       BCa          \n95%   (-3.245, -0.346 )  \nCalculations and Intervals on Original Scale\n\n\nComme l’intervalle de confiance n’inclue pas 0, on conclue que les moyennes ne sont pas les mêmes.\n\n5.4.2 Permutation\nLes tests de permutation pour les modèles linéaires peuvent être effectués à l’aide du package lmPerm:\n\nm1Perm &lt;- lmp(\n  fklngth ~ location,\n  data = sturgeon,\n  perm = \"Prob\"\n)\n\n[1] \"Settings:  unique SS \"\n\n\nLa fonction lmp() fait tout le travail pour nous. Ici, cette fonction est effectuée avec l’option perm pour choisir la règle utilisée pour stopper les calculs. L’option Probs arrête les permutations quand la déviation standard estimée pour la p-valeur tombe sous un seuil déterminé. C’est l’une des nombreuses règles qui peuvent possiblement être utilisées pour ne faire les permutations que sur un sous-ensemble des permutations possibles (ce qui prendrait souvent trrrrrès longtemps).\n\nsummary(m1Perm)\n\n\nCall:\nlmp(formula = fklngth ~ location, data = sturgeon, perm = \"Prob\")\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-18.40921  -3.75370  -0.08439   3.76598  23.48055 \n\nCoefficients:\n          Estimate Iter Pr(Prob)  \nlocation1   0.8573 2522   0.0385 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.454 on 184 degrees of freedom\nMultiple R-Squared: 0.02419,    Adjusted R-squared: 0.01889 \nF-statistic: 4.562 on 1 and 184 DF,  p-value: 0.03401 \n\n\n\n\nIter: la règle a limité à 2522 permutations le calcul. Notez que ce nombre va varier à chaque fois que vous tournerez cet petit bout de code. Ce sont des résultats obtenus par permutations aléatoires, donc vous devez vous attendre à de la variabilité. .\n\nPr(Prob): La p-valeur estimée pour H0 est 0.0385. La différence observée pour fklngth between entre les deux sites était plus grande que les valeurs permutées environ (1 - 0.0385= 96.2%) des 2522 permutations. Notez que 2522 permutations ce n’est pas un si grand nombre de permutations que ça, et donc les faibles valeurs de p ne sont pas très précises. Si vous voulez des valeurs précises de p, vous devrez faire plus de permutations. Vous pouvez ajuster 2 paramètres: maxIter, le nombre maximal de permutations (défaut 5000) et Ca, le seuil de précision désiré qui arrête les permutations quand l’erreur- type de p est plus petite que Ca*p (défaut=0.1)\n\nF-statistic: Le reste est la sortie standard pour un modèle ajusté à des données, avec le test paramétrique. Ici, la p-valeur, présumant que toutes les conditions d’application sont remplies, est 0.034.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#comparer-les-moyennes-de-deux-échantillons-appariés",
    "href": "32-t_test.html#comparer-les-moyennes-de-deux-échantillons-appariés",
    "title": "\n5  Comparaison de deux échantillons\n",
    "section": "\n5.5 Comparer les moyennes de deux échantillons appariés",
    "text": "5.5 Comparer les moyennes de deux échantillons appariés\n\n\n\n\n\n\nAvertissement\n\n\n\nPour la section suivante veuillez télécharger le nouveau fichier skulldat_2020.csv qui a été récemment ajouté sur Brightspace.\n\n\nDans certaines expériences les mêmes individus sont mesurés deux fois, par exemple avant et après un traitement ou encore à deux moments au cours de leur développement. Les mesures obtenues lors de ces deux événements ne sont pas indépendantes, et des comparaisons de ces mesures appariées doivent être faites.\nLe fichier skulldat_2020.csv contient des mesures de la partie inférieure du visage de jeunes filles d’Amérique du Nord prises à 5 ans, puis à 6 ans (données de Newman and Meredith, 1956).\n\nPour débuter, éprouvons l’hypothèse que la largeur de la figure est la même à 5 ans et à 6 ans en assumant que les mesures viennent d’échantillons indépendants.\n\n\nskull &lt;- read.csv(\"data/skulldat_2020.csv\")\nt.test(width ~ age,\n  data = skull,\n  alternative = \"two.sided\"\n)\n\n\n    Welch Two Sample t-test\n\ndata:  width by age\nt = -1.7812, df = 27.93, p-value = 0.08576\nalternative hypothesis: true difference in means between group 5 and group 6 is not equal to 0\n95 percent confidence interval:\n -0.43002624  0.03002624\nsample estimates:\nmean in group 5 mean in group 6 \n       7.461333        7.661333 \n\n\nJusqu’à maintenant, nous avons spécifié le test de t en utilisant une notation de type formule avec y ~ x où y est la variable pour laquelle on souhaite comparer les moyennes et x correspond à une variable définissant les groupes. Cela marche bien lorsque les données de sont pas pairées et sont présentées dans un format de type long où les données prise dans une même catégorie ou sur une même personne sont simplement les unes en-dessous des autres avec des colonnes indiquant l’appartenance des mesures aux différentes catégories (voir la structure de skull par exemple) Dans l’objet skull, il y a 3 colonnes:\n\n\nwidth: largeur de la tête\n\nage: age lors de la mesure\n\nid: identité de la personne\n\n\nhead(skull)\n\n  width age id\n1  7.33   5  1\n2  7.53   6  1\n3  7.49   5  2\n4  7.70   6  2\n5  7.27   5  3\n6  7.46   6  3\n\n\nQuand les données sont pairées, il faut indiquer comment elle doivent être associées. Dans notre exemple, elles sont pairées par individu. Le format de données de type long indique cet appariement via la colonne id. Cependant, la fonction t.test ne permet pas de le prendre en compte. Il faut donc transformer les données en format de type large ou horizontale ou il y a une colonne différente pour chaque catégorie. Dans notre example, on souhaite avoir un fichier avec une colonne de mesure par age et où chaque ligne correspond à une personne différente. On peut modifier le format des données avec le code suivant.\n\nskull_h &lt;- data.frame(id = unique(skull$id))\nskull_h$width5 &lt;- skull$width[match(skull_h$id, skull$id) & skull$age == 5]\nskull_h$width6 &lt;- skull$width[match(skull_h$id, skull$id) & skull$age == 6]\nhead(skull_h)\n\n  id width5 width6\n1  1   7.33   7.53\n2  2   7.49   7.70\n3  3   7.27   7.46\n4  4   7.93   8.21\n5  5   7.56   7.81\n6  6   7.81   8.01\n\n\nMaintenant, effectuons le test apparié qui est approprié: Que conclure? Comment les résultats diffèrent-ils de la première analyse? Pourquoi?\n\nt.test(skull_h$width5, skull_h$width6,\n  alternative = \"two.sided\",\n  paired = TRUE\n)\n\n\n    Paired t-test\n\ndata:  skull_h$width5 and skull_h$width6\nt = -19.72, df = 14, p-value = 1.301e-11\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.2217521 -0.1782479\nsample estimates:\nmean difference \n           -0.2 \n\n\nLa première analyse a comme supposition que les deux échantillons de filles de 5 et 6 ans sont indépendants, alors que la deuxième analyse a comme supposition que la même fille a été mesurée deux fois, une fois à 5 ans, et la deuxième fois à 6 ans.\nNotez que, dans le premier cas, on accepte l’hypothèse nulle, mais que le test apparié rejette l’hypothèse nulle. Donc, le test qui est approprié (le test apparié) indique un effet très significatif de l’âge, mais le test inapproprié suggère que l’âge n’importe pas. C’est parce qu’il y a une très forte corrélation entre la largeur du visage à 5 et 6 ans:\n\ngraphskull &lt;- ggplot(data = skull_h, aes(x = width5, y = width6)) +\n  geom_point() +\n  labs(x = \"Skull width at age 5\", y = \"Skull width at age 6\") +\n  geom_smooth() +\n  scale_fill_continuous(low = \"lavenderblush\", high = \"red\")\ngraphskull\n\n\n\nRelation entre la taille de la tête à 5 et 6 ans\n\n\n\nAvec r = 0.9930841. En présence d’une si forte corrélation, l’erreur-type de la différence appariée de largeur du visage entre 5 et 6 ans est beaucoup plus petit que l’erreur-type de la différence entre la largeur moyenne à 5 ans et la largeur moyenne à 6 ans. Par conséquent, la statistique t associée est beaucoup plus élevée pour le test apparié, la puissance du test est plus grande, et la valeur de p plus petite.\n\nRépétez l’analyse en utilisant l’alternative nonparamétrique, le test Wil-coxon signed-rank. (Que concluez-vous?\n\n\nwilcox.test(skull_h$width5, skull_h$width6,\n  alternative = \"two.sided\",\n  paired = TRUE\n)\n\nWarning in wilcox.test.default(skull_h$width5, skull_h$width6, alternative =\n\"two.sided\", : cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  skull_h$width5 and skull_h$width6\nV = 0, p-value = 0.0007193\nalternative hypothesis: true location shift is not equal to 0\n\n\nDonc on tire la même conclusion qu’avec le test de t apparié et conclue qu’il y a des différences significatives entre la taille des crânes de filles âgées de 5 et 6 ans (quelle surprise!).\nMais, attendez une minute! On a utilisé des tests bilatéraux ici mais, compte tenu de s connaissances sur la croissance des enfants, une hypothèse unilatérale serait préférable. Ceci peut être accommodé en modifiant l’option “alternative”. On utilise l’hypothèse alternative pour décider entre “less” ou “greater”. Ici on s’attends que si il y a une différence, width5 va être inférieur à width6, donc on utiliserait “less”.\n\nt.test(skull_h$width5, skull_h$width6,\n  alternative = \"less\",\n  paired = TRUE\n)\n\n\n    Paired t-test\n\ndata:  skull_h$width5 and skull_h$width6\nt = -19.72, df = 14, p-value = 6.507e-12\nalternative hypothesis: true mean difference is less than 0\n95 percent confidence interval:\n       -Inf -0.1821371\nsample estimates:\nmean difference \n           -0.2 \n\nwilcox.test(skull_h$width5, skull_h$width6,\n  alternative = \"less\",\n  paired = TRUE\n)\n\nWarning in wilcox.test.default(skull_h$width5, skull_h$width6, alternative =\n\"less\", : cannot compute exact p-value with ties\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  skull_h$width5 and skull_h$width6\nV = 0, p-value = 0.0003597\nalternative hypothesis: true location shift is less than 0\n\n\nPour estimer la puissance d’un test de t avec R, il faut utiliser la fonction power.t.test(). Il faut spécifier l’argument type = \"paired\", utiliser la moyenne et l’écart-type de la différence au sein des paires dans les arguments delta et sd.\n\nskull_h$diff &lt;- skull_h$width6 - skull_h$width5\npower.t.test(\n  n = 15,\n  delta = mean(skull_h$diff),\n  sd = sd(skull_h$diff),\n  type = \"paired\"\n)\n\n\n     Paired t test power calculation \n\n              n = 15\n          delta = 0.2\n             sd = 0.03927922\n      sig.level = 0.05\n          power = 1\n    alternative = two.sided\n\nNOTE: n is number of *pairs*, sd is std.dev. of *differences* within pairs",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "32-t_test.html#références",
    "href": "32-t_test.html#références",
    "title": "\n5  Comparaison de deux échantillons\n",
    "section": "\n5.6 Références",
    "text": "5.6 Références\nBumpus, H.C. (1898) The elimination of the unfit as illustrated by the introduced sparrow, Passer domesticus. Biological Lectures, Woods Hole Biology Laboratory, Woods Hole, 11 th Lecture: 209 - 226.\nNewman, K.J. and H.V. Meredith. (1956) Individual growth in skele- tal bigonial diameter during the childhood period from 5 to 11 years of age. Amer. J. Anat. 99: 157 - 187.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Comparaison de deux échantillons</span>"
    ]
  },
  {
    "objectID": "33-anova.html",
    "href": "33-anova.html",
    "title": "\n6  ANOVA à un critère de classification\n",
    "section": "",
    "text": "6.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:\nlibrary(ggplot2)\nlibrary(car)\nlibrary(multcomp)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#set-ano",
    "href": "33-anova.html#set-ano",
    "title": "\n6  ANOVA à un critère de classification\n",
    "section": "",
    "text": "les paquets R:\n\nggplot2\nmultcomp\ncar\n\n\nles fichiers de données\n\nDam10dat.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#anova-à-un-critère-de-classification-et-comparaisons-multiples",
    "href": "33-anova.html#anova-à-un-critère-de-classification-et-comparaisons-multiples",
    "title": "\n6  ANOVA à un critère de classification\n",
    "section": "\n6.2 ANOVA à un critère de classification et comparaisons multiples",
    "text": "6.2 ANOVA à un critère de classification et comparaisons multiples\nL’ANOVA à un critère de classification est l’analogue du test de t pour des comparaisons de moyennes de plus de deux échantillons. Les conditions d’application du test sont essentiellement les mêmes, et lorsque appliqué à deux échantillons ce test est mathématiquement équivalent au test de t.\nEn 1961-1962, le barrage Grand Rapids était construit sur la rivière Saskatchewan en amont de Cumberland House. On croit que durant la construction plusieurs gros esturgeons restèrent prisonniers dans des sections peu profondes et moururent. Des inventaires de la population d’esturgeons furent faits en 1954, 1958, 1965 et 1966. Au cours de ces inventaires, la longueur à la fourche (frklngth) furent mesurées (pas nécessairement sur chaque poisson cependant). Ces données sont dans le fichier Dam10dat.csv.\n\n6.2.1 Visualiser les données\n\nÀ partir des données, vous devez d’abord changer le type de donnée de la variable year, pour que R traite year comme une variable discontinue (factor) plutôt que continue.\n\n\nDam10dat &lt;- read.csv(\"data/Dam10dat.csv\")\nDam10dat$year &lt;- as.factor(Dam10dat$year)\nstr(Dam10dat)\n\n'data.frame':   118 obs. of  21 variables:\n $ year    : Factor w/ 4 levels \"1954\",\"1958\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ fklngth : num  45 50 39 46 54.5 49 42.5 49 56 54 ...\n $ totlngth: num  49 NA 43 50.5 NA 51.7 45.5 52 60.2 58.5 ...\n $ drlngth : logi  NA NA NA NA NA NA ...\n $ drwght  : num  16 20.5 10 17.5 19.7 21.3 9.5 23.7 31 27.3 ...\n $ rdwght  : num  24.5 33 15.5 28.5 32.5 35.5 15.3 40.5 51.5 43 ...\n $ sex     : int  1 1 1 2 1 2 1 1 1 1 ...\n $ age     : int  24 33 17 31 37 44 23 34 33 47 ...\n $ lfkl    : num  1.65 1.7 1.59 1.66 1.74 ...\n $ ltotl   : num  1.69 NA 1.63 1.7 NA ...\n $ ldrl    : logi  NA NA NA NA NA NA ...\n $ ldrwght : num  1.2 1.31 1 1.24 1.29 ...\n $ lrdwght : num  1.39 1.52 1.19 1.45 1.51 ...\n $ lage    : num  1.38 1.52 1.23 1.49 1.57 ...\n $ rage    : int  4 6 3 6 7 7 4 6 6 7 ...\n $ ryear   : int  1954 1954 1954 1954 1954 1954 1954 1954 1954 1954 ...\n $ ryear2  : int  1958 1958 1958 1958 1958 1958 1958 1958 1958 1958 ...\n $ ryear3  : int  1966 1966 1966 1966 1966 1966 1966 1966 1966 1966 ...\n $ location: int  1 1 1 1 1 1 1 1 1 1 ...\n $ girth   : logi  NA NA NA NA NA NA ...\n $ lgirth  : logi  NA NA NA NA NA NA ...\n\n\n\nEnsuite, visualisez les données comme dans le labo pour les tests de t. Créez un histogramme avec ligne de densité et un Box plot par année. Que vous révèlent ces données?\n\n\nmygraph &lt;- ggplot(Dam10dat, aes(x = fklngth)) +\n  labs(x = \"Fork length (cm)\") +\n  geom_density() +\n  geom_rug() +\n  geom_histogram(aes(y = ..density..),\n    color = \"black\",\n    alpha = 0.3\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(Dam10dat$fklngth),\n      sd = sd(Dam10dat$fklngth)\n    ),\n    color = \"red\"\n  )\n\n# display graph, by year\nmygraph + facet_wrap(~year, ncol = 2)\n\n\n\nDistribution de la longueur des esturgeons par année\n\n\n\n\nboxplot(fklngth ~ year, data = Dam10dat)\n\n\n\nBoxplot de la longueur pas annéee\n\n\n\nIl semble que la taille des esturgeons est un peu plus petite après la construction du barrage, mais les données sont très variables et les effets ne sont pas parfaitement clairs. Il y a peut-être des problèmes de normalité avec les échantillons de 1954 et 1966, et il y a probablement des valeurs extrêmes dans les échantillons de 1958 et 1966. On va continuer en testant les conditions d’application de l’ANOVA. Il faut d’abord faire l’analyse et examiner les résidus.\n\n6.2.2 Vérifier les conditions d’application de l’ANOVA paramétrique\nL’ANOVA paramétrique a trois conditions principales d’application :\n\nles résidus sont normalement distribués,\nla variance des résidus est égale dans tous les traitements (homoscédasticité) et\nles résidus sont indépendants les uns des autres.\n\nCes conditions doivent être remplies avant qu’on puisse se fier aux résultats de l’ANOVA paramétrique.\n\nFaites une ANOVA à un critère de classification sur fklngth par année et produisez les graphiques diagnostiques\n\n\n# Fit anova model and plot residual diagnostics\nanova.model1 &lt;- lm(fklngth ~ year, data = Dam10dat)\npar(mfrow = c(2, 2))\nplot(anova.model1)\n\n\n\nConditions d’applications de l’ANOVA\n\n\n\n\n\n\n\n\n\nAvertissement\n\n\n\nFaire attention dans le cadre d’une ANOVA à ce que la variable indépendante soit bien un facteur factor. Si la variable indépendante est reconnu comme du texte character alors vous n’obtiendrez que 3 graphiques et un message d’erreur du type:\n`hat values (leverages) are all = 0.1\nand there are no factor predictors; no plot no. 5`\n\n\nD’après les graphiques, on peut douter de la normalité et de l’homogénéité des variances. Notez qu’il y a un point qui ressort vraiment avec une forte valeur résiduelle (cas numéro 59) et qu’il ne s’aligne pas bien avec les autres valeurs: c’est la valeur extrême qui avait été détectée plus tôt. Ce point fera sans doute gonfler la variance résiduelle du groupe auquel il appartient.\nDes tests formels nous confirmeront ou infirmeront nos conclusions faites à partir de ces graphiques.\n\nFaites un test de normalité sur les résidus de l’ANOVA.\n\n\nshapiro.test(residuals(anova.model1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model1)\nW = 0.91571, p-value = 1.63e-06\n\n\nCe test confirme nos soupçons: les résidus ne sont pas distribués normalement. Il faut cependant garder à l’esprit que la puissance est grande et que même de petites déviations de la normalité sont suffisantes pour rejeter l’hypothèse nulle.\n\nEnsuite, éprouvez l’hypothèse d’égalité des variances (homoscedasticité):\n\n\nleveneTest(fklngth ~ year, data = Dam10dat)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value  Pr(&gt;F)  \ngroup   3  2.8159 0.04234 *\n      114                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa valeur de p vous dit que vous pouvez rejeter l’hypothèse nulle qu’il n’y a aucune différence dans les variances entre les années. Alors, nous concluons que les variances ne sont pas homogènes.\n\n6.2.3 Faire l’ANOVA\n\nFaites une ANOVA de fklnght en choisissant / en présumant pour l’instant que les conditions d’application sont suffisamment remplies. Que concluez-vous?\n\n\nsummary(anova.model1)\n\n\nCall:\nlm(formula = fklngth ~ year, data = Dam10dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2116  -2.6866  -0.7116   2.2103  26.7885 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  48.0243     0.8566  56.061  &lt; 2e-16 ***\nyear1958      0.1872     1.3335   0.140  0.88859    \nyear1965     -5.5077     1.7310  -3.182  0.00189 ** \nyear1966     -3.3127     1.1684  -2.835  0.00542 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.211 on 114 degrees of freedom\nMultiple R-squared:  0.1355,    Adjusted R-squared:  0.1128 \nF-statistic: 5.957 on 3 and 114 DF,  p-value: 0.0008246\n\n\n\n\nCoefficients: Estimates Les 4 coefficients peuvent être utilisés pour obtenir les valeurs prédites par le modèle (i.e. les moyennes de chaque groupe). La fklngth moyenne de la première année (1954) est 48.0243. Les coefficients pour les 3 autres années sont la différence entre la moyenne de l’année en question et la moyenne de 1954. La moyenne pour 1965 est 48.0243-5.5077=42.5166. Pour chaque coefficient, on a également accès à l’erreur-type, une valeur de t et la probabilité qui lui est associée (H0 que le coefficient est 0). Les poissons étaient plus petits après la construction du barrage qu’en 1954. Vous devez prendre ces p-valeurs avec un grain de sel, car elles ne sont pas corrigées pour les comparaisons multiples et. En général, je porte peu d’attention à cette partie des résultats imprimés et me concentre sur ce qui suit.\n\nResidual standard error: La racine carrée de la variance des résidus (valeurs observées moins valeurs prédites) qui correspond à la variabilité inexpliquée par le modèle (variation de la taille des poissons capturés la même année).\n\nMutiple R-squared Le R-carré est la proportion de la variabilité de la variable dépendante qui peut être expliquée par le modèle. Ici, le modèle explique 13.5% de la variabilité. Les différences de taille d’une année à l’autre sont relativement petites lorsqu’on les compare à la variation de taille entre les poissons capturés la même année.\n\n\n\nF-Statistic La p-valeur associée au test “omnibus” que toutes les moyennes sont égales. Ici, p est beaucoup plus petit que 0.05 et on rejetterait H0 pour conclure que fklngth varie selon les années.\n\nLa commande anova() produit le tableau d’ANOVA standard qui contient la plupart de cette information:\n\nanova(anova.model1)\n\nAnalysis of Variance Table\n\nResponse: fklngth\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nyear        3  485.26 161.755  5.9574 0.0008246 ***\nResiduals 114 3095.30  27.152                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa variabilité totale de fklngth, mesurée par la somme des carrés des écarts (Sum sq) est partitionnée en ce qui peut être expliqué par l’année (485.26) et la variabilité résiduelle inexpliquée (3095.30). L’année explique bien (485.26/(3095.30+485.26)=.1355 or 13.55% de la variabilité). Le carré moyen des résidus (Residual Mean Sq) est leur variance.\n\n6.2.4 Les comparaisons multiples\n\nLa fonction pairwise.t.test() peut être utilisée pour comparer des moyennes et ajuster (ou non, si désiré) les probabilités pour le nombre de comparaisons en utilisant l’une des options pour p.adj:\n\nCompare toutes les moyennes sans ajuster les probabilités\n\npairwise.t.test(Dam10dat$fklngth, Dam10dat$year,\n  p.adj = \"none\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Dam10dat$fklngth and Dam10dat$year \n\n     1954   1958   1965  \n1958 0.8886 -      -     \n1965 0.0019 0.0022 -     \n1966 0.0054 0.0079 0.1996\n\nP value adjustment method: none \n\n\nOption bonf ajuste les p-valeurs avec la correction de Bonferroni. Ici, il y a 6 valeurs de p calculées, et la correction de Bonferroni revient à simplement multiplier la p-valeur par 6 (sauf si le résultat est supérieur à 1. Si tel est le cas, la p-value ajustée est 1).\n\npairwise.t.test(Dam10dat$fklngth, Dam10dat$year,\n  p.adj = \"bonf\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Dam10dat$fklngth and Dam10dat$year \n\n     1954  1958  1965 \n1958 1.000 -     -    \n1965 0.011 0.013 -    \n1966 0.033 0.047 1.000\n\nP value adjustment method: bonferroni \n\n\nOption “holm” is est la correction séquentielle de Bonferroni dans laquelle les p-valeurs sont ordonnées de (i=1) la plus faible à (N) la plus grande. La correction pour les p-valeurs est (N-i+1). Ici, il y a N=6 paires de moyennes qui sont comparées. La plus petite valeur de p non corrigée est 0.0019 pour 1954 vs 1965. La p-valeur corrigée est donc \\(0.0019*(6-1+1)=0.011\\). La seconde plus petite p-valeur est 0.0022. Sa p-valeur corrigée est 0.0022*(6-2+1)=0.011. Pour la p-valeur la plus élevée, la correction est (N-N+1)=1, donc la p-valeur corrigée est égale à la p-valeur brute.\n\npairwise.t.test(Dam10dat$fklngth, Dam10dat$year,\n  p.adj = \"holm\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Dam10dat$fklngth and Dam10dat$year \n\n     1954  1958  1965 \n1958 0.889 -     -    \n1965 0.011 0.011 -    \n1966 0.022 0.024 0.399\n\nP value adjustment method: holm \n\n\nL’option “fdr” sert à contrôler le “false discovery rate”.\n\npairwise.t.test(Dam10dat$fklngth, Dam10dat$year,\n  p.adj = \"fdr\"\n)\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  Dam10dat$fklngth and Dam10dat$year \n\n     1954   1958   1965  \n1958 0.8886 -      -     \n1965 0.0066 0.0066 -     \n1966 0.0108 0.0119 0.2395\n\nP value adjustment method: fdr \n\n\nLes quatre méthodes mènent ici à la même conclusion: les poissons sont plus gros après la construction du barrage et toutes les comparaisons entre les années 50 et 60 sont significatives alors que les différences entre 54 et 58 ou 65 et 66 ne le sont pas. La conclusion ne dépend pas du choix de méthode.\nDans d’autres situations, vous pourriez obtenir des résultats contradictoires. Alors, quelle méthode choisir? Les p-valeurs qui ne sont pas corrigées sont certainement suspectes lorsqu’il y a plusieurs comparaisons. D’un autre coté, la correction de Bonferroni est conservatrice et le devient encore plus lorsqu’il y a de très nombreuses comparaisons. Des travaux récents suggèrent que la correction fdr est un bon compromis lorsqu’il y a beaucoup de comparaisons.\nLa méthode de Tukey est l’une des plus populaires et est facile à utiliser en R (notez cependant qu’il y a un petit bug qui se manifeste quand la variable indépendante peut ressembler à un nombre plutôt qu’un facteur, ce qui explique la petite pirouette avec paste0 dans le code):\n\nDam10dat$myyear &lt;- as.factor(paste0(\"m\", Dam10dat$year))\nTukeyHSD(aov(fklngth ~ myyear, data = Dam10dat))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = fklngth ~ myyear, data = Dam10dat)\n\n$myyear\n                  diff        lwr        upr     p adj\nm1958-m1954  0.1872141  -3.289570  3.6639986 0.9990071\nm1965-m1954 -5.5076577 -10.021034 -0.9942809 0.0100528\nm1966-m1954 -3.3126964  -6.359223 -0.2661701 0.0274077\nm1965-m1958 -5.6948718 -10.436304 -0.9534397 0.0116943\nm1966-m1958 -3.4999106  -6.875104 -0.1247171 0.0390011\nm1966-m1965  2.1949612  -2.240630  6.6305526 0.5710111\n\n\n\nplot(TukeyHSD(aov(fklngth ~ myyear, data = Dam10dat)))\n\n\n\nDifférence anuelles dans la longueur des esturgeons\n\n\n\nLes intervalles de confiance, corrigés pour les comparaisons multiples par la méthode de Tukey, sont illustrés pour les différences entre années. Malheureusement les légendes ne sont pas complètes, mais l’ordre est le même que dans le tableau précédent.\nLe package multcomp peut produire de meilleurs graphiques, mais requiert un peu plus de code:\n\n# Alternative way to compute Tukey multiple comparisons\n# set up a one-way ANOVA\nanova.fkl.vs.year &lt;- aov(aov(fklngth ~ myyear, data = Dam10dat))\n# set up all-pairs comparisons for factor `year'\n\nmeandiff &lt;- glht(anova.fkl.vs.year, linfct = mcp(\n  myyear =\n    \"Tukey\"\n))\nconfint(meandiff)\n\n\n     Simultaneous Confidence Intervals\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: aov(formula = aov(fklngth ~ myyear, data = Dam10dat))\n\nQuantile = 2.5934\n95% family-wise confidence level\n \n\nLinear Hypotheses:\n                   Estimate lwr      upr     \nm1958 - m1954 == 0   0.1872  -3.2710   3.6455\nm1965 - m1954 == 0  -5.5077  -9.9970  -1.0184\nm1966 - m1954 == 0  -3.3127  -6.3430  -0.2824\nm1965 - m1958 == 0  -5.6949 -10.4110  -0.9787\nm1966 - m1958 == 0  -3.4999  -6.8571  -0.1427\nm1966 - m1965 == 0   2.1950  -2.2170   6.6069\n\nplot(meandiff)\n\n\n\nDifférence anuelles dans la longueur des esturgeons\n\n\n\nC’est un peu mieux, mais ce qui le serait encore plus c’est un graphique des moyennes, avec leurs intervalles de confiance ajustés pour les comparaisons multiples:\n\n# Compute and plot means and Tukey CI\nmeans &lt;- glht(\n  anova.fkl.vs.year,\n  linfct = mcp(myyear = \"Tukey\")\n)\ncimeans &lt;- cld(means)\n# use sufficiently large upper margin\n# plot\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\nplot(cimeans)\n\n\n\nDifférence anuelles dans la longueur des esturgeons\n\n\n\nNotez les lettres au dessus du graphique: les années étiquetées avec la même lettre ne diffèrent pas significativement l’une de l’autre.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#transformations-de-données-et-anova-non-paramétrique",
    "href": "33-anova.html#transformations-de-données-et-anova-non-paramétrique",
    "title": "\n6  ANOVA à un critère de classification\n",
    "section": "\n6.3 Transformations de données et ANOVA non-paramétrique",
    "text": "6.3 Transformations de données et ANOVA non-paramétrique\nDans l’exemple précédent sur les différences annuelles de la variable fklgnth, on a noté que les conditions d’application de l’ANOVA n’étaient pas remplies. Si les données ne remplissent pas les conditions de l’ANOVA paramétrique, il y a 3 options :\n\nNe rien faire. Si les effectifs dans chaque groupe sont grands, on peut relaxer les conditions d’application car l’ANOVA est alors assez robuste aux violations de normalité (mais moins aux violations d’homoscedasticité),\non peut transformer les données\non peut faire une analyse non-paramétrique.\n\n\nRefaites l’ANOVA de la section précédente après avoir transformé en faisant le logarithme à la base de 10. Avec les données transformées, est-ce que les problèmes qui avaient été identifiés disparaissent ?\n\n\n# Fit anova model on log10 of fklngth and plot residual diagnostics\npar(mfrow = c(2, 2))\nanova.model2 &lt;- lm(log10(fklngth) ~ year, data = Dam10dat)\nplot(anova.model2)\n\n\n\nConditions d’application de l’ANOVA\n\n\n\nLes graphiques diagnostiques des résidus donnent:\nLes graphiques sont à peine mieux ici. Si on fait le test Wilk-Shapiro sur les résidus, on obtient:\n\nshapiro.test(residuals(anova.model2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model2)\nW = 0.96199, p-value = 0.002048\n\n\nAlors, on a toujours des problèmes avec la normalité et on est juste sur le seuil de décision pour l’égalité des variances. Vous avez le choix à ce point:\n\nessayer de trouver une autre transformation pour mieux rencontrer les conditions d’application\nassumer que les données sont rencontrent suffisamment les conditions d’application\nfaire une ANOVA non-paramétrique.\n\n\nL’analogue non-paramétrique de l’ANOVA à un critère de classification le plus employé est le test de Kruskall-Wallis. Faites ce test sur fklngth et comparez les résultats à ceux de l’analyse paramétrique. Que concluez-vous?\n\n\nkruskal.test(fklngth ~ year, data = Dam10dat)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  fklngth by year\nKruskal-Wallis chi-squared = 15.731, df = 3, p-value = 0.001288\n\n\nLa conclusion est donc la même qu’avec l’ANOVA paramétrique: on rejette l’hypothèse nulle que le rang moyen est le même pour chaque année. Donc, même si les conditions d’application de l’analyse paramétrique n’étaient pas parfaitement rencontrées, les conclusions sont les mêmes, ce qui illustre la robustesse de l’ANOVA paramétrique.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#examen-des-valeurs-extrêmes",
    "href": "33-anova.html#examen-des-valeurs-extrêmes",
    "title": "\n6  ANOVA à un critère de classification\n",
    "section": "\n6.4 Examen des valeurs extrêmes",
    "text": "6.4 Examen des valeurs extrêmes\nVous devriez avoir remarqué au cours des analyses précédentes qu’il y avait peut-être des valeurs extrêmes dans les données. Ces points étaient évidents dans le Box Plot de fklngth by year et ont été notés comme les points 59, 23, et 87 dans les diagrammes de probabilité des résidus et dans le diagramme de dispersion des résidus et des valeurs estimées. En général, vous devez avoir de très bonnes raisons pour enlever des valeurs extrêmes de la base de données (i.e. vous savez qu’il y a eu une erreur avec un cas). Cependant, il est quand même toujours valable de voir comment l’analyse change en enlevant des valeurs extrêmes de la base de données.\n\nRépétez l’ANOVA originale sur fklngth et year mais faites le avec un sous-ensemble de données sans les valeurs extrêmes. Est-ce que les conclusions ont changé?\n\n\nDamsubset &lt;- Dam10dat[-c(23, 59, 87), ] # removes obs 23, 59 and 87\naov.Damsubset &lt;- aov(fklngth ~ as.factor(year), Damsubset)\nsummary(aov.Damsubset)\n\n                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nas.factor(year)   3  367.5  122.50   6.894 0.000267 ***\nResiduals       111 1972.4   17.77                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nshapiro.test(residuals(aov.Damsubset))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(aov.Damsubset)\nW = 0.98533, p-value = 0.2448\n\n\n\nleveneTest(fklngth ~ year, Damsubset)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value   Pr(&gt;F)   \ngroup   3  4.6237 0.004367 **\n      111                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nL’élimination de trois valeurs extrêmes améliore un peu les choses, mais ce n’est pas parfait. On a toujours une problème avec les variances, mais les résidus sont maintenant normaux. Cependant, le fait que la conclusion qu’on tire de l’ANOVA originale ne change pas en enlevant les points renforce le fait qu’on n’a pas une bonne raison pour enlever les points.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "33-anova.html#test-de-permutation",
    "href": "33-anova.html#test-de-permutation",
    "title": "\n6  ANOVA à un critère de classification\n",
    "section": "\n6.5 Test de permutation",
    "text": "6.5 Test de permutation\nCommande R pour un test de permutation d’une ANOVA à un critère de classification.\n\n#############################################################\n# Permutation Test for one-way ANOVA\n# modified from code written by David C. Howell\n# http://www.uvm.edu/~dhowell/StatPages/More_Stuff/Permutation%20Anova/PermTestsAnova.html\n# set desired number of permutations\nnreps &lt;- 500\n# to simplify reuse of this code, copy desired dataframe to mydata\nmydata &lt;- Dam10dat\n# copy model formula to myformula\nmyformula &lt;- as.formula(\"fklngth ~ year\")\n# copy dependent variable vector to mydep\nmydep &lt;- mydata$fklngth\n# copy independent variable vector to myindep\nmyindep &lt;- as.factor(mydata$year)\n################################################\n# You should not need to modify code chunk below\n################################################\n# Compute observed F value for original sample\nmod1 &lt;- lm(myformula, data = mydata) # Standard Anova\nANOVA &lt;- summary(aov(mod1)) # Save summary to variable\nobservedF &lt;- ANOVA[[1]]$\"F value\"[1] # Save observed F value\n# Print standard ANOVA results\ncat(\n  \" The standard ANOVA for these data follows \",\n  \"\\n\"\n)\n\nprint(ANOVA, \"\\n\")\ncat(\"\\n\")\ncat(\"\\n\")\nprint(\"Resampling as in Manly with unrestricted sampling of observations. \")\n\n# Now start resampling\nFboot &lt;- numeric(nreps) # initalize vector to receive permuted\nvalues\nFboot[1] &lt;- observedF\nfor (i in 2:nreps) {\n  newdependent &lt;- sample(mydep, length(mydep)) # randomize dep\n  var\n  mod2 &lt;- lm(newdependent ~ myindep) # refit model\n  b &lt;- summary(aov(mod2))\n  Fboot[i] &lt;- b[[1]]$\"F value\"[1] # store F stats\n}\npermprob &lt;- length(Fboot[Fboot &gt;= observedF]) / nreps\ncat(\n  \" The permutation probability value is: \", permprob,\n  \"\\n\"\n)\n# end of code chunk for permutation\n\nVersion lmPerm du test de permutation.\n\n## lmPerm version of permutation test\nlibrary(lmPerm)\n# for generality, copy desired dataframe to mydata\n# and model formula to myformula\nmydata &lt;- Dam10dat\nmyformula &lt;- as.formula(\"fklngth ~ year\")\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(myformula, data = mydata)\n# Calculate permutation p-value\nanova(lmp(myformula, data = mydata, perm = \"Prob\", center = FALSE, Ca = 0.001))",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ANOVA à un critère de classification</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html",
    "href": "34-anova_mult.html",
    "title": "\n7  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "",
    "text": "7.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:\nlibrary(multcomp)\nlibrary(car)\nlibrary(tidyverse)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#set-anomul",
    "href": "34-anova_mult.html#set-anomul",
    "title": "\n7  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "",
    "text": "les paquets R:\n\nmulticomp\ncar\ntidyverse\n\n\nles fichiers de données\n\nStu2wdat.csv\nStu2mdat.csv\nnr2wdat.csv\nnestdat.csv\nwmcdat2.csv\nwmc2dat2.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#plan-factoriel-à-deux-facteurs-de-classification-et-réplication",
    "href": "34-anova_mult.html#plan-factoriel-à-deux-facteurs-de-classification-et-réplication",
    "title": "\n7  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n7.2 Plan factoriel à deux facteurs de classification et réplication",
    "text": "7.2 Plan factoriel à deux facteurs de classification et réplication\nIl est fréquent de vouloir analyser l’effet de plusieurs facteurs simultanément. L’ANOVA factorielle à deux critères de classification permet d’examiner deux facteurs à la fois, mais la même approche peut être utilisée pour 3, 4 ou même 5 facteurs quoique l’interprétation des résultats devienne beaucoup plus complexe.\nSupposons que vous êtes intéressés par l’effet de deux facteurs : site (location, Cumberland House ou The Pas) et sexe (sex, mâle ou femelle) sur la taille des esturgeons. Comme l’effectif n’est pas le même pour tous les groupes, c’est un plan qui n’est pas balancé. Notez aussi qu’il y a des valeurs manquantes pour certaines variables, ce qui veut dire que chaque mesure n’a pas été effectuée sur chaque poisson.\n\n7.2.1 ANOVA à effets fixes\n\nExaminez d’abord les données en faisant des box plots de rdwght pour sex et location des données du fichier Stu2wdat.csv .\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nStu2wdat &lt;- read.csv(\"data/Stu2wdat.csv\")\n\nggplot(Stu2wdat, aes(x = sex, y = rdwght)) +\ngeom_boxplot(notch = TRUE) +\nfacet_grid(~location)\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nLes graphiques montrent qu’aux deux sites les femelles sont probablement plus grandes que les mâles, mais que les tailles ne varient pas beaucoup d’un site à l’autre. La présence de valeurs extrêmes sur ces graphiques suggère qu’il y aura peut-être des problèmes avec la condition de normalité des résidus.\n\nGénérez les statistiques sommaires pour RDWGHT par sex et Location.\n\n\nStu2wdat %&gt;%\n  group_by(sex, location) %&gt;%\n  summarise(\n    mean = mean(rdwght, na.rm = TRUE), sd = sd(rdwght, na.rm = TRUE), n = n()\n  )\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 × 5\n# Groups:   sex [2]\n  sex            location        mean    sd     n\n  &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 \"FEMALE      \" \"CUMBERLAND  \"  27.4  9.33    51\n2 \"FEMALE      \" \"THE_PAS     \"  28.0 12.5     55\n3 \"MALE        \" \"CUMBERLAND  \"  22.1  4.79    34\n4 \"MALE        \" \"THE_PAS     \"  20.6  9.92    46\n\n\nCes résultats supportent l’interprétation des box plots: Les femelles sont plus grosses que les mâles, et la différence de taille entre les deuxsites sont petites.\n\nÀ l’aide du fichier Stu2wdat.csv faites une ANOVA factorielle à deux critères de classification:\n\n\n# Fit anova model and plot residual diagnostics\n# but first, save current par and set graphic page to hold 4 graphs\nopar &lt;- par(mfrow = c(2, 2))\nanova.model1 &lt;- lm(rdwght ~ sex + location + sex:location,\n  contrasts = list(sex = contr.sum, location = contr.sum),\n  data = Stu2wdat\n)\nanova(anova.model1)\n\nAnalysis of Variance Table\n\nResponse: rdwght\n              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nsex            1  1839.6 1839.55 18.6785 2.569e-05 ***\nlocation       1     4.3    4.26  0.0433    0.8355    \nsex:location   1    48.7   48.69  0.4944    0.4829    \nResiduals    178 17530.4   98.49                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nAvertissement\n\n\n\nAttention, R imprime les sommes des carrés séquentielles (Type I) les carrés moyens et probabilités associés. Vous ne pouvez pas vous y fier si votre plan d’expérience n’est pas parfaitement balancé. Dans cet exemple, le nombre de poissons capturés change selon le site et le sexe et le plan d’expérience n’est donc pas balancé.\n\n\nVous devez extraire les sommes de carrés partielles (Type III). Le moyen le plus simple que j’ai trouvé est d’utiliser la fonction Anova()du package car (notez la différence subtile, Anova() n’est pas la même chose que anova(), R est impitoyable et distingue les majuscules des minuscules). Malheureusement, Anova() ne suffit pas; il faut également spécifier le type de contraste dans le modèle avecl’argument contrasts = list(sex = contr.sum,location = contr.sum)\n\nlibrary(car)\nAnova(anova.model1, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n             Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)  106507   1 1081.4552 &lt; 2.2e-16 ***\nsex            1745   1   17.7220 4.051e-05 ***\nlocation          9   1    0.0891    0.7656    \nsex:location     49   1    0.4944    0.4829    \nResiduals     17530 178                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSuite à l’ANOVA, on accepte deux hypothèses nulles: (1) que l’effet du sexe ne varie pas entre les sites (pas d’interaction significative) et (2) qu’il n’y a pas de différence de taille des esturgeons (peu importe le sexe) entre les deux sites. D’un autre coté, on rejette l’hypothèse nulle qu’il n’y a pas de différence de taille entre les esturgeons mâles et les femelles, tel que suggéré par les graphiques.\n\npar(mfrow = c(2, 2))\nplot(anova.model1)\n\n\n\nConditions d’application ANOVA model1\n\n\n\nCependant, on ne peut se fier à ces résultats sans vérifier si les conditions d’application de l’ANOVA étaient remplies. Un examen des graphiques des résidus, en haut, montre que les résidus semblent être distribués plus ou moins normalement, si ce n’est des 3 valeurs extrêmes qui sont notées sur le diagramme de probabilité (cas 101, 24,& 71). D’après le graphique des résidus vs les valeurs prédites, on voit que l’étendue des résidus est plus ou moins égale pour les valeurs estimées, sauf encore pour 2 ou 3 cas. Si on éprouve la normalité, on obtient:\n\nshapiro.test(residuals(anova.model1))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model1)\nW = 0.87213, p-value = 2.619e-11\n\n\nAlors, il y a évidence que les résidus ne sont pas distribués normalement.\nNous allons utiliser le test de Levene pour examiner l’homoscédasticité des résidus, de la même façon qu’on a fait pour l’ANOVA à un critère de classification.\n\nlibrary(car)\nleveneTest(rdwght ~ sex * location, data = Stu2wdat)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value  Pr(&gt;F)  \ngroup   3  3.8526 0.01055 *\n      178                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSi les résidus étaient homoscédastiques, on accepterait l’hypothèse nulle que le absres moyen ne varie pas entre les niveaux de sexe et location (i.e., sexloc). Le tableau d’ANOVA ci-dessus montre que l’hypothèse est rejetée. Il y a donc évidence d’hétéroscédasticité. En bref, nous avons donc plusieurs conditions d’application qui ne sont pas respectées. La question qui reste est: ces violations sont-elles suffisantes pour invalider nos conclusions ?\n\n\n\n\n\n\nExercice\n\n\n\nRépétez la même analyse avec les données du fichier stu2mdat.csv . Que concluez-vous? Supposons que vous vouliez comparer la taille des mâles et des femelles. Comment cette comparaison diffère entre les deux ensembles de données ?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nStu2mdat &lt;- read.csv(\"data/Stu2mdat.csv\")\nanova.model2 &lt;- lm(\n  formula = rdwght ~ sex + location + sex:location,\n  contrasts = list(sex = contr.sum, location = contr.sum),\n  data = Stu2mdat\n)\nsummary(anova.model2)\nAnova(anova.model2, type = 3)\n\n\n\n\n\n\n\nCall:\nlm(formula = rdwght ~ sex + location + sex:location, data = Stu2mdat, \n    contrasts = list(sex = contr.sum, location = contr.sum))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.917  -6.017  -0.580   4.445  65.743 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     24.5346     0.7461  32.885  &lt; 2e-16 ***\nsex1            -0.5246     0.7461  -0.703    0.483    \nlocation1        0.2227     0.7461   0.299    0.766    \nsex1:location1   3.1407     0.7461   4.210 4.05e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.924 on 178 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.09744,   Adjusted R-squared:  0.08223 \nF-statistic: 6.405 on 3 and 178 DF,  p-value: 0.0003817\n\n\nNotez que cette fois les femelles sont plus grandes que les mâles à Cumberland House, mais que c’est le contraire à The Pas. Quel est le résultat de l’ANOVA (n’oubliez pas qu’il faut des Type III sums of squares pour les résultats)?\n\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n             Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)  106507   1 1081.4552 &lt; 2.2e-16 ***\nsex              49   1    0.4944    0.4829    \nlocation          9   1    0.0891    0.7656    \nsex:location   1745   1   17.7220 4.051e-05 ***\nResiduals     17530 178                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDans ce cas, le terme de l’interaction (sex:location) est maintenant significatif mais les effets principaux ne le sont pas.\n\nVous trouverez utile ici de créer des graphiques pour les deux fichiers de données pour comparer les interactions entre sex et location. Le graphique d’interaction montre les relations entre les moyennes de chaque combinaison de facteurs (appelées aussi les moyennes des ce$lules).Générez un graphique illustrant les intéractions en utilisant la fonction allEffects du package effects :\n\n\nlibrary(effects)\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nallEffects(anova.model1)\n\n model: rdwght ~ sex + location + sex:location\n\n sex*location effect\n              location\nsex            CUMBERLAND   THE_PAS     \n  FEMALE           27.37347     27.97717\n  MALE             22.14118     20.64652\n\nplot(allEffects(anova.model1), \"sex:location\")\n\n\n\nEffet du sexe et du lieu sur le poids des esturgeons\n\n\n\n\nallEffects(anova.model2)\n\n model: rdwght ~ sex + location + sex:location\n\n sex*location effect\n              location\nsex            CUMBERLAND   THE_PAS     \n  FEMALE           27.37347     20.64652\n  MALE             22.14118     27.97717\n\nplot(allEffects(anova.model2), \"sex:location\")\n\n\n\nEffet du sexe et du lieu sur le poids des esturgeons\n\n\n\nIl y a une différence importante entre les résultats obtenus avec stu2wdat et stu2mdat. Dans le premier cas, puisqu’il n’y a pas d’interaction, on peut regrouper les données des deux niveaux d’un facteur (le site, par exemple) pour éprouver l’hypothèse d’un effet de l’autre facteur (le sexe). En fait, si on fait cela et que l’on calcule une ANOVA à un critère de classification (sex), on obtient:\n\nAnova(aov(rdwght ~ sex, data = Stu2wdat), type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n            Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept)  78191   1 800.440 &lt; 2.2e-16 ***\nsex           1840   1  18.831 2.377e-05 ***\nResiduals    17583 180                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotez que la somme des carrés des résidus (17583) est presque égale à celle du modèle complet (17530) de l’ANOVA factorielle à deux facteurs. C’est parce que dans cette anova factorielle, le terme d’interaction et le terme représentant l’effet du site n’expliquent qu’une partie infime de la variabilité. D’un autre coté, si on essaie le même truc avec stu2mdat, on obtient:\n\nAnova(aov(rdwght ~ sex, data = Stu2mdat), type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n            Sum Sq  Df  F value Pr(&gt;F)    \n(Intercept)  55251   1 515.0435 &lt;2e-16 ***\nsex            113   1   1.0571 0.3053    \nResiduals    19309 180                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIci la somme des carrées des résidus (19309) est beaucoup plus grande que celle de l’ANOVA factorielle (175306) parce qu’une partie importante de la variabilité expliquée par le modèle est associée à l’interaction. Notez que si on n’avait fait que cette analyse, on conclurait que les esturgeons mâles et femelles ont la même taille. Mais en fait leur taille diffère; seulement la différence est à l’avantage des mâles à un site et à l’avantage des femelles à l’autre. Il est donc délicat d’interpréter l’effet principal (sexe) en présence d’une interaction significative…\n\n7.2.2 ANOVA à effets mixtes\nLes analyses qui précèdent négligent un point important: location pourrait être traité comme un facteur aléatoire et sex est fixe. Par conséquent le modèle approprié d’ANOVA est de type mixte.\nNotez que dans toutes les analyses qui précèdent, R a traité cette ANOVA comme si elle etait de type effet fixe seulement, et les termes principaux et celui d’interaction ont été testés en utilisant le carré moyen des résidus comme dénominateur des tests de F. Cependant, pour une ANOVA de type mixte, ces effets devraient être testés en utilisant le carré moyen du terme d’interaction, ou en combinant la somme des carrés de l’erreur et de l’interaction (selon le statisticien consulté!).\nEn utilisant Stu2wdat, refaites un tableau d’ANOVA pour RDWGHT en considérant location comme facteur aléatoire et sex comme un facteur fixe. Pour ce faire, vous devrez recalculer les valeurs de F pour sex et location en utilisant le carré moyen de l’interaction sex:location au lieu du carré moyen des résidus comme dénominateur. Le mieux c’est de le faire à la mitaine ent travaillant avec les Type III Sums of squares du tableau d’ANOVA.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAnova(anova.model1, type = 3)\n\n\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: rdwght\n             Sum Sq  Df   F value    Pr(&gt;F)    \n(Intercept)  106507   1 1081.4552 &lt; 2.2e-16 ***\nsex            1745   1   17.7220 4.051e-05 ***\nlocation          9   1    0.0891    0.7656    \nsex:location     49   1    0.4944    0.4829    \nResiduals     17530 178                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPour sex, la nouvelle valeur de F (le rapport des carrés moyens) est de\n\\[F = \\frac{(1745/1)}{(49/1)} = 35.6\\]\nPour obtenir la valeur de p correspondant à cette statistique F, il faut utilisez la fonction de probabilité de la distribtuion de F pf(F, df1, df2, lower.tail = FALSE), où F est la valeur de F calculée, et df1 et df2 sont les degrés de liberté du numérateur (sex) et dénominateur(SEX:location).\n\npf(35.6, 1, 1, lower.tail = FALSE)\n\n[1] 0.1057152\n\n\nNotez que maintenant la valeur de p pour sex n’est plus significative. C’est parce que le carré moyen de l’erreur dans l’ANOVA initiale est plus petit que celui associé à l’interaction, mais surtout parce que le nombre de degrés de liberté pour le dénominateur du test de F est passé de 178 à 1 seulement. En général, c’est beaucoup plus difficile d’obtenir des résultats significatifs quand les degrés de liberté pour le dénominateur sont petits.\n\n\n\n\n\n\nNote\n\n\n\nLes modèles mixtes qui sont une généralisation de l’ANOVA à effets mixtes sont maintenant extrêmement bien développé et sont à favoriser lors d’analyse incluant des effets dit aléatoires.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#plan-factoriel-à-deux-facteurs-de-classification-sans-réplication",
    "href": "34-anova_mult.html#plan-factoriel-à-deux-facteurs-de-classification-sans-réplication",
    "title": "\n7  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n7.3 Plan factoriel à deux facteurs de classification sans réplication",
    "text": "7.3 Plan factoriel à deux facteurs de classification sans réplication\nDans certains plans d’expérience il n’y a pas de réplicats pour chaque combinaison de facteurs, par exemple parce qu’il serait trop coûteux de faire plus d’une observation. L’ANOVA à deux critères de classification est quand même possible dans ces circonstances, mais il y a une limitation importante.\n\n\n\n\n\n\nAvertissement\n\n\n\nComme il n’y a pas de réplicats, on ne peut estimer la variance du terme d’erreur. En effet on ne peut qu’estimer la somme des carrés associés à chacun des facteurs principaux, et la quantité de variabilité qui reste (Remainder Mean Square) représente la somme de la variabilité attribuable à l’interaction et au terme d’erreur. Cela a une implication importante. Dans le cas d’un modèle avec uniquement des effets fixes ou pour l’effet aléatoire d’un modèle d’ANOVA mixtes on ne peut tester les effets principaux que si on est sur qu’il n’y a pas d’interaction.\n\n\nUn limnologiste qui étudie Round Lake dans le Parc Algonquin prend une seule mesure de température (temp,en degrés C) à 10 profondeurs différentes (depth, en m) à quatre dates (date) au cours de l’été. Ses données sont au fichier nr2wdat.csv.\n\nEffectuez une ANOVA à deux critères de classification en utilisant temp comme variable dépendante et date et depth comme variables indépendantes (vous devez changer le type de données pour DEPTH pour que R traite cette variable comme un facteur et non pas une var$able continue).\n\n\nnr2wdat &lt;- read.csv(\"data/nr2wdat.csv\")\nnr2wdat$depth &lt;- as.factor(nr2wdat$depth)\nanova.model4 &lt;- lm(temp ~ date + depth, data = nr2wdat)\nAnova(anova.model4, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: temp\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 1511.99  1 125.5652 1.170e-11 ***\ndate         591.15  3  16.3641 2.935e-06 ***\ndepth       1082.82  9   9.9916 1.450e-06 ***\nResiduals    325.12 27                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSi on suppose que c’est un modèle d’ANOVA mixte (date aléatoire, Depth fixe), que concluez vous? (Indice: faites un graphique d’interaction température en fonction de la profondeur et la date, pour voir ce qui se passe).\n\ninteraction.plot(nr2wdat$depth, nr2wdat$date, nr2wdat$temp)\n\n\n\nEffet du mois et de la profondeur sur la température\n\n\n\nLa température diminue significativement en profondeur. Pour tester l’effet du mois (le facteur aléatoire), on doit présumer qu’il n’y a pas d’interaction entre la profondeur et le mois (donc que l’effet de la profondeur sur la température est le même à chaque mois). C’est peu probable: si vous faites un graphique de la température en fonction de la profondeur pour chaque mois, vous observerez que le profil de température change au fur et à mesure du développement de la thermocline. Bref, comme le profil change au cours de l’été, ce modèle ne fait pas de très bonnes prédictions.\nJetez un coup d’oeil sur les graphiques des résidus:\n\npar(mfrow = c(2, 2))\nplot(anova.model4)\n\n\n\nConditions d’applications du modèle anova.model4\n\n\n\n\nshapiro.test(residuals(anova.model4))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model4)\nW = 0.95968, p-value = 0.1634\n\n\nLe test de normalité sur les résidus donne p = 0.16, donc l’hypothèse de normalité ne semble pas être sérieusement en doute. Pour l’égalité des variances, on peut seulement comparer entre les mois en utilisant les profondeurs comme réplicats (ou l’inverse). En utilisant les profondeurs comme réplicats, on obtient:\n\nleveneTest(temp ~ date, data = nr2wdat)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value    Pr(&gt;F)    \ngroup  3  17.979 2.679e-07 ***\n      36                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIl y a donc un problème d’hétéroscédasticité, comme on peut très bien voir dans le graphique des résidus vs les valeurs estimées. Cette analyse n’est donc pas très satisfaisante: il y a des violations des conditions d’application et il semble y avoir une interaction entre depth et date qui pourrait invalider l’analyse.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#plans-hiérarchiques",
    "href": "34-anova_mult.html#plans-hiérarchiques",
    "title": "\n7  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n7.4 Plans hiérarchiques",
    "text": "7.4 Plans hiérarchiques\nUn design expérimental fréquent implique la division de chaque groupe du facteur majeur en sous-groupes aléatoires. Par exemple, une généticienne intéressée par l’effet du génotype sur la résistance à la dessiccation chez la drosophile effectue une expérience. Pour chaque génotype (facteur principal) elle prépare trois chambres de croissance (sous-groupes) avec une température et humidité contrôlées. Dans chaque chambre de croissance, elle place cinq larves, puis mesure le nombre d’heures pendant lesquelles chaque larve survit. Les données ont donc un structure hiérarchique. Il ya des observations répétées dans chaque chambre au sein de chaque génotype.\n\nLe fichier nestdat.csv contient les résultats d’une expérience semblable. Il contient trois variables : genotype, chamber et survival. Effectuez une ANOVA hiérarchique avec survival comme variable dépendante et genotype et chamber/genotype comme variables indépendantes.\n\n\nnestdat &lt;- read.csv(\"data/nestdat.csv\")\nnestdat$chamber &lt;- as.factor(nestdat$chamber)\nnestdat$genotype &lt;- as.factor(nestdat$genotype)\nanova.nested &lt;- lm(survival ~ genotype / chamber, data = nestdat)\n\nQue concluez-vous de cette analyse ? Que devrait être la prochaine étape ? (Indice: si l’effet de Chamber / genotype n’est pas significatif, vous pouvez augmenter la puissance des comparaisons entre génotypes en regroupant les chambres de chaque génotype.). Faites-le ! N’oubliez pas de vérifier les conditions d’applications de l’ANOVA!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nanova(anova.nested)\n\nAnalysis of Variance Table\n\nResponse: survival\n                 Df  Sum Sq Mean Sq  F value Pr(&gt;F)    \ngenotype          2 2952.22 1476.11 292.6081 &lt;2e-16 ***\ngenotype:chamber  6   40.65    6.78   1.3432 0.2639    \nResiduals        36  181.61    5.04                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow = c(2, 2))\nplot(anova.nested)\n\n\n\nConditions d’applications du modèle anova.nested\n\n\n\n\n\n\nOn conclue de cette analyse que la variation entre les chambres de croissance n’est pas significative, mais qu’on doit rejeter l’hypothèse nulle que tous les génotypes ont la même résistance à la dessiccation.\nComme l’effet hiérarchique chamber / genotype n’est pas significatif, on peut regrouper les observations pour augmenter le nombre de degrés de liberté:\n\nanova.simple &lt;- lm(survival ~ genotype, data = nestdat)\nanova(anova.simple)\n\nAnalysis of Variance Table\n\nResponse: survival\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ngenotype   2 2952.22 1476.11  278.93 &lt; 2.2e-16 ***\nResiduals 42  222.26    5.29                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDonc on conclue qu’il y a une variation significative de résistance à la dessiccation entre les trois génotypes.\nLe graphique de survival en fonction du génotype suggère que la résistance à la dessiccation varie entre chaque génotype. On peut combiner cela avec un test de Tukey.\n\npar(mfrow = c(1, 1))\n# Compute and plot means and Tukey CI\nmeans &lt;- glht(anova.simple, linfct = mcp(\n  genotype =\n    \"Tukey\"\n))\ncimeans &lt;- cld(means)\n# use sufficiently large upper margin\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\n# plot\nplot(cimeans, las = 1) # las option to put y-axis labels as God intended them\n\n\n\nEffet du genotype sur la résistance à la dessication avec un test de Tukey\n\n\n\nOn conclue donc que la résistance à la dessiccation (R), telle que mesurée par la survie dans des conditions chaudes et sèches, varie significativement entre les trois génotypes avec R(AA) &gt; R(Aa) &gt; R(aa).\nCependant, avant d’accepter cette conclusion, il faut éprouver les conditions d’application du test. Voici les diagnostics des résidus pour l’ANOVA à un critère de classification (non hiérarchique):\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(anova.simple)\n\n\n\nConditions d’applications du modèle anova.simple\n\n\n\n\n\n\nDonc, toutes les conditions d’application semblent être remplies, et on peut donc accepter les conclusions. Notez que si l’on compare le carré moyen des résidus de l’ANOVA hiérarchique et de l’ANOVA à un critère de classification (5.045 vs 5.292), ils sont presque identiques. Cela n’est pas surprenant compte tenu de la faible variabilité associée aux chambres de croissance pour chaque génotype.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#anova-non-paramétrique-avec-deux-facteurs-de-classification",
    "href": "34-anova_mult.html#anova-non-paramétrique-avec-deux-facteurs-de-classification",
    "title": "\n7  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n7.5 ANOVA non paramétrique avec deux facteurs de classification",
    "text": "7.5 ANOVA non paramétrique avec deux facteurs de classification\nL’ANOVA non paramétrique à deux critères de classification est une extension de celle à un critère de classification vue précédemment. Elle débute par une ANOVA faite sur les données transformées en rangs. Elle peut se faire sur des données avec ou sans réplicats.\nÀ partir du fichier stu2wdat.csv, effectuez une ANOVA non paramétrique à deux facteurs de classification pour examiner l’effet de sex et location sur rank(rdwght).\n\naov.rank &lt;- aov(\n  rank(rdwght) ~ sex * location,\n  contrasts = list(\n    sex = contr.sum, location = contr.sum\n  ),\n  data = Stu2wdat\n)\n\nL’extension de Schreirer-Ray-Hare au test de Kruskall-Wallis se fait ensuite à la main. Il faut d’abord calculer la statistique H égale au rapport de la somme des carrées de l’effet testé, divisée par le carré moyen total. On calcule la statistique H pour chacun des termes. Les statistiques H sont ensuite comparées à une distribution théorique de \\(\\chi^2\\) (chi-carré) en utilisant la commande pchisq(H, df, lower.tail = FALSE), où H et df sont les statistiques H calculées et les degrés de libertés, respectivement.\nTestez l’effet de sex et location sur rdwght. Que concluez-vous ? Comment ce résultat se compare-t-il à celui obtenu en faisant l’ANOVA paramétrique faite précédemment ?\n\nAnova(aov.rank, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: rank(rdwght)\n              Sum Sq  Df  F value    Pr(&gt;F)    \n(Intercept)  1499862   1 577.8673 &lt; 2.2e-16 ***\nsex            58394   1  22.4979 4.237e-06 ***\nlocation        1128   1   0.4347    0.5105    \nsex:location    1230   1   0.4738    0.4921    \nResiduals     472383 182                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nPour calculer l’extension Schreirer-Ray-Hare au test de Kruskall-Wallis, on doit d’abord calculer le carré moyen total (MS), i.e. la variance des données transformées en rang. Ici, on a 186 observations, donc des rangs; 1, 2, 3, … 186. La variance de cette série de 186 valeurs peut être calculée simplement par var(1:186).\nDonc on peut calculer la statistique H pour chaque terme:\n\nHsex &lt;- 58394 / var(1:186)\nHlocation &lt;- 1128 / var(1:186)\nHsexloc &lt;- 1230 / var(1:186)\n\nEt convertir ces statistiques en valeur de ps:\n\n# sex\nHsex\n\n[1] 20.14628\n\npchisq(Hsex, 1, lower.tail = FALSE)\n\n[1] 7.173954e-06\n\n# location\nHlocation\n\n[1] 0.3891668\n\npchisq(Hlocation, 1, lower.tail = FALSE)\n\n[1] 0.5327377\n\n# sex:location\nHsexloc\n\n[1] 0.4243574\n\npchisq(Hsexloc, 1, lower.tail = FALSE)\n\n[1] 0.5147707\n\n\nCes résultats sont semblables aux résultats de l’ANOVA non-paramétrique à deux critères de classification. Malgré la puissance réduite, il y a encore un effet significatif du sexe, mais ni interaction ni effet du site.\nIl y a toutefois une différence importante. Rappelez-vous que dans l’ANOVA paramétrique il y avait un effet significatif de sex en considérant le problème comme un modèle ANOVA à effet fixe. Cependant, si on traite le problème comme un modèle d’ANOVA à effet mixte l’effet significatif de sex peut en principe disparaître parce que le nombre de degré de liberté (dl) associés au carré moyen (CM) de l’interaction est plus faible que le nombre de dl du CM de l’erreur du modèle à effet fixes. Dans ce cas ci, cependant, le CM de l’interaction est environ la moitié du CM de l’erreur. Par conséquent, l’effet significatif de sex pourrait devenir encore plus significatif si le problème est analysé (comme il se doit) comme une ANOVA mixte. Encore une fois on peut voir l’importance de spécifier le modèle adéquat en ANOVA.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#comparaisons-multiples",
    "href": "34-anova_mult.html#comparaisons-multiples",
    "title": "\n7  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n7.6 Comparaisons multiples",
    "text": "7.6 Comparaisons multiples\nLes épreuves d’hypothèses subséquentes en ANOVA à plus d’un critère de classification dépendent des résultats initiaux de l’ANOVA. Si vous êtes intéressés à comparer des effets moyens d’un facteur pour tous les niveaux d’un autre facteur (par exemple l’effet du sexe sur la taille des esturgeons peu importe d’où ils viennent), alors vous pouvez procéder exactement tel que décrit dans la section sur les comparaisons multiples suivant l’ANOVA à un critère de classification. Pour comparer les moyennes des cellules entre elles, il faut spécifier l’interaction comme variable qui représente le groupe.\nLe fichier wmcdat2.csv contient des mesures de consommation d’oxygène, o2cons, de deux espèces, species, d’un mollusque (une patelle) à trois concentrations différentes d’eau de mer, conc. Ces données sont présentées à la p. 332 de Sokal et Rohlf 1995.\n\nEffectuez une ANOVA factorielle à deux critères de classification sur ces données en utilisant o2cons comme variable dépendante et species et conc comme les facteurs (il va probablement falloir changer le type de données de variable conc à facteur). Que concluez-vous ?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwmcdat2 &lt;- read.csv(\"data/wmcdat2.csv\")\nwmcdat2$species &lt;- as.factor(wmcdat2$species)\nwmcdat2$conc &lt;- as.factor(wmcdat2$conc)\nanova.model5 &lt;- lm(o2cons ~ species * conc, data = wmcdat2)\nAnova(anova.model5, type = 3)\n\n\n\n\n\n\nAnova Table (Type III tests)\n\nResponse: o2cons\n              Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept)  1185.60  1 124.0165 4.101e-14 ***\nspecies         0.09  1   0.0097   0.92189    \nconc           74.90  2   3.9172   0.02755 *  \nspecies:conc   23.93  2   1.2514   0.29656    \nResiduals     401.52 42                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComme l’effectif dans chaque cellule est relativement petit, il faudrait idéalement refaire cette analyse avec une ANOVA non-paramétrique. Pour le moment, contentons nous de la version paramétrique.\nExaminons les graphiques diagnostiques:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(anova.model5)\n\n\n\n\n\n\n\n\n\n\nLes variances semblent donc égales. Le test de normalité donne:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nshapiro.test(residuals(anova.model5))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(anova.model5)\nW = 0.93692, p-value = 0.01238\n\n\n\n\n\nIl y a donc évidence de non-normalité, mais à part ça tout semble aller. Comme l’ANOVA est relativement robuste à la non-normalité, on va regarder de l’autre coté. (Si vous voulez être plus confiants, vous pouvez tourner une ANOVA non paramétrique. Vous arriverez aux mêmes conclusions.)\n\nÀ la suite des résultats que vous venez d’obtenir, quelles moyennes voudriez-vous comparer ? Pourquoi?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\najouter une explication ici\n\n\n\nOn conclue donc qu’il n’y a pas de différence entre les espèces et que l’effet de la concentration ne dépends pas de l’espèce (il n’y a pas d’interaction). Par conséquent, les seules comparaisons justifiables sont entre les concentrations:\n\n# fit simplified model\nanova.model6 &lt;- aov(o2cons ~ conc, data = wmcdat2)\n# Make Tukey multiple comparisons\nTukeyHSD(anova.model6)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = o2cons ~ conc, data = wmcdat2)\n\n$conc\n           diff       lwr        upr     p adj\n75-50  -4.63625 -7.321998 -1.9505018 0.0003793\n100-50 -3.25500 -5.940748 -0.5692518 0.0141313\n100-75  1.38125 -1.304498  4.0669982 0.4325855\n\npar(mfrow = c(1, 1))\n# Graph of all comparisons for conc\ntuk &lt;- glht(anova.model6, linfct = mcp(conc = \"Tukey\"))\n# extract information\ntuk.cld &lt;- cld(tuk)\n# use sufficiently large upper margin\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\n# plot\nplot(tuk.cld)\n\n\n\nComparaison de Tukey des moyennes de consommation d’oxygèn en fonction del la concentration\n\n\npar(old.par)\n\nIl y a donc une différence de consommation d’oxygène significative lorsque la salinité est réduite de 50%, mais pas à 25% de réduction.\n\nRépétez les deux analyses précédentes sur les données du fichier wmc2dat2.csv. Comment les résultats se comparent-ils à ceux obt$nus précédemment ?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nwmc2dat2 &lt;- read.csv(\"data/wmc2dat2.csv\")\nwmc2dat2$species &lt;- as.factor(wmc2dat2$species)\nwmc2dat2$conc &lt;- as.factor(wmc2dat2$conc)\nanova.model7 &lt;- lm(o2cons ~ species * conc, data = wmc2dat2)\n\n\n\n\nEn utilisant wmc2dat2.csv, on obtient:\n\n\nAnova Table (Type III tests)\n\nResponse: o2cons\n             Sum Sq Df F value    Pr(&gt;F)    \n(Intercept)  343.09  1 36.2132 3.745e-07 ***\nspecies      133.52  1 14.0929 0.0005286 ***\nconc          66.76  2  3.5232 0.0385011 *  \nspecies:conc 168.15  2  8.8742 0.0006101 ***\nResiduals    397.91 42                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDans ce cas ci, il y a une interaction significative, et il n’est par conséquent pas approprié de comparer les moyennes regroupées par espèce ou concentration. Ceci est clairement visualisé par un graphique d’interaction:\n\nwith(wmc2dat2, interaction.plot(conc, species, o2cons))\n\n\n\n\n\n\n\n\nToujours en utilisant les données de wmc2dat2.csv, comparez les 6 moyennes avec l’ajustement Bonferonni. Pour ce faire, il sera utile de créer une nouvelle variable qui combine species et conc:\n\n\nwmc2dat2$species.conc &lt;- as.factor(paste0(wmc2dat2$species, wmc2dat2$conc))\n\nensuite on peut faire les comparaisons de Bonferroni:\n\nwith(wmc2dat2, pairwise.t.test(o2cons, species.conc, p.adj = \"bonf\"))\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  o2cons and species.conc \n\n     A100   A50    A75    B100   B50   \nA50  0.1887 -      -      -      -     \nA75  1.0000 1.0000 -      -      -     \nB100 0.7223 1.0000 1.0000 -      -     \nB50  1.0000 0.0079 0.0929 0.0412 -     \nB75  0.6340 1.0000 1.0000 1.0000 0.0350\n\nP value adjustment method: bonferroni \n\n\nCes comparaisons sont un peu plus difficiles à interpréter, mais l’analyse examine essentiellement les différences entre les concentrations de l’eau dans l’espèce A (nommé adj1) et pour les différences entre les concentrations dans l’espèce B (nommé adj2). Cette analyse indique que la différence principale est entre la concentration de 50% pour l’espèce B et les concentrations de 75 et 100% de l’espèce B, tandis qu’il n’y a aucunes différences significatives pour l’espèce A.\nJe trouve ces tableaux de résultats peu satisfaisants parce qu’ils indiquent seulement les valeur de ps sans indices de la taille de l’effet. On peut obtenir à la fois le résultat des tests de comparaison multiple et un indice de la taille de l’effet à l’aide du code suivant:\n\n# fit one-way anova comparing all combinations of species.conc combinations\nanova.modelx &lt;- aov(o2cons ~ species.conc, data = wmc2dat2)\ntuk2 &lt;- glht(anova.modelx, linfct = mcp(species.conc = \"Tukey\"))\n# extract information\ntuk2.cld &lt;- cld(tuk2)\n# use sufficiently large upper margin\nold.par &lt;- par(mai = c(1, 1, 1.25, 1))\n# plot\nplot(tuk2.cld)\n\n\n\n\n\n\npar(old.par)\n\nDans cette analyse on a utilisé le CM = 9.474 du modèle d’ANOVA pour comparer les moyennes. En ce faisant, on présume qu’il s’agit d’une situation d’ANOVA à effet fixes, ce qui n’est peut-être pas le cas (conc est certainement fixe, mais species peut être fixe ou aléatoire).",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#test-de-permutation-pour-lanova-à-deux-facteurs-de-classification",
    "href": "34-anova_mult.html#test-de-permutation-pour-lanova-à-deux-facteurs-de-classification",
    "title": "\n7  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n7.7 Test de permutation pour l’ANOVA à deux facteurs de classification",
    "text": "7.7 Test de permutation pour l’ANOVA à deux facteurs de classification\nQuand les données ne rencontrent pas les conditions d’application des tests paramétriques d’ANOVA à un ou plusieurs facteurs de classification, il est possible d’utiliser les tests de permutation comme une alternative aux tests non-paramétriques pour calculer des p-valeurs. Le code suivant est pour un modèle I d’une ANOVA à deux facteurs de classification. Je vous laisse le soin d’adapter ce code pour d’autres modèles. (J’offre même des points boni pour une solution élégante pour des modèles à plusieurs facteurs de classification).\n\n###########################################################\n# Permutation test for two way ANOVA\n# Ter Braak creates residuals from cell means and then permutes across\n# all cells\n# This can be accomplished by taking residuals from the full model\n# modified from code written by David C. Howell\n# http://www.uvm.edu/~dhowell/StatPages/More_Stuff/Permutation%20Anova/PermTestsAnova.html\nnreps &lt;- 500\ndependent &lt;- Stu2wdat$rdwght\nfactor1 &lt;- as.factor(Stu2wdat$sex)\nfactor2 &lt;- as.factor(Stu2wdat$location)\nmy.dataframe &lt;- data.frame(dependent, factor1, factor2)\nmy.dataframe.noNA &lt;- my.dataframe[complete.cases(my.dataframe), ]\nmod &lt;- lm(dependent ~ factor1 + factor2 + factor1:factor2,\n  data = my.dataframe.noNA\n)\nres &lt;- mod$residuals\nTBint &lt;- numeric(nreps)\nTB1 &lt;- numeric(nreps)\nTB2 &lt;- numeric(nreps)\nANOVA &lt;- summary(aov(mod))\ncat(\n  \" The standard ANOVA for these data follows \",\n  \"\\n\"\n)\nF1 &lt;- ANOVA[[1]]$\"F value\"[1]\nF2 &lt;- ANOVA[[1]]$\"F value\"[2]\nFinteract &lt;- ANOVA[[1]]$\"F value\"[3]\nprint(ANOVA)\ncat(\"\\n\")\ncat(\"\\n\")\nTBint[1] &lt;- Finteract\nfor (i in 2:nreps) {\n  newdat &lt;- sample(res, length(res), replace = FALSE)\n  modb &lt;- summary(aov(newdat ~ factor1 + factor2 +\n    factor1:factor2,\n  data = my.dataframe.noNA\n  ))\n  TBint[i] &lt;- modb[[1]]$\"F value\"[3]\n  TB1[i] &lt;- modb[[1]]$\"F value\"[1]\n  TB2[i] &lt;- modb[[1]]$\"F value\"[2]\n}\nprobInt &lt;- length(TBint[TBint &gt;= Finteract]) / nreps\nprob1 &lt;- length(TB1[TB1 &gt;= F1]) / nreps\nprob2 &lt;- length(TB2[TB1 &gt;= F2]) / nreps\ncat(\"\\n\")\ncat(\"\\n\")\nprint(\"Resampling as in ter Braak with unrestricted sampling\nof cell residuals. \")\ncat(\n  \"The probability for the effect of Interaction is \",\n  probInt, \"\\n\"\n)\ncat(\n  \"The probability for the effect of Factor 1 is \",\n  prob1, \"\\n\"\n)\ncat(\n  \"The probability for the effect of Factor 2 is \",\n  prob2, \"\\n\"\n)\n\nSi vous avez la chance d’avoir accès au package lmPerm, vous pouvez effectuer le test de permutation beaucoup plus rapidement et facilement:\n\n#######################################################################\n## lmPerm version of permutation test\nlibrary(lmPerm)\n# for generality, copy desired dataframe to mydata\n# and model formula to myformula\nmydata &lt;- Stu2wdat\nmyformula &lt;- as.formula(\"rdwght ~ sex+location+sex:location\")\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(myformula, data = mydata)\n# Calculate permutation p-value\nanova(lmp(myformula, data = mydata, perm = \"Prob\", center = FALSE, Ca = 0.001))",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "34-anova_mult.html#bootstrap-pour-lanova-à-deux-facteurs-de-classification",
    "href": "34-anova_mult.html#bootstrap-pour-lanova-à-deux-facteurs-de-classification",
    "title": "\n7  ANOVA à critères multiples : plans factoriels et hiérarchiques\n",
    "section": "\n7.8 Bootstrap pour l’ANOVA à deux facteurs de classification",
    "text": "7.8 Bootstrap pour l’ANOVA à deux facteurs de classification\nDans la plupart des cas, les tests de permutation seront plus appropriés que le bootstrap pour les designs d’ANOVA. J’ai quand même un bout de code qui pourra servir si vous en avez besoin:\n\n############################################################\n###########\n# Bootstrap for two-way ANOVA\n# You possibly want to edit bootfunction.mod1 to return other values\n# Here it returns the standard coefficients of the fitted model\n# Requires boot library\n#\nnreps &lt;- 5000\ndependent &lt;- Stu2wdat$rdwght\nfactor1 &lt;- as.factor(Stu2wdat$sex)\nfactor2 &lt;- as.factor(Stu2wdat$location)\nmy.dataframe &lt;- data.frame(dependent, factor1, factor2)\nmy.dataframe.noNA &lt;- my.dataframe[complete.cases(my.dataframe), ]\nlibrary(boot)\n# Fit model on observed data\nmod1 &lt;- aov(dependent ~ factor1 + factor2 + factor1:factor2,\n  data = my.dataframe.noNA\n)\n\n\n# Bootstrap 1000 time using the residuals bootstraping methods to\n# keep the same unequal number of observations for each level of the indep. var.\nfit &lt;- fitted(mod1)\ne &lt;- residuals(mod1)\nX &lt;- model.matrix(mod1)\nbootfunction.mod1 &lt;- function(data, indices) {\n  y &lt;- fit + e[indices]\n  bootmod &lt;- lm(y ~ X)\n  coefficients(bootmod)\n}\nbootresults &lt;- boot(my.dataframe.noNA, bootfunction.mod1,\n  R = 1000\n)\nbootresults\n## Calculate 90% CI and plot bootstrap estimates separately for each model parameter\nboot.ci(bootresults, conf = 0.9, index = 1)\nplot(bootresults, index = 1)\nboot.ci(bootresults, conf = 0.9, index = 3)\nplot(bootresults, index = 3)\nboot.ci(bootresults, conf = 0.9, index = 4)\nplot(bootresults, index = 4)\nboot.ci(bootresults, conf = 0.9, index = 5)\nplot(bootresults, index = 5)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>ANOVA à critères multiples : plans factoriels et hiérarchiques</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html",
    "href": "35-reg_mult.html",
    "title": "\n8  Régression multiple\n",
    "section": "",
    "text": "8.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#set-reg-mul",
    "href": "35-reg_mult.html#set-reg-mul",
    "title": "\n8  Régression multiple\n",
    "section": "",
    "text": "les paquets R:\n\nggplot2\ncar\nlmtest\nsimpleboot\nboot\nMuMIn\n\n\nles fichiers de données\n\nMregdat.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#conseils-généraux",
    "href": "35-reg_mult.html#conseils-généraux",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.2 Conseils généraux",
    "text": "8.2 Conseils généraux\nLes variables qui intéressent les biologistes sont généralement influencées par plusieurs facteurs, et une description exacte ou une prédiction de la variable dépendante requiert que plus d’une variable soit incluse dans le modèle. La régression multiple permet de quantifier l’effet de plusieurs variables continues sur la variable dépendante.\nIl est important de réaliser que la maîtrise de la régression multiple ne s’acquiert pas instantanément. Les débutants doivent garder à l’esprit plusieurs points importants :\n\nUn modèle de régression multiple peut être hautement significatif même si aucun des termes pris isolément ne l’est (ceci est causé par la multicolinéarité),\nUn modèle peut ne pas être significatif alors que l’un ou plusieurs des termes le sont (ceci est un signe d’un modèle trop complexe (“overfitting”)) et,\nÀ moins que les variables indépendantes soient parfaitement orthogonales (c’est-à-dire qu’il n’y ait aucune corrélation entre elles et donc pas de multicolinéarité) les diverses approches de sélection des variables indépendantes peuvent mener à des modèles différents.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#premières-régressions-multiples",
    "href": "35-reg_mult.html#premières-régressions-multiples",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.3 Premières régressions multiples",
    "text": "8.3 Premières régressions multiples\nLe fichier Mregdat.csv contient des données de richesse spécifique de quatre groupes d’organismes dans 30 marais de la région Ottawa-Cornwall-Kingston. Les variables sont:\n\nla richesse spécifique:\n\ndes oiseaux (bird, et son logarithme base 10 logbird)\ndes mammifères (mammal, logmam)\ndes amphibiens et reptiles (herptile, logherp)\ndes vertébrés (totsp, logtot)\n\n\nles coordonnées des sites (lat, long)\nla superficie du marais (logarea)\nle pourcentage du marais inondé toute l’année (swamp)\nle pourcentage des terres couvertes par des forêts dans un rayon de 1km du marais (cpfor2)\nla densité des routes pavées (en m/ha) dans un rayon de 1km du marais (thtden).\n\nNous allons nous concentrer sur les amphibiens et les reptiles (herptile) pour cet exemple, il est donc avisé d’examiner la distribution de cette variable et les corrélations avec les variables indépendantes potentielles:\n\nmydata &lt;- read.csv(\"data/Mregdat.csv\")\nscatterplotMatrix(\n  ~ logherp + logarea + cpfor2 + thtden + swamp,\n  regLine = TRUE, smooth = TRUE, diagonal = TRUE,\n  data = mydata\n)\n\n\n\nMatrice de rélation et densité pour la richesse spécifique des amphibiens et reptiles\n\n\n\n\nEn utilisant les données de ce fichier, faites la régression simple de logherp en fonction de logarea . Que concluez-vous à partir de cette analyse?\n\n\nmodel.loga &lt;- lm(logherp ~ logarea, data = mydata)\nsummary(model.loga)\n\n\nCall:\nlm(formula = logherp ~ logarea, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.38082 -0.09265  0.00763  0.10409  0.46977 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.18503    0.15725   1.177 0.249996    \nlogarea      0.24736    0.06536   3.784 0.000818 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1856 on 26 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.3552,    Adjusted R-squared:  0.3304 \nF-statistic: 14.32 on 1 and 26 DF,  p-value: 0.0008185\n\npar(mfrow = c(2, 2))\nplot(model.loga)\n\n\n\nConditions d’applications de la régression de logherp sur logarea\n\n\n\nIl semble donc y avoir une relation positive entre la richesse spécifique des reptiles et des amphibiens et la surface des marais. La régression n’explique cependant qu’environ le tiers de la variabilité (R 2 =0.355). L’analyse des résidus indique qu’il n’y a pas de problème avec la normalité, l’homoscédasticité, ni l’indépendance.\n\nFaites ensuite la régression de logherp en fonction de cpfor2 . Que concluez-vous?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.logcp &lt;- lm(logherp ~ cpfor2, data = mydata)\nsummary(model.logcp)\n\n\nCall:\nlm(formula = logherp ~ cpfor2, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.49095 -0.10266  0.05881  0.16027  0.25159 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.609197   0.104233   5.845 3.68e-06 ***\ncpfor2      0.002706   0.001658   1.632    0.115    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2202 on 26 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.09289,   Adjusted R-squared:  0.058 \nF-statistic: 2.662 on 1 and 26 DF,  p-value: 0.1148\n\n\n\n\n\nIci, on doit accepter l’hypothèse nulle et conclure qu’il n’y a pas de relation entre la richesse spécifique dans les marais (logherp) et la proportion de forêts sur les terres adjacentes (cpfor2). Qu’est-ce qui arrive quand on fait une régression avec les 2 variables indépendantes?\n\nRefaites la régression de logherp enfonction de logarea et cpfor2 à la fois. Que concluez-vous?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.mcp &lt;- lm(logherp ~ logarea + cpfor2, data = mydata)\nsummary(model.mcp)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40438 -0.11512  0.01774  0.08187  0.36179 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.027058   0.166749   0.162 0.872398    \nlogarea     0.247789   0.061603   4.022 0.000468 ***\ncpfor2      0.002724   0.001318   2.067 0.049232 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.175 on 25 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.4493,    Adjusted R-squared:  0.4052 \nF-statistic:  10.2 on 2 and 25 DF,  p-value: 0.0005774\n\n\n\n\n\nOn voit donc qu’on peut rejeter les 2 hypothèses nulles que la pente de la régression de logherp sur logarea est zéro et que la pente de la régression de logherp sur cpfor2 est zéro. Pourquoi cpfor2 devient-il un facteur significatif dans la régression multiple alors qu’il n’est pas significatif dans la régression simple? Parce qu’il est parfois nécessaire de contrôler pour l’effet d’une variable pour pouvoir détecter les effets plus subtils d’autres variables. Ici, il y a une relation significative entre logherp et logarea qui masque l’effet de cpfor2 sur logherp . Lorsque le modèle tient compte des deux variables explicatives, il devient possible de détecter l’effet de cpfor2 .\n\nAjustez un autre modèle, cette fois en remplaçant cpfor2 par thtden (logherp ~ logarea + thtden). Que concluez-vous?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.mden &lt;- lm(logherp ~ logarea + thtden, data = mydata)\nsummary(model.mden)\n\n\nCall:\nlm(formula = logherp ~ logarea + thtden, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.31583 -0.12326  0.02095  0.13201  0.31674 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.37634    0.14926   2.521 0.018437 *  \nlogarea      0.22504    0.05701   3.947 0.000567 ***\nthtden      -0.04196    0.01345  -3.118 0.004535 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1606 on 25 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.5358,    Adjusted R-squared:  0.4986 \nF-statistic: 14.43 on 2 and 25 DF,  p-value: 6.829e-05\n\n\n\n\n\nOn rejette donc l’hypothèse nulle que la richesse spécifique n’est pas influencée par la taille des marais (logarea) ni par la densité des routes (thtden). Notez qu’ici il y a une relation négative significative entre la richesse spécifique des amphibiens et reptiles et la densité des routes sur les terres adjacentes, tandis que la relation est positive pour la taille des marais et pour la densité des forêts (cpfor2 ; résultat de la dernière régression).\nLe \\(R^2\\) de ce modèle est plus élevé que pour le précédent, reflétant une corrélation plus forte entre logherp et thtden qu’entre logherp et cpfor2 .\nLa richesse spécifique des reptiles et amphibiens semble donc reliée à la surface de marais (logarea), la densité des routes (thtden), et possiblement au couvert forestier sur les terres adjacentes aux marais (cpfor2). Cependant, les trois variables ne sont peut-être pas nécessaires dans un modèle prédictif. Si deux des trois variables (disons cpfor2 et thtden) sont parfaitement corrélées, alors l’effet de thtden ne serait rien de plus que celui de cpfor2 (et vice-versa) et un modèle incluant l’une des deux variables ferait des prédictions identiques à un modèle incluant ces deux variables (en plus de logarea).\n\nEstimez un modèle de régression avec logherp comme variable dépendante et logarea, cpfor2 et thtden comme variables indépendantes. Que concluez-vous?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.mtri &lt;- lm(logherp ~ logarea + cpfor2 + thtden, data = mydata)\nsummary(model.mtri)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2 + thtden, data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30729 -0.13779  0.02627  0.11441  0.29582 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.284765   0.191420   1.488 0.149867    \nlogarea      0.228490   0.057647   3.964 0.000578 ***\ncpfor2       0.001095   0.001414   0.774 0.446516    \nthtden      -0.035794   0.015726  -2.276 0.032055 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1619 on 24 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.5471,    Adjusted R-squared:  0.4904 \nF-statistic: 9.662 on 3 and 24 DF,  p-value: 0.0002291\n\n\n\n\n\nPlusieurs choses sont à noter ici: 1. Tel que prédit, le coefficient de régression pour cpfor2 n’est plus significativement différent de 0. Une fois que la variabilité attribuable à logarea et thtden est enlevée, il ne reste qu’une fraction nonsignificative de la variabilité attribuable à cpfor2 . 2. Le \\(R^2\\) pour ce modèle(0.547) n’est que légèrement supérieur au \\(R^2\\) du modèle avec seulement logarea et thtden (.536), ce qui confirme que cpfor2 n’explique pas grand-chose de plus.\nNotez aussi que même si le coefficient de régression pour thtden n’a pas beaucoup changé par rapport à ce qui avait été estimé lorsque seul thtden et logarea étaient dans le modèle (0-.036 vs -0.042), l’erreur type pour l’estimé du coefficient est plus grand, et ce modèle plus complexe mène à un estimé moins précis. Si la corrélation entre thtden et cpfor2 était plus forte, la décroissance de la précision serait encore plus grande.\nOn peut comparer les deux derniers modèles (i.e., le modèle incluant les 3 variables et celui avec seulement logarea and thtden) pour décider lequel privilégier.\n\nanova(model.mtri, model.mden)\n\nAnalysis of Variance Table\n\nModel 1: logherp ~ logarea + cpfor2 + thtden\nModel 2: logherp ~ logarea + thtden\n  Res.Df     RSS Df Sum of Sq     F Pr(&gt;F)\n1     24 0.62937                          \n2     25 0.64508 -1 -0.015708 0.599 0.4465\n\n\nCette comparaison révèle que le modèle à 3 variables ne fait pas de prédictions significativement meilleures que le modèle avec seulement Logarea et thtden . Ce résultat n’est pas surprenant puisque le test de signification pour cpfor2 dans le modèle complet indique qu’il faut accepter l’hypothèse nulle.\nÀ la suite de cette analyse, on doit conclure que:\n\nLe meilleur modèle est celui incluant thtden et logarea .\nIl y a une relation négative entre la richesse spécifique des amphibiens et reptiles et la densité des routes sur les terres adjacentes.\nIl y a une relation positive entre la richesse spécifique et la taille des marais.\n\nNotez que le “meilleur” modèle n’est pas nécessairement le modèle parfait, seulement le meilleur n’utilisant que ces trois variables indépendantes. Il est évident qu’il y a d’autres facteurs qui contrôlent la richesse spécifique dans les marais puisque, même le “meilleur” modèle n’explique que la moitié de la variabilité.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#régression-multiple-pas-à-pas-stepwise",
    "href": "35-reg_mult.html#régression-multiple-pas-à-pas-stepwise",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.4 Régression multiple pas-à-pas (stepwise)",
    "text": "8.4 Régression multiple pas-à-pas (stepwise)\nQuand le nombre de variables prédictives est restreint, comme dans l’exemple précédent, il est aisé de comparer manuellement les modèles pour sélectionner le plus adéquat. Cependant, lorsque le nombre de variables indépendantes augmente, cette approche n’est rapidement plus utilisable et il est alors utile d’utiliser une méthode automatisée.\nLa sélection pas à pas avec R utilise le Critère Informatif de Akaike (Akaike Information Criterion, \\(AIC = 2* ln(RSS) + 2K\\) où K le nombre de variables indépendantes, n est le nombre d’observations, et RSS est la somme des carrés des résidus) comme mesure de la qualité d’ajustement des modèles. Cette mesure favorise la précision des prédictions et pénalise la complexité. Lorsque l’on compare des modèles par AIC, le modèle avec le plus petit AIC est le modèle à préférer.\n\nUtiliser la fonction step pour activer la sélection pas à pas des variables indépendantes sur le modèles de régression incluant logarea, cpfor2 et thtden:\n\n\n# Stepwise Regression\nstep.mtri &lt;- step(model.mtri, direction = \"both\")\n\nStart:  AIC=-98.27\nlogherp ~ logarea + cpfor2 + thtden\n\n          Df Sum of Sq     RSS     AIC\n- cpfor2   1   0.01571 0.64508 -99.576\n&lt;none&gt;                 0.62937 -98.267\n- thtden   1   0.13585 0.76522 -94.794\n- logarea  1   0.41198 1.04135 -86.167\n\nStep:  AIC=-99.58\nlogherp ~ logarea + thtden\n\n          Df Sum of Sq     RSS     AIC\n&lt;none&gt;                 0.64508 -99.576\n+ cpfor2   1   0.01571 0.62937 -98.267\n- thtden   1   0.25092 0.89600 -92.376\n- logarea  1   0.40204 1.04712 -88.013\n\nstep.mtri$anova # display results\n\n      Step Df   Deviance Resid. Df Resid. Dev       AIC\n1          NA         NA        24  0.6293717 -98.26666\n2 - cpfor2  1 0.01570813        25  0.6450798 -99.57640\n\n\nR nous donne:\n\nL’ajustement (mesuré par AIC) du modèle complet en premier lieu.\nL’AIC des modèles dans lesquels une variable a été enlevée du modèle complet. Notez que c’est seulement en enlevant cpfor2 du modèle qu’on peut réduire l’AIC\nLa valeur de AIC pour les modèles auxquels on enlève ou on ajoute une variable au modèle sélectionné à la première étape.(i.e. logherp ~ logarea + thtden). Notez qu’aucun des modèles n’a un AIC inférieur à ce modèle.\n\nAu lieu de débuter par le modèle complet (saturé) et enlever des termes, on peut commencer par le modèle nul et ajouter des termes:\n\n# Forward selection approach\nmodel.null &lt;- lm(logherp ~ 1, data = mydata)\nstep.f &lt;- step(\n  model.null,\n  scope = ~ . + logarea + cpfor2 + thtden, direction = \"forward\"\n)\n\nStart:  AIC=-82.09\nlogherp ~ 1\n\n          Df Sum of Sq    RSS     AIC\n+ logarea  1   0.49352 0.8960 -92.376\n+ thtden   1   0.34241 1.0471 -88.013\n+ cpfor2   1   0.12907 1.2605 -82.820\n&lt;none&gt;                 1.3895 -82.091\n\nStep:  AIC=-92.38\nlogherp ~ logarea\n\n         Df Sum of Sq     RSS     AIC\n+ thtden  1   0.25093 0.64508 -99.576\n+ cpfor2  1   0.13078 0.76522 -94.794\n&lt;none&gt;                0.89600 -92.376\n\nStep:  AIC=-99.58\nlogherp ~ logarea + thtden\n\n         Df Sum of Sq     RSS     AIC\n&lt;none&gt;                0.64508 -99.576\n+ cpfor2  1  0.015708 0.62937 -98.267\n\nstep.f$anova # display results\n\n       Step Df  Deviance Resid. Df Resid. Dev       AIC\n1           NA        NA        27  1.3895281 -82.09073\n2 + logarea -1 0.4935233        26  0.8960048 -92.37639\n3  + thtden -1 0.2509250        25  0.6450798 -99.57640\n\n\nLe résultat final est le même, mais la trajectoire est différente. Dans ce cas, R débute avec le modèle le plus simple et ajoute une variable indépendante à chaque étape, sélectionnant la variable minimisant AIC à cette étape. Le modèle de départ a donc seulement une ordonnée à l’origine. Puis, logarea est ajouté, suivi de thtden. cpfor2 n’est pas ajouté au modèle, car son addition fait augmenter l’AIC.\nIl est recommandé de comparer le résultat final de plusieurs approches. Si le modèle retenu diffère selon l’approche utilisée, c’est un signe que le “meilleur” modèle est possiblement difficile à identifier et que vous devriez être circonspects dans vos inférences. Dans notre exemple, pas de problème: toutes les méthodes convergent sur le même modèle final.\nPour conclure cette section, quelques conseils concernant les méthodes automatisées de sélection des variables indépendantes:\n\nLes différentes méthodes de sélection des variables indépendantes peuvent mener à des modèles différents. Il est souvent utile d’essayer plus d’une méthode et de comparer les résultats. Si les résultats diffèrent, c’est presque toujours à cause de multicolinéarité entre les variables indépendantes.\nAttention à la régression pas-à-pas. Les auteurs de SYSTAT en disent:\n\n\nStepwise regression is probably the most abused computerized statistical technique ever devised. If you think you need automated stepwise regression to solve a particular problem, you probably don’t. Professional statisticians rarely use automated stepwise regression because it does not necessarily find the “best” fitting model, the “real” model, or alternative “plausible” models. Furthermore, the order in which variables enter or leave a stepwise program is usually of no theoretical significance. You are always better off thinking about why a model could generate your data and then testing that model.\n\nEn bref, on abuse trop souvent de cette technique.\n\nIl faut toujours garder à l’esprit que l’existence d’une régression significative n’est pas suffisante pour prouver une relation causale.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#détecter-la-multicolinéarité",
    "href": "35-reg_mult.html#détecter-la-multicolinéarité",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.5 Détecter la multicolinéarité",
    "text": "8.5 Détecter la multicolinéarité\nLa multicolinéarité est la présence de corrélations entre les variables indépendantes. Lorsqu’elle est extrême (multicolinéarité parfaite) elle empêche l’estimation des modèles statistiques. Lorsqu’elle est grande ou modérée, elle réduit la puissance de détection de l’effet des variables indépendantes individuellement, mais elle n’empêche pas le modèle de faire des prédictions.\nUn des indices les plus utilisés pour quantifier la multicolinéarité et le facteur d’inflation de la variance (VIF, variance inflation factor). Le fichier d’aide du package HH explique ainsi son calcul:\n\nA simple diagnostic of collinearity is the variance inflation factor, VIF one for each regression coefficient (other than the intercept). Since the condition of collinearity involves the predictors but not the response, this measure is a function of the X’s but not of Y. The VIF for predictor i is \\(1/(1-R_i^2)\\), where \\(R_i^2\\) is the \\(R^2\\) from a regression of predictor i against the remaining predictors. If \\(R_i^2\\) is close to 1, this means that predictor i is well explained by a linear function of the remaining predictors, and, therefore, the presence of predictor i in the model is redundant. Values of VIF exceeding 5 are considered evidence of collinearity: The information carried by a predictor having such a VIF is contained in a subset of the remaining predictors. If, however, all of a model’s regression coefficients differ significantly from 0 (p-value &lt; .05), a somewhat larger VIF may be tolerable.\n\nBref, les VIF indiquent de combien l’incertitude de chaque coefficient de régression est augmentée par la multicolinéarité.\nAttrappe. Il y a plusieurs fonctions vif() (j’en connais au moins trois dans les extensions car, HH et DAAG), et je ne sais pas en quoi elles diffèrent.\nOn peut calculer les VIF avec la fonction vif() de l’extension car: :\n\nlibrary(car)\nvif(model.mtri)\n\n logarea   cpfor2   thtden \n1.022127 1.344455 1.365970 \n\n\nIci, il n’y a pas d’évidence de multicolinéarité car toutes les valeurs de VIF sont près de 1.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#régression-polynomiale",
    "href": "35-reg_mult.html#régression-polynomiale",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.6 Régression polynomiale",
    "text": "8.6 Régression polynomiale\nLa régression requiert la linéarité de la relation entre les variables dépendante et indépendante(s). Lorsque la relation n’est pas linéaire, il est parfois possible de linéariser la relation en effectuant une transformation sur une ou plusieurs variables. Cependant, dans bien des cas il est impossible de transformer les axes pour rendre la relation linéaire. On doit alors utiliser une forme ou l’autre de régression nonlinéaire. La forme la plus simple de régression non-linéaire est la régression polynomiale dans laquelle les variables indépendantes sont à une puissance plus grande que 1 (Ex : \\(X^2\\) ou \\(X^3\\)).\n\nFaites un diagramme de dispersion des résidus (residual) de la régression logherp ~ logarea en fonction de swamp .\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# problème avec les données de manquantes dans logherp\nmysub &lt;- subset(mydata, !is.na(logherp))\n# ajouter les résidus dans les donnée\nmysub$resloga &lt;- residuals(model.loga)\nggplot(data = mysub, aes(y = resloga, x = swamp)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\nRelation entre swamp et les résidus de la régression entre logherp et logarea\n\n\n\n\n\n\n\nL’examen de ce graphique suggère qu’il y a une forte relation entre les deux variables, mais qu’elle n’est pas linéaire. Essayez de faire une régression de residual sur swamp . Quelle est votre conclusion?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.resloga &lt;- lm(resloga ~ swamp, mysub)\nsummary(model.resloga)\n\n\nCall:\nlm(formula = resloga ~ swamp, data = mysub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.35088 -0.13819  0.00313  0.10849  0.45802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.084571   0.109265   0.774    0.446\nswamp       -0.001145   0.001403  -0.816    0.422\n\nResidual standard error: 0.1833 on 26 degrees of freedom\nMultiple R-squared:  0.02498,   Adjusted R-squared:  -0.01252 \nF-statistic: 0.666 on 1 and 26 DF,  p-value: 0.4219\n\n\n\n\n\nEn deux mots, l’ajustement est épouvantable! Malgré le fait que le graphique suggère une relation très forte entre les deux variables. Cependant, cette relation n’est pas linéaire… (ce qui est également apparent si vous examinez les résidus du modèle linéaire).\n\nRefaites la régression d’en haut, mais cette fois incluez un terme pour représenter \\(swamp^2\\) . L’expression devrait apparaître comme: \\(Y ~ X + I(X^2)\\) . Que concluez-vous? Qu’est-ce que l’examen des résidus de cette régression multiple révèle?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.resloga2 &lt;- lm(resloga ~ swamp + I(swamp^2), mysub)\nsummary(model.resloga2)\n\n\nCall:\nlm(formula = resloga ~ swamp + I(swamp^2), data = mysub)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.181185 -0.085350  0.007377  0.067327  0.242455 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -7.804e-01  1.569e-01  -4.975 3.97e-05 ***\nswamp        3.398e-02  5.767e-03   5.892 3.79e-06 ***\nI(swamp^2)  -2.852e-04  4.624e-05  -6.166 1.90e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1177 on 25 degrees of freedom\nMultiple R-squared:  0.6132,    Adjusted R-squared:  0.5823 \nF-statistic: 19.82 on 2 and 25 DF,  p-value: 6.972e-06\n\n\n\n\n\nIl devient évident que si on corrige la richesse spécifique pour la taille des marais, une fraction importante de la variabilité résiduelle peut être associée à swamp, selon une relation quadratique. Si vous examinez les résidus, vous observerez que l’ajustement est nettement meilleur qu’avec le modèle linéaire.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(model.resloga2)\n\n\n\nRelation\n\n\n\n\n\n\n\nEn vous basant sur les résultats de la dernière analyse, comment suggérez-vous de modifier le modèle de régression multiple? Quel est, d’après vous, le meilleur modèle? Pourquoi? Ordonnez les différents facteurs en ordre croissant de leur effet sur la richesse spécifique des reptiles.\n\nSuite à ces analyses, il semble opportun d’essayer d’ajuster un modèle incluant logarea, thtden, cpfor2, swamp et swamp^2 :\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.poly1 &lt;- lm(\n  logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2),\n  data = mydata\n)\nsummary(model.poly1)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2), \n    data = mydata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.201797 -0.056170 -0.002072  0.051814  0.205626 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.203e-01  1.813e-01  -1.766   0.0912 .  \nlogarea      2.202e-01  3.893e-02   5.656 1.09e-05 ***\ncpfor2      -7.864e-04  9.955e-04  -0.790   0.4380    \nthtden      -2.929e-02  1.048e-02  -2.795   0.0106 *  \nswamp        3.113e-02  5.898e-03   5.277 2.70e-05 ***\nI(swamp^2)  -2.618e-04  4.727e-05  -5.538 1.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1072 on 22 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8181,    Adjusted R-squared:  0.7767 \nF-statistic: 19.78 on 5 and 22 DF,  p-value: 1.774e-07\n\n\n\n\n\nLes résultats de cette analyse suggèrent qu’on devrait probablement exclure cpfor2 du modèle:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.poly2 &lt;- lm(\n  logherp ~ logarea + thtden + swamp + I(swamp^2),\n  data = mydata\n)\nsummary(model.poly2)\n\n\nCall:\nlm(formula = logherp ~ logarea + thtden + swamp + I(swamp^2), \n    data = mydata)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.19621 -0.05444 -0.01202  0.07116  0.21295 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.461e-01  1.769e-01  -1.957   0.0626 .  \nlogarea      2.232e-01  3.842e-02   5.810 6.40e-06 ***\nthtden      -2.570e-02  9.364e-03  -2.744   0.0116 *  \nswamp        2.956e-02  5.510e-03   5.365 1.89e-05 ***\nI(swamp^2)  -2.491e-04  4.409e-05  -5.649 9.46e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1063 on 23 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8129,    Adjusted R-squared:  0.7804 \nF-statistic: 24.98 on 4 and 23 DF,  p-value: 4.405e-08\n\n\n\n\n\nEst-ce qu’il y a possiblement un problème de multicolinéarité?\n\nvif(model.poly2)\n\n   logarea     thtden      swamp I(swamp^2) \n  1.053193   1.123491  45.845845  45.656453 \n\n\nLes valeurs d’inflation de la variance (VIF) pour les deux termes de swamp sont beaucoup plus élevés que le seuil de 5. Cependant, c’est la norme pour les termes polynomiaux et on ne doit pas s’en préoccuper outre mesure, surtout quand les deux termes sont hautement significatifs dans le modèle. Les fortes valeurs de VIF indiquent que les coefficients pour ces deux termes ne sont pas estimés précisément, mais leur utilisation dans le modèle permet tout de même de faire de bonnes prédictions (i.e. ils décrivent la réponse à swamp).",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#vérifier-les-conditions-dapplication-de-modèles-de-régression-multiple",
    "href": "35-reg_mult.html#vérifier-les-conditions-dapplication-de-modèles-de-régression-multiple",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.7 Vérifier les conditions d’application de modèles de régression multiple",
    "text": "8.7 Vérifier les conditions d’application de modèles de régression multiple\nToutes les techniques de sélection des modèles présument que les conditions d’applications (indépendance, normalité, homoscédasticité, linéarité) sont remplies. Comme il y a un grand nombre de modèles qui peuvent être ajustés, il peut paraître quasi impossible de vérifier si les conditions sont remplies à chaque étape de construction. Cependant, il est souvent suffisant d’examiner les résidus du modèle complet (saturé) puis du modèle final. Les termes qui ne contribuent pas significativement à l’ajustement n’affectent pas beaucoup les résidus et donc les résidus du modèle final sont généralement similaires à ceux du modèle complet.\nExaminons donc les graphiques diagnostiques du modèle final:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2, 2))\nplot(model.poly2)\n\n\n\nConditions d’application du modèle model.poly2\n\n\n\n\n\n\nTout semble acceptable dans ce cas. Pour convaincre les sceptiques, on peut faire les tests formels des conditions d’application:\n\nshapiro.test(residuals(model.poly2))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(model.poly2)\nW = 0.9837, p-value = 0.9278\n\n\nLes résidus ne dévient pas significativement de la normalité. Bien.\n\nlibrary(lmtest)\nbptest(model.poly2)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model.poly2\nBP = 3.8415, df = 4, p-value = 0.4279\n\n\nPas de déviation d’homoscédasticité non plus. Bien.\n\ndwtest(model.poly2)\n\n\n    Durbin-Watson test\n\ndata:  model.poly2\nDW = 1.725, p-value = 0.2095\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nPas de corrélation sérielle des résidus, donc pas d’évidence de nonindépendance.\n\nresettest(model.poly2, type = \"regressor\", data = mydata)\n\n\n    RESET test\n\ndata:  model.poly2\nRESET = 0.9823, df1 = 8, df2 = 15, p-value = 0.4859\n\n\nEt finalement, pas de déviation significative de la linéarité. Donc tout semble acceptable.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#visualiser-la-taille-deffet",
    "href": "35-reg_mult.html#visualiser-la-taille-deffet",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.8 Visualiser la taille d’effet",
    "text": "8.8 Visualiser la taille d’effet\nLes coefficients de la régression multiple peuvent mesurer la taille d’effet, quoiqu’il puisse être nécessaire de les standardiser pour qu’ils ne soient pas influencés par les unités de mesure. Mais un graphique est souvent plus informatif. Dans ce contexte, les graphiques des résidus partiels (appelés components+residual plots dans R) sont particulièrement utiles. Ces graphique illustrent comment la variable dépendante, corrigée pour l’effet des autres variables dans le modèle, varie avec chacune des variables indépendantes du modèle. Voyons voir:\n\n# Evaluate visually linearity and effect size\n# component + residual plot\ncrPlots(model.poly2)\n\n\n\nGraphiques de résidus partiels du modèle model.poly2\n\n\n\nNotez que l’échelle de l’axe des y varie sur chaque graphique. Pour thtden, la variable dépendante (log10(richesse des herptiles)) varie d’environ 0.4 unités entre la valeur minimum et maximum de thtden. Pour logarea, la variation est d’environ 0.6 unité log. Pour swamp, l’interprétation est plus compliquée parce qu’il y a deux termes qui quantifient son effet, et que ces termes ont des signes opposés (positif pour swamp et négatif pour swamp^2) ce qui donne une relation curvilinéaire de type parabole. Le graphique ne permet pas de bien visualiser cela. Ceci dit, ces graphique n’indiquent pas vraiment de violation de linéarité.\nPour illustrer ce qui serait visible sur ces graphiques si il y avait une déviation de linéarité, enlevons le terme du second degré pour swamp, puis on va refaire ces graphiques et effectuer le test RESET.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.nopoly &lt;- lm(\n  logherp ~ logarea + thtden + swamp,\n  data = mydata\n)\ncrPlots(model.nopoly)\n\n\n\nGraphiques de résidus partiels du modèle model.nopoly\n\n\n\n\n\n\nLa relation non-linéaire avec swamp devient évidente. Et le test RESET détecte bien cette non-linéarité:\n\nresettest(model.nopoly, type = \"regressor\")\n\n\n    RESET test\n\ndata:  model.nopoly\nRESET = 6.7588, df1 = 6, df2 = 18, p-value = 0.0007066",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#tester-la-présence-dinteractions",
    "href": "35-reg_mult.html#tester-la-présence-dinteractions",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.9 Tester la présence d’interactions",
    "text": "8.9 Tester la présence d’interactions\nLorsqu’il y a plusieurs variables indépendantes, vous devriez toujours garder à l’esprit la possibilité d’interactions. Dans la majorité des situations de régression multiple cela n’est pas évident parce que l’addition de termes d’interaction augmente la multicolinéarité des termes du modèle, et parce qu’il n’y a souvent pas assez d’observations pour éprouver toutes les interactions ou que les observations ne sont pas suffisamment balancées pour faire des tests puissants pour les interactions. Retournons à notre modèle “final” et voyons ce qui se passe si on essaie d’ajuster un modèle saturé avec toutes les interactions:\n\nfullmodel.withinteractions &lt;- lm(\n  logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2),\n  data = mydata\n)\nsummary(fullmodel.withinteractions)\n\n\nCall:\nlm(formula = logherp ~ logarea * cpfor2 * thtden * swamp * I(swamp^2), \n    data = mydata)\n\nResiduals:\nALL 28 residuals are 0: no residual degrees of freedom!\n\nCoefficients: (4 not defined because of singularities)\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                            -5.948e+03        NaN     NaN      NaN\nlogarea                                 3.293e+03        NaN     NaN      NaN\ncpfor2                                  7.080e+01        NaN     NaN      NaN\nthtden                                  9.223e+02        NaN     NaN      NaN\nswamp                                   1.176e+02        NaN     NaN      NaN\nI(swamp^2)                             -3.517e-01        NaN     NaN      NaN\nlogarea:cpfor2                         -3.771e+01        NaN     NaN      NaN\nlogarea:thtden                         -4.781e+02        NaN     NaN      NaN\ncpfor2:thtden                          -1.115e+01        NaN     NaN      NaN\nlogarea:swamp                          -7.876e+01        NaN     NaN      NaN\ncpfor2:swamp                           -1.401e+00        NaN     NaN      NaN\nthtden:swamp                           -1.920e+01        NaN     NaN      NaN\nlogarea:I(swamp^2)                      5.105e-01        NaN     NaN      NaN\ncpfor2:I(swamp^2)                       3.825e-03        NaN     NaN      NaN\nthtden:I(swamp^2)                       7.826e-02        NaN     NaN      NaN\nswamp:I(swamp^2)                       -2.455e-03        NaN     NaN      NaN\nlogarea:cpfor2:thtden                   5.359e+00        NaN     NaN      NaN\nlogarea:cpfor2:swamp                    8.743e-01        NaN     NaN      NaN\nlogarea:thtden:swamp                    1.080e+01        NaN     NaN      NaN\ncpfor2:thtden:swamp                     2.620e-01        NaN     NaN      NaN\nlogarea:cpfor2:I(swamp^2)              -5.065e-03        NaN     NaN      NaN\nlogarea:thtden:I(swamp^2)              -6.125e-02        NaN     NaN      NaN\ncpfor2:thtden:I(swamp^2)               -1.551e-03        NaN     NaN      NaN\nlogarea:swamp:I(swamp^2)               -4.640e-04        NaN     NaN      NaN\ncpfor2:swamp:I(swamp^2)                 3.352e-05        NaN     NaN      NaN\nthtden:swamp:I(swamp^2)                 2.439e-04        NaN     NaN      NaN\nlogarea:cpfor2:thtden:swamp            -1.235e-01        NaN     NaN      NaN\nlogarea:cpfor2:thtden:I(swamp^2)        7.166e-04        NaN     NaN      NaN\nlogarea:cpfor2:swamp:I(swamp^2)                NA         NA      NA       NA\nlogarea:thtden:swamp:I(swamp^2)                NA         NA      NA       NA\ncpfor2:thtden:swamp:I(swamp^2)                 NA         NA      NA       NA\nlogarea:cpfor2:thtden:swamp:I(swamp^2)         NA         NA      NA       NA\n\nResidual standard error: NaN on 0 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 27 and 0 DF,  p-value: NA\n\n\nNotez les coefficients manquants aux dernières lignes: on ne peut inclure les 32 termes si on a seulement 28 observations. Il manque des observations, le R carré est 1, et le modèle “prédit” parfaitement les données.\nSi on essaie une méthode automatique pour identifier le “meilleur” modèle dans ce gâchis, R refuse:\n\nstep(fullmodel.withinteractions)\n\nError in step(fullmodel.withinteractions): AIC is -infinity for this model, so 'step' cannot proceed\n\n\nBon, est-ce qu’on oublie tout ça et qu’on accepte le modèle final sans ce soucier des interactions? Non, pas encore. Il y a un compromis possible: comparer notre modèle “final” à un modèle qui contient au moins un sous-ensemble des interactions, par exemple toutes les interactions du second degré, pour éprouver si l’addition de ces interactions améliore beaucoup l’ajustement du modèle.\n\nfull.model.2ndinteractions &lt;- lm(\n  logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2)\n    + logarea:cpfor2\n    + logarea:thtden\n    + logarea:swamp\n    + cpfor2:thtden\n    + cpfor2:swamp\n    + thtden:swamp,\n  data = mydata\n)\nsummary(full.model.2ndinteractions)\n\n\nCall:\nlm(formula = logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + \n    logarea:cpfor2 + logarea:thtden + logarea:swamp + cpfor2:thtden + \n    cpfor2:swamp + thtden:swamp, data = mydata)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.216880 -0.036534  0.003506  0.042990  0.175490 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     4.339e-01  6.325e-01   0.686 0.502581    \nlogarea        -1.254e-01  2.684e-01  -0.467 0.646654    \ncpfor2         -9.344e-03  7.205e-03  -1.297 0.213032    \nthtden         -1.833e-01  9.035e-02  -2.028 0.059504 .  \nswamp           3.569e-02  7.861e-03   4.540 0.000334 ***\nI(swamp^2)     -3.090e-04  7.109e-05  -4.347 0.000500 ***\nlogarea:cpfor2  2.582e-03  2.577e-03   1.002 0.331132    \nlogarea:thtden  7.017e-02  3.359e-02   2.089 0.053036 .  \nlogarea:swamp  -5.290e-04  2.249e-03  -0.235 0.816981    \ncpfor2:thtden  -2.095e-04  6.120e-04  -0.342 0.736544    \ncpfor2:swamp    4.651e-05  5.431e-05   0.856 0.404390    \nthtden:swamp    2.248e-04  4.764e-04   0.472 0.643336    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.108 on 16 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.8658,    Adjusted R-squared:  0.7735 \nF-statistic: 9.382 on 11 and 16 DF,  p-value: 4.829e-05\n\n\nCe modèle s’ajuste un peu mieux aux données que les modèle “final” (il explique 86.6% de la variance de logherp, comparé à 81.2% pour le modèle “final” sans interactions), mais il compte deux fois plus de paramètres. De plus, si vous examinez les coefficients, il se passe d’étranges choses: le signe pour logare a changé par exemple. C’est un des symptômes de la multicolinéarité. Allons voir les facteurs d’inflation de la variance:\n\nvif(full.model.2ndinteractions)\n\nthere are higher-order terms (interactions) in this model\nconsider setting type = 'predictor'; see ?vif\n\n\n       logarea         cpfor2         thtden          swamp     I(swamp^2) \n      49.86060       78.49622      101.42437       90.47389      115.08457 \nlogarea:cpfor2 logarea:thtden  logarea:swamp  cpfor2:thtden   cpfor2:swamp \n      66.97792       71.69894       67.27034       14.66814       29.41422 \n  thtden:swamp \n      20.04410 \n\n\nAie! tous les VIF sont plus grands que 5, pas seulement les termes incluant swamp. Cette forte multicolinéarité empêche de quantifier avec précision l’effet de ces interactions. De plus, ce modèle avec interactions n’est pas plus informatif que le modèle “final” puisque son AIC est plus élevé (souvenez-vous qu’on privilégie le modèle avec la valeur d’AIC la plus basse):\n\nAIC(model.poly1)\n\n[1] -38.3433\n\nAIC(full.model.2ndinteractions)\n\n[1] -34.86123\n\n\nOn peut également utiliser la fonction anova() pour comparer l’ajustement des deux modèles et vérifier si l’addition des termes d’intération améliore significativement l’ajustement:\n\nanova(model.poly1, full.model.2ndinteractions)\n\nAnalysis of Variance Table\n\nModel 1: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2)\nModel 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + \n    logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + \n    thtden:swamp\n  Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)\n1     22 0.25282                           \n2     16 0.18651  6  0.066314 0.9481  0.489\n\n\nIci, l’addition des termes d’interaction ne réduit pas significativement la variabilité résiduelle du modèle “complet”. Qu’en est-il de la si on compare le modèle avec interaction et notre modèle “final”?\n\nanova(model.poly2, full.model.2ndinteractions)\n\nAnalysis of Variance Table\n\nModel 1: logherp ~ logarea + thtden + swamp + I(swamp^2)\nModel 2: logherp ~ logarea + cpfor2 + thtden + swamp + I(swamp^2) + logarea:cpfor2 + \n    logarea:thtden + logarea:swamp + cpfor2:thtden + cpfor2:swamp + \n    thtden:swamp\n  Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)\n1     23 0.25999                           \n2     16 0.18651  7  0.073486 0.9006 0.5294\n\n\nLe test indique que ces deux modèles ont des variances résiduelles comparables, et donc que l’addition des termes d’interaction et de cpfor2 au modèle final n’apporte pas grand chose.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#recherche-du-meilleur-modèle-fondée-sur-la-théorie-de-linformation",
    "href": "35-reg_mult.html#recherche-du-meilleur-modèle-fondée-sur-la-théorie-de-linformation",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.10 Recherche du meilleur modèle fondée sur la théorie de l’information",
    "text": "8.10 Recherche du meilleur modèle fondée sur la théorie de l’information\nUne des principales critiques des méthodes pas-à-pas (stepwise) est que les valeurs de p ne sont pas strictement interprétables à cause du grand nombre de tests qui sont implicites dans le processus. C’est le problème des comparaisons ou tests multiples: en construisant un modèle linéaire (comme une régression multiple) à partir d’un grand nombre de variables et de leurs interactions, il y a tellement de combinaisons possibles qu’un ajustement de Bonferroni rendrait les tests trop conservateurs.\nUne alternative, défendue par Burnham et Anderson (2002, Model selection and multimodel inference: a practical information-theoretic approach. 2nd ed), est d’utiliser l’AIC (ou mieux encore AICc qui est plus approprié quand le nombre d’observations est inférieur à 40 fois le nombre de variables indépendantes) pour ordonner les modèles et identifier un sousensemble de modèles qui sont les meilleurs. On peut ensuite calculer les moyennes des coefficients pondérées par la probabilité que chacun des modèles soit le meilleur pour obtenir des coefficients qui sont plus robustes et moins sensibles à la multicolinéarité.\nL’approche de comparaison par AIC a d’abord été développé pour comparer un ensemble de modèle préalablement défini basé sur les connaissance du sytème et les hypothèses biologiques. Cependant, certains ont développé une approche plutôt brutale et sans scrupule de faire tous les modèles possibles et de les comparer par AIC. Cette approche a été suivie dans le package MuMIn. Les comparaisons de modèle par AICdoivent être faites en utilisant exactement le même jeu de données pour chaque modèle. Il faut donc s’arrurer d’enlever les données manquantes et de spécifier dans la fonction lm de ne pas marcher s’il y a des données manquantes.\n\n\n\n\n\n\nNote\n\n\n\nJe ne supporte pas l’approche stepwise ni l’approche par AIC. Je déteste l’approche par la fonction dredge() qui selon moi va à l’encontre de la philosophie des AIC et de la parsimonie. Je soutiens de dévelooper un modèle basé sur des hypothèses biologiques et de reporter ce modèle avec tous les effets significatifs ou non.\n\n\n\n# refaire le modèle en s'assurant qu'il n'y a pas de \"NA\" et en spécificant na.action\nfull.model.2ndinteractions &lt;- update(\n  full.model.2ndinteractions,\n  . ~ .,\n  data = mysub,\n  na.action = \"na.fail\"\n)\n\nlibrary(MuMIn)\ndd &lt;- dredge(full.model.2ndinteractions)\n\nFixed term is \"(Intercept)\"\n\n\nL’objet dd contient tous les modèles possibles (i.e. ceux qui ont toutes les combinaisons possibles) en utilisant les termes du modèle full.model.2ndinteractions ajusté précédemment. On peut ensuite extraire de l’objet dd le sous-ensemble de modèles qui ont un AICc semblable au meilleur modèle (Burnham et Anderson suggèrent que les modèles qui dévient par plus de 7 unités d’AICc du meilleur modèle ont peu de support empirique).\n\n# get models within 2 units of AICc from the best model\ntop.models.1 &lt;- get.models(dd, subset = delta &lt; 2)\navgmodel1 &lt;- model.avg(top.models.1) # compute average parameters\nsummary(avgmodel1) # display averaged model\n\n\nCall:\nmodel.avg(object = top.models.1)\n\nComponent model call: \nlm(formula = logherp ~ &lt;2 unique rhs&gt;, data = mysub, na.action = \n     na.fail)\n\nComponent models: \n      df logLik   AICc delta weight\n12345  7  27.78 -35.95  0.00   0.55\n1234   6  25.78 -35.56  0.39   0.45\n\nTerm codes: \n    I(swamp^2)        logarea          swamp         thtden logarea:thtden \n             1              2              3              4              5 \n\nModel-averaged coefficients:  \n(full average) \n                 Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)    -2.145e-01  2.308e-01   2.406e-01   0.891    0.373    \nlogarea         1.356e-01  1.089e-01   1.119e-01   1.213    0.225    \nswamp           3.180e-02  5.971e-03   6.273e-03   5.070    4e-07 ***\nI(swamp^2)     -2.669e-04  4.770e-05   5.011e-05   5.326    1e-07 ***\nthtden         -6.985e-02  5.233e-02   5.361e-02   1.303    0.193    \nlogarea:thtden  2.131e-02  2.487e-02   2.545e-02   0.837    0.403    \n \n(conditional average) \n                 Estimate Std. Error Adjusted SE z value Pr(&gt;|z|)    \n(Intercept)    -2.145e-01  2.308e-01   2.406e-01   0.891   0.3727    \nlogarea         1.356e-01  1.089e-01   1.119e-01   1.213   0.2253    \nswamp           3.180e-02  5.971e-03   6.273e-03   5.070    4e-07 ***\nI(swamp^2)     -2.669e-04  4.770e-05   5.011e-05   5.326    1e-07 ***\nthtden         -6.985e-02  5.233e-02   5.361e-02   1.303   0.1927    \nlogarea:thtden  3.882e-02  2.114e-02   2.237e-02   1.735   0.0827 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nconfint(avgmodel1) # display CI for averaged coefficients\n\n                       2.5 %       97.5 %\n(Intercept)    -0.6860022996  0.257064603\nlogarea        -0.0836067896  0.354883299\nswamp           0.0195105703  0.044099316\nI(swamp^2)     -0.0003650809 -0.000168656\nthtden         -0.1749296690  0.035236794\nlogarea:thtden -0.0050266778  0.082666701\n\n\n\nLa liste des modèles qui sont à 4 unités ou moins de l’AICc du meilleur modèle. Les variables dans chaque modèle sont codées et on retrouve la clé en dessous du tableau.\nPour chaque modèle, en plus de l’AICc, le poids Akaike est calculé. C’est un estimé de la probabilité que ce modèle est le meilleur. Ici on voit que le premier modèle (le meilleur) a seulement 34% des chance d’être vraiment le meilleur.\nÀ partir de ce sous-ensemble de modèles, la moyenne pondérée des coefficients (en utilisant les poids Akaike) est calculée, avec in IC à 95%. Notez que les termes absents d’un modèle sont considérés avoir un coefficient de 0 pour ce terme.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#bootstrap-et-régression-multiple",
    "href": "35-reg_mult.html#bootstrap-et-régression-multiple",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.11 Bootstrap et régression multiple",
    "text": "8.11 Bootstrap et régression multiple\nQuand les données ne rencontrent pas les conditions d’application de normalité et d’homoscédasticité et que les transformations n’arrivent pas à corriger ces violations, le bootstrap peut être utilisé pour calculer des intervalles de confiance pour les coefficients. Si la distribution des coefficients bootstrappés est symétrique et approximativement normale, on peut utiliser les percentiles empiriques pour calculer les limites de confiance.\nLe code qui suit, utilisant le package simpleboot, a été conçu pour être facilement modifiable et calcule les limites des IC à partir des percentiles empiriques.\n\n############################################################\n#######\n# Bootstrap analysis the simple way with library simpleboot\n# Define model to be bootstrapped and the data source used\nmymodel &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2), data = mydata)\n# Set the number of bootstrap iterations\nnboot &lt;- 1000\nlibrary(simpleboot)\n# R is the number of bootstrap iterations\n# Setting rows to FALSE indicates resampling of residuals\nmysimpleboot &lt;- lm.boot(mymodel, R = nboot, rows = FALSE)\n# Extract bootstrap coefficients\nmyresults &lt;- sapply(mysimpleboot$boot.list, function(x) x$coef)\n# Transpose matrix so that lines are bootstrap iterations and columns are coefficients\ntmyresults &lt;- t(myresults)\n\nVous pouvez ensuite faire des graphiques pour voir les résultats. Lorsque vous tournerez ce code, il y aura une pause pour vous permettre d’examiner la distribution pour chaque coefficient du modèle sur des graphiques:\n\n# Plot histograms of bootstrapped coefficients\nncoefs &lt;- length(data.frame(tmyresults))\npar(mfrow = c(1, 2), mai = c(0.5, 0.5, 0.5, 0.5), ask = TRUE)\nfor (i in 1:ncoefs) {\n  lab &lt;- colnames(tmyresults)[i]\n  x &lt;- tmyresults[, i]\n  plot(density(x), main = lab, xlab = \"\")\n  abline(v = mymodel$coef[i], col = \"red\")\n  abline(v = quantile(x, c(0.025, 0.975)))\n  hist(x, main = lab, xlab = \"\")\n  abline(v = quantile(x, c(0.025, 0.975)))\n  abline(v = mymodel$coef[i], col = \"red\")\n}\n\n\n\n\n\nDistribution des estimé par bootstrap pour logarea\n\n\n\nLe graphique de droite illustre la densité lissée (kernel density) et celui de gauche est l’histogramme des estimés bootstrap du coefficient. La ligne rouge sur le graphique indique la valeur du coefficient ordinaire (pas bootstrap) et les deux lignes verticales noires marquent les limites de l’intervalle de confiance à 95%. Ici l’IC ne contient pas 0, et donc on peut conclure que l’effet de logarea sur logherp est significativement positif.\nLes limites précises peuvent être obtenues par:\n\n# Display empirical bootstrap quantiles (not corrected for bias)\np &lt;- c(0.005, 0.01, 0.025, 0.05, 0.95, 0.975, 0.99, 0.995)\napply(tmyresults, 2, quantile, p)\n\n      (Intercept)   logarea       thtden      swamp    I(swamp^2)\n0.5%  -0.76307584 0.1291407 -0.047584473 0.01772250 -0.0003497203\n1%    -0.72831577 0.1409443 -0.046010909 0.01863688 -0.0003424577\n2.5%  -0.66810810 0.1575280 -0.042287583 0.02058147 -0.0003247691\n5%    -0.60753598 0.1681706 -0.039566732 0.02145910 -0.0003126071\n95%   -0.09290077 0.2808696 -0.011264473 0.03773842 -0.0001842991\n97.5% -0.03427477 0.2943330 -0.008526632 0.03888998 -0.0001770746\n99%   -0.01146876 0.3031307 -0.005051748 0.04089202 -0.0001621921\n99.5%  0.01210649 0.3091569 -0.002046873 0.04175795 -0.0001581856\n\n\nCes intervalles de confiances ne sont pas fiables si la distribution des estimés bootstrap n’est pas Gaussienne. Dans ce cas, il vaut mieux calculer des coefficients non-biaisés (bias-corrected accelerated confidence limits, BCa):\n\n################################################\n# Bootstrap analysis in multiple regression with BCa confidence intervals\n# Preferable when parameter distribution is far from normal\n# Bootstrap 95% BCa CI for regression coefficients\n\nlibrary(boot)\n# function to obtain regression coefficients for each iteration\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ] # allows boot to select sample\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n# bootstrapping with 1000 replications\nresults &lt;- boot(\n  data = mydata, statistic = bs, R = 1000,\n  formula = logherp ~ logarea + thtden + swamp + I(swamp^2)\n)\n# view results\n\nPour obtenir les résultats, le code suivant va produire le graphique standard pour chaque coefficient, et les estimés BCa pour l’intervalle de confiance\n\nplot(results, index = 1) # intercept\nplot(results, index = 2) # logarea\nplot(results, index = 3) # thtden\nplot(results, index = 4) # swamp\nplot(results, index = 5) # swamp^2\n\n# get 95% confidence intervals\nboot.ci(results, type = \"bca\", index = 1)\nboot.ci(results, type = \"bca\", index = 2)\nboot.ci(results, type = \"bca\", index = 3)\nboot.ci(results, type = \"bca\", index = 4)\nboot.ci(results, type = \"bca\", index = 5)\n\nPour logarea, cela donne:\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = results, type = \"bca\", index = 2)\n\nIntervals : \nLevel       BCa          \n95%   ( 0.1118,  0.3206 )  \nCalculations and Intervals on Original Scale\n\n\n\n\n\n\n\n\nNotez que l’intervalle BCa va de 0.12 à 0.32, alors que l’intervalle standard était de 0.16 à 0.29. L’intervalle BCa est ici plus grand du côté inférieur et plus petit du côté supérieur comme il se doit compte tenu de la distribution non-Gaussienne et asymétrique des estimés bootstrap.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "35-reg_mult.html#perm_reg_mult",
    "href": "35-reg_mult.html#perm_reg_mult",
    "title": "\n8  Régression multiple\n",
    "section": "\n8.12 Test de permutation",
    "text": "8.12 Test de permutation\nLes tests de permutations sont plus rarement effectués en régression multiple que le bootstrap. Voici un fragment de code pour le faire tout de même.\n\n############################################################\n##########\n# Permutation in multiple regression\n#\n# using lmperm library\nlibrary(lmPerm)\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(logherp ~ logarea + thtden + swamp + I(swamp^2),\n  data = mydata\n)\nmymodelProb &lt;- lmp(\n  logherp ~ logarea + thtden + swamp + I(swamp^2),\n  data = mydata, perm = \"Prob\"\n)\nsummary(mymodel)\nsummary(mymodelProb)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Régression multiple</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html",
    "href": "36-ancova_glm.html",
    "title": "\n9  ANCOVA et modèle linéaire général\n",
    "section": "",
    "text": "9.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#set-anco",
    "href": "36-ancova_glm.html#set-anco",
    "title": "\n9  ANCOVA et modèle linéaire général\n",
    "section": "",
    "text": "les paquets R:\n\nggplot2\ncar\nlmtest\n\n\nles fichiers de données\n\nanc1dat.csv\nanc3dat.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#modèle-linéaire-général",
    "href": "36-ancova_glm.html#modèle-linéaire-général",
    "title": "\n9  ANCOVA et modèle linéaire général\n",
    "section": "\n9.2 Modèle linéaire général",
    "text": "9.2 Modèle linéaire général\nLes modèles linéaires généraux ou General Linear Model en anglais sont différent des modèles linéaires généralisés (ou generalized linear model, GLM). Les modèles linéaires généraux sont des modèles statistiques de la forme \\(Y = B \\mathbf{X} + E\\), ou Y est un vecteur contenant la variable dépendante continue, B est un vecteur des paramètres estimés, \\(\\mathbf{X}\\) et la matrice des différents variables indépendantes et E est un vecteur de résidus homoscédastiques et normalement distribués. Tous les tests que nous avons étudiés à date (test de t, régression linéaire simple, ANOVA à un facteur de classification, ANOVA à plusieurs facteurs de classification et régression multiple) sont formulés ainsi. Notez que tous les modèles que nous avons rencontrés à ce jour ne contiennent qu’un type de variable indépendante (soit continue ou discontinue). Dans cet exercice de laboratoire, vous allez ajuster des modèles qui ont les deux types de variables indépendantes.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#ancova",
    "href": "36-ancova_glm.html#ancova",
    "title": "\n9  ANCOVA et modèle linéaire général\n",
    "section": "\n9.3 ANCOVA",
    "text": "9.3 ANCOVA\nANCOVA est l’abréviation pour l’analyse de covariance. C’est un type de modèle linéaire général dans lequel il y a une (ou plusieurs) variable indépendante continue (parfois appelé la covariable) et une (ou plusieurs) variable indépendante discontinue. Dans la présentation traditionnelle de l’ANCOVA dans les manuels de biostatistique, le modèle ANCOVA ne contient pas de termes d’interaction entre les variables continues et discontinues. Par conséquent, on doit précéder l’ajustement de ce modèle (réduit parce que sans terme d’interaction), par un test de signification de l’interaction qui correspond à éprouver l’égalité des pentes (coefficients pour la ou les variables continues) entre les différents niveaux de la ou les variables discontinues (i.e un test d’homogénéité des pentes).",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#homogénéité-des-pentes",
    "href": "36-ancova_glm.html#homogénéité-des-pentes",
    "title": "\n9  ANCOVA et modèle linéaire général\n",
    "section": "\n9.4 Homogénéité des pentes",
    "text": "9.4 Homogénéité des pentes\nPour répondre à de nombreuses questions biologiques, il est nécessaire de déterminer si deux (ou plus de deux) régressions diffèrent significativement. Par exemple, pour comparer l’efficacité de deux insecticides on doit comparer la relation entre leur dose et la mortalité. Ou encore, pour comparer le taux de croissance des mâles et des femelles on doit comparer la relation entre la taille et l’âge des mâles et des femelles.\nComme chaque régression linéaire est décrite par deux paramètres, la pente et l’ordonnée à l’origine, on doit considérer les deux dans la comparaison. Le modèle d’ANCOVA, à strictement parler, n’éprouve que l’hypothèse d’égalité des ordonnées à l’origine. Cependant, avant d’ajuster ce modèle, il faut éprouver l’hypothèse d’égalité des pentes (homogénéité des pentes).\n\n9.4.1 Cas 1 - La taille en fonction de l’âge (exemple avec pente commune)\n\n\n\n\n\n\nExercice\n\n\n\nEn utilisant les données du fichier anc1dat.csv, éprouvez l’hypothèse que le taux de croissance des esturgeons mâles et femelles de The Pas est le même (données de 1978-1980). Comme mesure du taux de croissance, nous allons utiliser la pente de la régression du log 10 de la longueur à la fourche, lfkl, sur le log 10 de l’âge, l’age.\n\n\nCommençons par examiner les données. Pour faciliter la comparaison, il serait utile de tracer la droite de régression et la trace lowess pour ainsi plus facilement évaluer la linéarité. On peut aussi ajouter un peu de trucs R pour obtenir des légendes plus complètes (remarquez l’utilisation de la commande expression() pour obtenir des indices):\n\nanc1dat &lt;- read.csv(\"data/anc1dat.csv\")\nanc1dat$sex &lt;- as.factor(anc1dat$sex)\nmyplot &lt;- ggplot(data = anc1dat, aes(x=lage,    y=log10(fklngth)))+facet_grid(.~sex)+geom_point()\nmyplot &lt;- myplot+\n  stat_smooth(method = lm, se=FALSE)+\n  stat_smooth(se=FALSE, color=\"red\") +\n  labs(\n    y = expression(log[10]~(Fork~length)),\n    x = expression(log[10]~(Age))\n)\nmyplot\n\n\n\nLongueur des esturgeons en fonction de l’age\n\n\n\nLa transformation log-log rend la relation linéaire et, à première vue, il ne semble pas y avoir de problème évident avec les conditions d’application. Ajustons donc le modèle complet avec l’interaction:\n\nmodel.full1&lt;-lm(lfkl ~ sex + lage + sex:lage, data = anc1dat)\nAnova(model.full1, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 0.64444  1 794.8182 &lt; 2.2e-16 ***\nsex         0.00041  1   0.5043    0.4795    \nlage        0.07259  1  89.5312 4.588e-15 ***\nsex:lage    0.00027  1   0.3367    0.5632    \nResiduals   0.07135 88                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nProbabilité que le terme lage*sex n’affecte pas la longueur à la fourche (i.e. que la pente ne diffère pas entre les sexes, et que la différence de taille entre les mâles et femelles ne varie pas avec l’âge)\nAttrape. Notez que j’ai utilisé la fonction Anova() du package car avec un “a” majuscule au lieu de la fonction native anova() (avec un “a” minuscule”) associée aux objets produits par lm() pour obtenir les sommes de carrés de type III. Ces sommes des carrés des écarts de type III (partiels) sont calculées comme si la variable était entrée la dernière dans le modèle et correspondent à la différence entre la variance expliquée par le modèle complet et par le modèle dans lequel seule cette variable est omise. La fonction native anova() donne les sommes des carrés séquentielles, calculées au fur et à mesure que chaque variable est ajoutée au modèle nul avec seulement une ordonnée à l’origine. Dans de rares cas, les sommes des carrés de type I et III sont égales (quand le design est parfaitement orthogonal ou balancé). Dans la vaste majorité des cas, les sommes des carrés de type I et III sont différentes et je vous conseille de toujours utiliser les sommes des carrés de type III dans vos analyses.\nÀ partir de cette analyse, on devrait accepter les hypothèses nulles (1) d’égalité des pentes pour les deux sexes, et (2) que les ordonnées à l’origine sont les mêmes pour les deux sexes. Mais, avant d’accepter ces conclusions, il faut vérifier si les données rencontrent les conditions d’application, comme d’habitude…\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2,2))\nplot(model.full1)\n\n\n\nConditions d’applications du modèle model.full1\n\n\n\n\n\n\nEn ce qui concerne la normalité, ça a l’air d’aller quoiqu’il y a quelques points, en haut à droite, qui dévient de la droite. Si on effectue le test de Wilk-Shapiro (W = .9764, p = 0.09329), on confirme que les résidus ne dévient pas significativement de la normalité. Les résidus semblent homoscédastiques, mais si vous voulez vous en assurer, vous pouvez l’éprouver par un des tests formels. Ici j’utilise le test Breusch-Pagan, qui est approprié quand certaines des variables indépendantes sont continues (Le test de Levene n’est approprié que lorsqu’il n’y a que des variables discontinues).\n\nbptest(model.full1)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model.full1\nBP = 0.99979, df = 3, p-value = 0.8013\n\n\nComme l’hypothèse nulle de ce test est que les résidus sont homoscédastiques, et que p est relativement élevé, le test confirme l’évaluation visuelle. De plus, il n’y a pas de tendance évidente dans les résidus, suggérant qu’il n’y a pas de problème de linéarité. Ce qui peut également être éprouvé formellement:\n\nresettest(model.full1, power = 2:3, type = \"regressor\", data = anc1dat)\n\n\n    RESET test\n\ndata:  model.full1\nRESET = 0.59861, df1 = 2, df2 = 86, p-value = 0.5519\n\n\nLa dernière condition d’application est qu’il n’y a pas d’erreur de mesure sur la variable indépendante continue. On ne peut vraiment éprouver cette condition,, mais on sait que des estimés indépendants de l’âge des poissons obtenus par différents chercheurs donnent des âges qui concordent avec moins de 1-2 ans d’écart., ce qui est inférieur au 10% de la fourchette observée des âges et donc acceptable pour des analyses de modèles de type I (attention ici on ne parle pas des SC de type I, je sais, c’est facile de confondre…)\nVous noterez qu’il y a une observation qui a un résidu normalisé (studentized residual) qui est élevé, i.e. une valeur extrême (cas numéro 49). Éliminez-la de l’ensemble de données et refaites l’analyse. Vos conclusions changent-elles?\n\nmodel.full.no49&lt;-lm(lfkl ~ sex + lage + sex:lage, data = anc1dat[c(-49),])\nAnova(model.full.no49, type=3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value Pr(&gt;F)    \n(Intercept) 0.64255  1 895.9394 &lt;2e-16 ***\nsex         0.00038  1   0.5273 0.4697    \nlage        0.07378  1 102.8746 &lt;2e-16 ***\nsex:lage    0.00022  1   0.3135 0.5770    \nResiduals   0.06239 87                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLa conclusion ne change pas après avoir enlevé la valeur extrême. Comme on n’a pas de bonne raison d’éliminer cette valeur, il est probablement mieux de la conserver. Un examen des conditions d’application après avoir enlevé cette valeur révèle qu’elles sont toutes rencontrées.\n\n9.4.2 Cas 2 - Taille en fonction de l’âge (exemple avec des pentes différentes)\n\n\n\n\n\n\nExercice\n\n\n\nLe fichier anc3dat.csv contient des données sur des esturgeons mâles de deux sites (locate) : Lake of the Woods dans le Nord-Ouest de l’Ontario et Chruchill River dans le Nord du Manitoba. En utilisant la même procédure, éprouvez l’hypothèse que la pente de la régression de lfkl sur lage est la même aux deux sites (alors Locate est la variable en catégories et non pas sex). Que concluez-vous?\n\n\n\nanc3dat &lt;- read.csv(\"data/anc3dat.csv\")\nmyplot &lt;- ggplot(data = anc3dat, aes(x=lage, y = log10(fklngth))) +\n  facet_grid(.~locate) +\n  geom_point() +\n  stat_smooth(method = lm, se=FALSE)+\n  stat_smooth(se=FALSE, color=\"red\")+\n  labs(\n    y = expression(log[10]~(Fork~length)),\n    x = expression(log[10]~(Age))\n)\nmyplot\n\n\n\nLongueur des esturgeons en fonction de l’age d’après anc3dat\n\n\nmodel.full2&lt;-lm(lfkl ~ lage + locate + lage:locate, data = anc3dat)\nAnova(model.full2, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 0.62951  1 1078.632 &lt; 2.2e-16 ***\nlage        0.07773  1  133.185 &lt; 2.2e-16 ***\nlocate      0.00968  1   16.591 0.0001012 ***\nlage:locate 0.00909  1   15.575 0.0001592 ***\nResiduals   0.05136 88                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIci, on rejette les hypothèses nulles (1) que les pentes sont les mêmes dans les deux sites et (2) que les ordonnées à l’origine sont égales. En d’autres mots, si on veut prédire la longueur à la fourche d’un esturgeon à un âge donné précisément, il faut savoir de quel site il provient. Puisque les pentes diffèrent, il faut estimer des régressions séparées.\nMais avant d’accepter ces conclusions, on doit se convaincre que les conditions d’application sont rencontrées:\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npar(mfrow = c(2,2))\nplot(model.full2)\n\n\n\nConditions d’applications du modèle model.full2\n\n\n\n\n\n\nSi on examine les résidus selon les méthodes habituelles, on voit qu’il n’y a pas de problème de linéarité, ni d’homoscédasticité (BP = 2.8721, p = 0.4118). Cependant, le test de Wilk-Shapiro est significatif (W=0.97, p = 0.03). Étant donné la taille assez grande de l’échantillon (N=92), ce test a beaucoup de puissance, même si la déviation de normalité ne semble pas très grande. Compte-tenu de la robustesse relative des LM, de la taille de l’échantillon, on ne devrait pas ^tre trop inquiet de cette déviation de normalité.\nDonc, comme les conditions des LM sont suffisamment remplies, on peut accepter les résultats donnés par R. Tous les termes sont significatifs (location, lage, interaction). Ce modèle complet est équivalent à ajuster des régressions séparées pour chaque site. Pour obtenir les coefficients, on peut ajuster des régressions simples sur chaque sous-ensemble, ou extraire les coefficients ajustés du modèle complet:\n\nmodel.full2\n\n\nCall:\nlm(formula = lfkl ~ lage + locate + lage:locate, data = anc3dat)\n\nCoefficients:\n            (Intercept)                     lage       locateNELSON        \n                 1.2284                   0.3253                   0.2207  \nlage:locateNELSON        \n                -0.1656  \n\n\nPar défaut, la variable locate est encodée comme 0 pour le site qui vient le premier en ordre alphabétique (LofW) et 1 pour l’autre (Nelson). Les régressions pour chaque site deviennent donc:\nPour LofW: \\[\\begin{aligned}\nlfkl &= 1.2284 + 0.3253 \\times lage + 0.2207 \\times 0 - 0.1656 \\times 0 \\times lage \\\\\n&= 1.2284 + 0.3253 \\times lage\n\\end{aligned}\\]\nPour Nelson: \\[\\begin{aligned}\nlfkl &= 1.2284 + 0.3253 \\times lage + 0.2207 \\times 1 - 0.1656 \\times 1 \\times lage \\\\\n&= 1.4491 + 0.1597 \\times lage\n\\end{aligned}\\]\nVous pouvez vérifier en ajustant séparément les régressions pour chaque site:\n\nby(anc3dat, anc3dat$locate,function(x) lm(lfkl~lage, data=x))\n\nanc3dat$locate: LOFW        \n\nCall:\nlm(formula = lfkl ~ lage, data = x)\n\nCoefficients:\n(Intercept)         lage  \n     1.2284       0.3253  \n\n------------------------------------------------------------ \nanc3dat$locate: NELSON      \n\nCall:\nlm(formula = lfkl ~ lage, data = x)\n\nCoefficients:\n(Intercept)         lage  \n     1.4491       0.1597",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#le-modèle-dancova",
    "href": "36-ancova_glm.html#le-modèle-dancova",
    "title": "\n9  ANCOVA et modèle linéaire général\n",
    "section": "\n9.5 Le modèle d’ANCOVA",
    "text": "9.5 Le modèle d’ANCOVA\nSi le test d’homogénéité des pentes indique qu’elles diffèrent, alors on devrait estimer des régressions individuelles pour chaque niveau des variables discontinues. Cependant, si on accepte l’hypothèse d’égalité des pentes, l’étape suivante est de comparer les ordonnées à l’origine. Selon la “vieille école” i.e. l’approche traditionnelle, on ajuste un modèle avec la variable catégorique et la variable continue, mais sans interaction (le modèle ANCOVA sensus stricto) et on utilise la somme des carrés des écarts de type III, disons avec la fonction Anova(). C’est ce que la majorité des manuels de biostatistiques présentent.\nL’autre approche consiste à utiliser les résultats de l’analyse du modèle complet, et tester la signification de chaque terme à partir des sommes des carrés partiels. C’est plus rapide, mais moins puissant. Dans la plupart des cas, cette perte de puissance n’est pas trop préoccupante, sauf lorsque le modèle est très complexe et contient de nombreuses interactions non-significatives. Je vous suggère d’utiliser l’approche simplifiée, et de n’utiliser l’approche traditionnelle que lorsque vous acceptez l’hypothèse d’égalité des ordonnées à l’origine. Pourquoi?\nPuisque l’approche simplifiée est moins puissante, si vous rejetez quand même H0, alors votre conclusion ne changera pas, mais sera seulement renforcée, en utilisant l’approche traditionnelle.\nIci, je vais comparer l’approche traditionnelle et l’approche simplifiée. Rappelez-vous que vous voulez évaluer l’égalité des ordonnées à l’origine après avoir déterminé que les pentes étaient égales. Éprouver l’égalité des ordonnées à l’origine quand les pentes diffèrent (ou, si vous préférez, quand il y a une interaction) est rarement sensé, peut facilement être mal interprété, et ne devrait être effectué que rarement.\nDe retour aux données de anc1dat.csv, en comparant la relation entre lfkl et lage entre les sexes, nous avions obtenu les résultats suivants pour le modèle complet avec interactions:\n\nAnova(model.full1, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 0.64444  1 794.8182 &lt; 2.2e-16 ***\nsex         0.00041  1   0.5043    0.4795    \nlage        0.07259  1  89.5312 4.588e-15 ***\nsex:lage    0.00027  1   0.3367    0.5632    \nResiduals   0.07135 88                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOn avait déjà conclu que la pente ne varie pas entre les sexes (i.e. l’interaction n’est pas significative). Notez que la p-valeur associée au sexe (0.4795) n’est pas significative non plus.\nDe l’autre côté, selon l’approche traditionnelle, l’inférence quand à l’effet du sexe se fait à partir du modèle réduit (le modèle ANCOVA sensus stricto):\n\nmodel.ancova &lt;- lm(lfkl ~ sex + lage, data = anc1dat)\nAnova(model.ancova, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df   F value Pr(&gt;F)    \n(Intercept) 1.13480  1 1410.1232 &lt;2e-16 ***\nsex         0.00149  1    1.8513 0.1771    \nlage        0.14338  1  178.1627 &lt;2e-16 ***\nResiduals   0.07162 89                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model.ancova)\n\n\nCall:\nlm(formula = lfkl ~ sex + lage, data = anc1dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.093992 -0.018457 -0.000876  0.022491  0.081161 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.225533   0.032636  37.552   &lt;2e-16 ***\nsexMALE         -0.008473   0.006228  -1.361    0.177    \nlage             0.327253   0.024517  13.348   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02837 on 89 degrees of freedom\nMultiple R-squared:  0.696, Adjusted R-squared:  0.6892 \nF-statistic: 101.9 on 2 and 89 DF,  p-value: &lt; 2.2e-16\n\n\nDans ce modèle, sex n’est pas significatif et on conclue donc que l’ordonnée à l’origine ne diffère pas entre les sexes. Notez que la pvaleur est plus petite (0.1771 vs 0.4795), ce qui reflète la puissance accrue de l’approche traditionnelle. Toutefois, les conclusions sont les mêmes: les ordonnées à l’origine ne diffèrent pas.\n\n\n\n\n\n\nExercice\n\n\n\nEn examinant les graphiques diagnostiques, vous noterez qu’il y a trois observations dont la valeur absolue du résidu est grande (cas 19, 49, et 50). Ces observations pourraient avoir un effet disproportionné sur les résultats de l’analyse. Éliminez-les et refaites l’analyse. Les conclusions changent-elles ?\n\n\n\nmodel.ancova.nooutliers &lt;- lm(lfkl ~ sex + lage, data = anc1dat[c(-49, -50, -19),])\nAnova(model.ancova.nooutliers, type = 3)\n\nAnova Table (Type III tests)\n\nResponse: lfkl\n             Sum Sq Df   F value  Pr(&gt;F)    \n(Intercept) 1.09160  1 1896.5204 &lt; 2e-16 ***\nsex         0.00232  1    4.0374 0.04764 *  \nlage        0.13992  1  243.0946 &lt; 2e-16 ***\nResiduals   0.04950 86                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(model.ancova.nooutliers)\n\n\nCall:\nlm(formula = lfkl ~ sex + lage, data = anc1dat[c(-49, -50, -19), \n    ])\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.058397 -0.018469 -0.000976  0.020696  0.040288 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.224000   0.028106  43.549   &lt;2e-16 ***\nsexMALE         -0.010823   0.005386  -2.009   0.0476 *  \nlage             0.328604   0.021076  15.591   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02399 on 86 degrees of freedom\nMultiple R-squared:  0.7706,    Adjusted R-squared:  0.7653 \nF-statistic: 144.4 on 2 and 86 DF,  p-value: &lt; 2.2e-16\n\n\nOuch! Les résultats changent. Il faudrait donc rejeter l’hypothèse nulle et conclure que les ordonnées à l’origine diffèrent! Une conclusion qualitativement différente de celle obtenue en considérant toutes les données. Pourquoi? Il y a deux raisons possibles : (1) les valeurs extrêmes influencent beaucoup les régressions ou (2) l’exclusion des valeurs extrêmes permet d’augmenter la puissance de détection d’une différence. La première explication est moins plausible parce que les valeurs extrêmes n’avaient pas une grande influence (leverage faible). Alors, la deuxième explication est plus plausible et vous pouvez le vérifier en faisant des régressions pour chaque sexe sans et avec les valeurs extrêmes. Si vous le faites, vous noterez que les ordonnées à l’origine pour chaque sexe ne changent presque pas alors que leurs erreurs-types changent beaucoup.\n\n\n\n\n\n\nExercice\n\n\n\nAjustez une régression simple entre lfkl et lage pour l’ensemble complet de données et aussi pour le sous-ensemble sans les 3 valeurs déviantes. Comparez ces modèles avec les modèles d’ANCOVA ajustés précédemment. Que concluez-vous ? Quel modèle, d’après vous, a le meilleur ajustement aux données ? Pourquoi ?\n\n\nLe modèle en excluant les valeurs extrêmes:\n\nmodel.linear.nooutliers&lt;-lm(lfkl ~ lage,data = anc1dat[c(-49, -50, -19),])\nsummary(model.linear.nooutliers)\n\n\nCall:\nlm(formula = lfkl ~ lage, data = anc1dat[c(-49, -50, -19), ])\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.055567 -0.017809 -0.002944  0.021272  0.044972 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.20378    0.02670   45.09   &lt;2e-16 ***\nlage         0.34075    0.02054   16.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02441 on 87 degrees of freedom\nMultiple R-squared:  0.7598,    Adjusted R-squared:  0.7571 \nF-statistic: 275.2 on 1 and 87 DF,  p-value: &lt; 2.2e-16\n\n\nPour la régression simple (sans les valeurs extrêmes) on obtient un R 2 de 0.76 et une erreur-type des résidus de 0.02441, En comparant à l’erreur-type des résidus du modèle d’ANCOVA (0.02399) on réalise que la qualité des prédictions est essentiellement la même, même en ajustant des ordonnées à l’origine différentes pour chaque groupe. Par conséquent, les bénéfices de l’inclusion d’un terme pour les différentes ordonnées à l’origine sont faibles alors que le coût, en terme de complexité du modèle, est élevé (33% d’augmentation du nombre de termes pour un très faible amélioration de la qualité d’ajustement). Si vous examinez les résidus de ce modèle, vous trouverez qu’ils sont à peu près O.K.)\nSi on ajuste une régression simple sur toutes les données, on obtient:\n\nmodel.linear&lt;-lm(lfkl ~ lage, data = anc1dat)\nsummary(model.linear)\n\n\nCall:\nlm(formula = lfkl ~ lage, data = anc1dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.090915 -0.018975 -0.002587  0.021270  0.085273 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.21064    0.03089   39.19   &lt;2e-16 ***\nlage         0.33606    0.02376   14.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0285 on 90 degrees of freedom\nMultiple R-squared:  0.6897,    Adjusted R-squared:  0.6863 \nF-statistic: 200.1 on 1 and 90 DF,  p-value: &lt; 2.2e-16\n\n\nEncore une fois, l’erreur-type des résidus (0.0285) pour cette régression unique est semblable à la variance du modèle d’ANCOVA (0.02837) et le modèle simplifié prédit presque aussi bien que le modèle plus complexe. (Ici encore, toutes les conditions d’application semblent remplies, si ce n’est de la valeur extrême).\nDonc, dans les deux cas (avec ou sans les valeurs extrêmes), l’addition d’un terme supplémentaire pour le sexe n’ajoute pas grand-chose. Il semble donc que le meilleur modèle soit celui de la régression simple. Un estimé raisonnablement précis de la taille des esturgeons peut être obtenu de la régression commune sur l’ensemble des résultats.\nNote: Il est fréquent que l’élimination de valeurs extrêmes en fasse apparaître d’autres. C’est parce que ces valeurs extrêmes dépendent de la variabilité résiduelle. Si on élimine les valeurs les plus déviantes, la variabilité résiduelle diminue, et certaines observations qui n’étaient pas si déviantes que cela deviennent proportionnellement plus déviantes. Notez aussi qu’en éliminant des valeurs extrêmes, l’effectif diminue et que la puissance décroît. Il faut donc être prudent.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#comparer-lajustement-de-modèles",
    "href": "36-ancova_glm.html#comparer-lajustement-de-modèles",
    "title": "\n9  ANCOVA et modèle linéaire général\n",
    "section": "\n9.6 Comparer l’ajustement de modèles",
    "text": "9.6 Comparer l’ajustement de modèles\nComme vous venez de le voir, le processus d’ajustement de modèles est itératif. La plupart du temps il y a plus d’un modèle qui peut être ajusté aux données et c’est à vous de choisir celui qui est le meilleur compromis entre la qualité d’ajustement (qu’on essaie de maximiser) et la complexité (qu’on essaie de minimiser). La stratégie de base en ajustant des modèles linéaires (ANOVA, régression, ANCOVA) est de privilégier le modèle le plus simple si la qualité d’ajustement n’est pas significativement plus mauvaise. R peut calculer une statistique F vous permettant de comparer l’ajustement de deux modèles. Dans ce cas, l’hypothèse nulle est que la qualité d’ajustement ne diffère pas entre les deux modèles.\n\n\n\n\n\n\nExercice\n\n\n\nEn utilisant les données de anc1dat comparez l’ajustement du modèle ANCOVA et de la régression commune:\n\n\n\nanova(model.ancova,model.linear)\n\nAnalysis of Variance Table\n\nModel 1: lfkl ~ sex + lage\nModel 2: lfkl ~ lage\n  Res.Df      RSS Df  Sum of Sq      F Pr(&gt;F)\n1     89 0.071623                            \n2     90 0.073113 -1 -0.0014899 1.8513 0.1771\n\n\nLa fonction anova() utilise la différence entre la somme des carrés des deux modèles et la divise par la différence entre le nombre de degrés de liberté pour obtenir un carré moyen. Ce carré moyen est utilisé au numérateur et est divisé par la variance résiduelle du modèle le plus complexe pour obtenir la statistique F. Dans ce cas-ci, le test de F n’est pas significatif, et on conclut que les deux modèles ont une qualité d’ajustement équivalente, et qu’on devrait donc privilégier le modèle le plus simple, la régression linéaire simple.\n\n\n\n\n\n\nExercice\n\n\n\nRefaites le même processus avec le données de anc3dat, ajustez le modèle complet avec interaction (LFKL~LAGE+LOCATE+LAGE:LOCATE) et sans interaction (LFKL~LAGE+LOCATE), Comparez l’ajustement des deux modèles, que concluez vous?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmodel.full.anc3dat&lt;-lm(lfkl ~ lage + locate + lage:locate, data = anc3dat)\nmodel.ancova.anc3dat&lt;-lm(lfkl ~ lage + locate, data = anc3dat)\nanova(model.full.anc3dat,model.ancova.anc3dat)\n\nAnalysis of Variance Table\n\nModel 1: lfkl ~ lage + locate + lage:locate\nModel 2: lfkl ~ lage + locate\n  Res.Df      RSS Df  Sum of Sq      F    Pr(&gt;F)    \n1     88 0.051358                                   \n2     89 0.060448 -1 -0.0090901 15.575 0.0001592 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCette fois-ci, le modèle plus complexe s’ajuste significativement mieux aux données. (Pas surprenant puisque nous avions précédemment conclu que l’interaction est significative avec ces données.)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#bootstrap",
    "href": "36-ancova_glm.html#bootstrap",
    "title": "\n9  ANCOVA et modèle linéaire général\n",
    "section": "\n9.7 Bootstrap",
    "text": "9.7 Bootstrap\n\n############################################################\n######\n# Bootstrap analysis\n# Bootstrap analysis BCa confidence intervals\n# Preferable when parameter distribution is far from normal\n# Bootstrap 95% BCa CI for regression coefficients\nlibrary(boot)\n\n# To simplify future modifications of the code in this file,\n# copy the data to a generic mydata dataframe\nmydata &lt;- anc3dat\n\n# create a myformula variable containing the formula for the model to be fitted\nmyformula &lt;- as.formula(lfkl ~ lage + locate + lage:locate)\n\n# function to obtain regression coefficients for each iteration\nbs &lt;- function(formula, data, indices) {\n  d &lt;- data[indices, ]\n  fit &lt;- lm(formula, data = d)\n  return(coef(fit))\n}\n# bootstrapping with 1000 replications\nresults &lt;- boot(data = mydata, statistic = bs, R = 1000, formula = myformula)\n\n# view results\nresults\nboot_res &lt;- summary(results)\nrownames(boot_res) &lt;- names(results$t0)\nboot_res\n\nop &lt;- par(ask = TRUE)\nfor (i in 1:length(results$t0)) {\n  plot(results, index = i)\n  title(names(results$t0)[i])\n}\npar(op)\n\n# get 95% confidence intervals\nfor (i in 1:length(results$t0)) {\n  cat(\"\\n\", names(results$t0)[i],\"\\n\")\n  print(boot.ci(results, type = \"bca\", index = i))\n}",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "36-ancova_glm.html#permutation-test",
    "href": "36-ancova_glm.html#permutation-test",
    "title": "\n9  ANCOVA et modèle linéaire général\n",
    "section": "\n9.8 Permutation test",
    "text": "9.8 Permutation test\n\n############################################################\n##########\n# Permutation test\n#\n# using lmperm library\n# To simplify future modifications of the code in this file,\n# copy the data to a generic mydata dataframe\nmydata&lt;-anc3dat\n# create a myformula variable containing the formula for the\n# model to be fitted\nmyformula&lt;-as.formula(lfkl ~ lage + locate + lage:locate)\nrequire(lmPerm2)\n# Fit desired model on the desired dataframe\nmymodel &lt;- lm(myformula, data = mydata)\n# Calculate p-values for each term by permutation\n# Note that lmp centers numeric variable by default, so to\n# get results that are\n# consistent with standard models, it is necessary to set\ncenter=FALSE\nmymodelProb &lt;- lmp(myformula, data = mydata, center=FALSE,\nperm = \"Prob\")\nsummary(mymodel)\nsummary(mymodelProb)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>ANCOVA et modèle linéaire général</span>"
    ]
  },
  {
    "objectID": "37-model_freq.html",
    "href": "37-model_freq.html",
    "title": "\n10  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "",
    "text": "10.1 Paquets et données requises pour le labo\nCe laboratoire nécessite:",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "37-model_freq.html#set-freq",
    "href": "37-model_freq.html#set-freq",
    "title": "\n10  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "",
    "text": "les paquets R:\n\nvcd\nvcdExtra\ncar\n\n\nles fichiers de données\n\nUSPopSurvey.csv\nloglin.csv\nsturgdat.csv",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "37-model_freq.html#organisation-des-données-3-formats",
    "href": "37-model_freq.html#organisation-des-données-3-formats",
    "title": "\n10  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n10.2 Organisation des données: 3 formats",
    "text": "10.2 Organisation des données: 3 formats\nLes résultats de certaines expériences sont sous forme de fréquences, par exemple le nombre de plantes infectées par un pathogène sous différents régimes d’infection, ou le nombre de tortues mâles et femelles qui éclosent à différentes températures (oui, chez les tortues le sexe dépends de la température!), etc. La question statistique qui se pose généralement est de savoir si la proportion des observations dans chaque catégorie (infecté vs non infecté, mâle vs femelle, etc) diffère significativement entre les traitements (régime d’infection ou température dans les deux exemples). Pour répondre à cette question, on peut organiser les données de manière à refléter comment les observations se retrouvent dans chaque catégorie. Il existe 3 façons d’organiser ces données. Vous devriez être capable de choisir la manière appropriée pour votre analyse, et savoir convertir entre elles avec R.\nLe fichier USPopSurvey.csv contient les donnée de recensement d’une ville du midwest américain en 1980:\n\nUSPopSurvey &lt;- read.csv(\"data/USPopSurvey.csv\")\nUSPopSurvey\n\n   ageclass    sex frequency\n1       0-9 female     17619\n2     10-19 female     17947\n3     20-29 female     21344\n4     30-39 female     19138\n5     40-49 female     13135\n6     50-59 female     11617\n7     60-69 female     11053\n8     70-79 female      7712\n9       80+ female      4114\n10      0-9   male     17538\n11    10-19   male     18207\n12    20-29   male     21401\n13    30-39   male     18837\n14    40-49   male     12568\n15    50-59   male     10661\n16    60-69   male      9374\n17    70-79   male      5348\n18      80+   male      1926\n\n\nNotez qu’il y a 18 lignes et 3 colonnes dans ce fichier. Chaque ligne donne le nombre de personnes (frequency) pour un sexe et une classe d’âge. Il y a 239539 individus qui ont été classifiés selon les 18 catégories (2 sexes x 9 classes d’âge). Cette manière de représenter les données est sous le format de fréquences (frequency form). C’est un format compact permettant d’enregistrer les données quand il y a seulement des variables catégoriques à représenter.\nLorsqu’il y a des variables continues, ce format ne peut être utilisé. Les données doivent être enregistrée sous le format de cas (case form) dans laquelle chaque observation (individu) est représenté par une ligne dans le fichier, et où chaque variable est représentée par une colonne. Le package vcdExtra contient la fonction expand.dft() qui permet de convertir de la forme de fréquence à la forme de cas. Par exemple, pour créer un data frame avec 239439 lignes et 2 colonnes (sex et ageclass) à partir du data frame USPopSurvey:\n\nUSPopSurvey.caseform &lt;- expand.dft(USPopSurvey, freq = \"frequency\")\nhead(USPopSurvey.caseform)\n\n  ageclass    sex\n1      0-9 female\n2      0-9 female\n3      0-9 female\n4      0-9 female\n5      0-9 female\n6      0-9 female\n\ntail(USPopSurvey.caseform)\n\n       ageclass  sex\n239534      80+ male\n239535      80+ male\n239536      80+ male\n239537      80+ male\n239538      80+ male\n239539      80+ male\n\n\nCes données peuvent finalement être organisées sous le format de tableau (table form) de contingence où chacune des n variables est représentée par une dimension d’un tableau n-dimensionnel (dans notre exemple on a 2 variables, sexe et classe d’âge, et les rangées pourraient représenter les classes d’âge et les colonnes chaque sexe). Les cellules de ce tableau contiennent les fréquences. Le format tableau peut être obtenu du format de fréquence ou de cas par la commande xtabs() :\n\n# convert case form to table form\nxtabs(~ ageclass + sex, USPopSurvey.caseform)\n\n        sex\nageclass female  male\n   0-9    17619 17538\n   10-19  17947 18207\n   20-29  21344 21401\n   30-39  19138 18837\n   40-49  13135 12568\n   50-59  11617 10661\n   60-69  11053  9374\n   70-79   7712  5348\n   80+     4114  1926\n\n# convert frequency form to table form\nxtabs(frequency ~ ageclass + sex, data = USPopSurvey)\n\n        sex\nageclass female  male\n   0-9    17619 17538\n   10-19  17947 18207\n   20-29  21344 21401\n   30-39  19138 18837\n   40-49  13135 12568\n   50-59  11617 10661\n   60-69  11053  9374\n   70-79   7712  5348\n   80+     4114  1926\n\n\n\n(#tab:unnamed-chunk-1)Fonctions permettant la conversion de données de fréquences entre les différents formats.\n\n\n\n\n\n\n\nDe (ligne) \\ Vers (colonne)\nCas\nFréquence\nTableau\n\n\n\nCas\n\nxtabs(~ A + B)\ntable(A, B)\n\n\nFréquence\nexpand.dft(X)\n\nxtabs(count ~ A + B)\n\n\nTableau\nexpand.dft(X)\nas.data.frame(X)",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "37-model_freq.html#visualiser-graphiquement-les-tableaux-de-contingence-et-test-dindépendance",
    "href": "37-model_freq.html#visualiser-graphiquement-les-tableaux-de-contingence-et-test-dindépendance",
    "title": "\n10  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n10.3 Visualiser graphiquement les tableaux de contingence et test d’indépendance",
    "text": "10.3 Visualiser graphiquement les tableaux de contingence et test d’indépendance\nLes tableaux de contingence peuvent servir à éprouver l’hypothèse d’indépendance des observations. Ceci équivaut à répondre à la question: est-ce que la classification des observations selon une variable (par exemple sex) indépendante de la classification par une autre variable (par exemple ageclass). En autres mots, est-ce que la proportion des mâles et femelles indépendante de l’âge ou varie avec l’âge?\nLe package vcd inclut la fonction mosaic() qui permet de représenter graphiquement le contenu d’un tableau de contingence:\n\nlibrary(vcd)\nUSTable &lt;- xtabs(frequency ~ ageclass + sex, data = USPopSurvey) # save the table form as USTable dataframe\n# Mosaic plot of the contingency table\nmosaic(USTable)\n\n\n\nReprésentation mosaique de la proportion des sexes par classe d’age\n\n\n\nCette mosaïque représente la proportion des observations dans chaque combinaison de catégories (ici il y a 18 catégories, 2 sexes x 9 classes d’âge). Les catégories contenant une plus grande proportion d’observations sont représentées par de plus grands rectangles. Visuellement, on peut voir que la proportion des mâles et femelles est approximativement égale chez les jeunes, mais que la proportion des femelles augmente chez les personnes âgées.\nLe test de Chi carré permet d’éprouver l’hypothèse nulle que la proportion des mâles et femelles ne change pas avec l’âge (est indépendante de l’âge):\n\n# Test of independence\nchisq.test(USTable) # runs chi square test of independence of sex and age class\n\n\n    Pearson's Chi-squared test\n\ndata:  USTable\nX-squared = 1162.6, df = 8, p-value &lt; 2.2e-16\n\n\nLa valeur p étant très faible, on rejette donc l’hypothèse nulle que âge et sexe sont indépendants. Ces graphiques mosaïques peuvent êtres colorés pour souligner les catégories qui contribuent le plus à cette dépendance:\n\n# Mosaic plot of the contingency table with shading\nmosaic(USTable, shade=TRUE)\n\n\n\nReprésentation mosaique de la proportion des sexes par classe d’age avec échelle de couleur\n\n\n\nLa couleur de chaque rectangle est proportionnelle à la déviation des fréquences observées de ce qui serait attendu si l’âge et le sexe étaient indépendants. Les classes d’âge 40-49 et 50-59 ont un rapport des sexe approximativement égal à celui de toutes les classes d’âge réunies. Il y a plus de jeunes mâles et de femelles âgées que si le rapport des sexe ne variait pas avec l’âge.et ces rectangles sont colorés en bleu. De l’autre côté, il y a moins de jeunes femelles et de mâles âgés que si le rapport des sexe était indépendant de l’âge, et ces rectangles sont en rouge. La valeur p à la droite de la figure est pour le test de Chi carré qui éprouve l’hypothèse nulle d’indépendance pour l’ensemble des observations, toutes classes d’âge confondues.\nL’estimation de la valeur p associée à la statistique du Chi carré est approximative lorsque les fréquences attendues sont faibles dans certaines cellules, et ce particulièrement pour les tableaux de contingence 2x2. Deux options permettant des valeurs p plus exactes sont préférées dans ce cas, et le choix dépends du nombre total d’observations. Pour de grands échantillons (comme ici avec plus de 200,000 observations!), une approche par simulation de type Monte Carlo est suggérée et peut être obtenue en ajoutant simulate.p.value=TRUE comme argument à la fonction chisq.test() :\n\n# Monte-carlo estimation of p value (better for small n)\nchisq.test(USTable, simulate.p.value = TRUE, B = 10000)\n\n\n    Pearson's Chi-squared test with simulated p-value (based on 10000\n    replicates)\n\ndata:  USTable\nX-squared = 1162.6, df = NA, p-value = 9.999e-05\n\n\nIci, la simulation a été faite B=10000 fois, et la valeur de Chi carré observée avec les données réelles n’a jamais été observée. Par conséquent, p a été estimé à 1/10001=9.999e-05, qui est beaucoup plus élevé que la valeur p estimée à partir de la distribution théorique de Chi carré (p&lt; 2.2e-16). Cette différence est due au moins en partie à un artéfacts de la simulation. Pour obtenir des valeurs p de l’ordre de 1e-16, il faut effectuer au moins 10 16 simulations. Et je ne suis pas aussi patient que ça!\nPour de petits tableaux de contingence avec des fréquences attendues petites, le test exact de Fisher peut servir à estimer la valeur p associée à l’hypothèse d’indépendance. Mais ce test ne peut être effectué avec de grands échantillons, comme ici:\n\n# Fisher exact test for contingency tables (small samples and small tables)\nfisher.test(USTable) # fails here because too many observations\n\nError in fisher.test(USTable): FEXACT error 40.\nOut of workspace.\n\nfisher.test(USTable, simulate.p.value = TRUE, B = 10000)\n\n\n    Fisher's Exact Test for Count Data with simulated p-value (based on\n    10000 replicates)\n\ndata:  USTable\np-value = 9.999e-05\nalternative hypothesis: two.sided",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "37-model_freq.html#régression-de-poisson-une-alternative-au-test-de-chi-carré-pour-les-tableaux-de-contingence",
    "href": "37-model_freq.html#régression-de-poisson-une-alternative-au-test-de-chi-carré-pour-les-tableaux-de-contingence",
    "title": "\n10  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n10.4 Régression de Poisson: une alternative au test de Chi carré pour les tableaux de contingence",
    "text": "10.4 Régression de Poisson: une alternative au test de Chi carré pour les tableaux de contingence\nRendu à ce stade, vous devriez avoir appris à apprécier la flexibilité et la généralité des modèles linéaires, et réaliser que le test de t est un cas spécial d’un modèle linéaire avec une variable indépendante catégorique. L’analyse des tableaux de contingence par le test du Chi carré peut également être généralisé. Un modèle linéaire généralisé pour une distribution de Poisson peut être utilisé quand la variable dépendante est une fréquence d’observations et les variables indépendantes sont catégorique (comme pour les tableaux de contingence, on parle alors de modèles log-linéaires), continue (régression Poisson), ou une combinaison de variables indépendante continues et catégoriques (aussi appelé régression de Poisson, mais avec des variables catégoriques en plus, analogue à l’ANCOVA sensu largo).\nCes modèles prédisent le logarithme naturel de la fréquence des observations en fonction des variables indépendantes. Comme pour les modèles linéaires qui présument de la normalité des résidus, on peut évaluer la qualité d’ajustement du modèle (par AICc par exemple) et la signification statistique des termes du modèle (par exemple en comparant l’ajustement d’un modèle “complet” et celui d’un modèle qui exclue un terme à tester). On peut également obtenir des estimés des paramètre pour chaque terme dans le modèle, avec des intervalles de confiance et des valeur p pour l’hypothèse nulle que ce terme n’a pas d’influence sur la fréquence.\nLa fonction glm() avec l’option family=poisson() permet l’estimation, par la méthode du maximum de vraisemblance, de modèles linéaires pour des fréquences. Comparativement aux modèles linéaires vus précédemment, une des particularité de ces modèles est que seuls les termes d’interaction sont d’intérêt. En partant des données de recensement en forme tableau, on peut ajuster un glm aux fréquences observées par sexe et classe d’âge par:\n\nmymodel &lt;- glm(frequency ~ sex * ageclass, family = poisson(), data = USPopSurvey)\nsummary(mymodel)\n\n\nCall:\nglm(formula = frequency ~ sex * ageclass, family = poisson(), \n    data = USPopSurvey)\n\nCoefficients:\n                       Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)            9.776733   0.007534 1297.730  &lt; 2e-16 ***\nsexmale               -0.004608   0.010667   -0.432   0.6657    \nageclass10-19          0.018445   0.010605    1.739   0.0820 .  \nageclass20-29          0.191793   0.010179   18.842  &lt; 2e-16 ***\nageclass30-39          0.082698   0.010441    7.921 2.36e-15 ***\nageclass40-49         -0.293697   0.011528  -25.477  &lt; 2e-16 ***\nageclass50-59         -0.416508   0.011951  -34.850  &lt; 2e-16 ***\nageclass60-69         -0.466276   0.012134  -38.428  &lt; 2e-16 ***\nageclass70-79         -0.826200   0.013654  -60.511  &lt; 2e-16 ***\nageclass80+           -1.454582   0.017316  -84.004  &lt; 2e-16 ***\nsexmale:ageclass10-19  0.018991   0.014981    1.268   0.2049    \nsexmale:ageclass20-29  0.007275   0.014400    0.505   0.6134    \nsexmale:ageclass30-39 -0.011245   0.014803   -0.760   0.4475    \nsexmale:ageclass40-49 -0.039519   0.016416   -2.407   0.0161 *  \nsexmale:ageclass50-59 -0.081269   0.017136   -4.742 2.11e-06 ***\nsexmale:ageclass60-69 -0.160154   0.017633   -9.083  &lt; 2e-16 ***\nsexmale:ageclass70-79 -0.361447   0.020747  -17.422  &lt; 2e-16 ***\nsexmale:ageclass80+   -0.754343   0.029598  -25.486  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5.3611e+04  on 17  degrees of freedom\nResidual deviance: 6.5463e-12  on  0  degrees of freedom\nAIC: 237.31\n\nNumber of Fisher Scoring iterations: 2\n\n\nL’ajustement du modèle complet, avec L’interaction triple sex:ageclass interaction, permet à la proportion des mâles et femelles de changer entre les classes d’âge, et donc d’estimer exactement les fréquences observées pour chaque combinaison de sexe et classe d’âge (notez que les résidus (deviance residuals) sont tous 0, et que l’estimé de déviance résiduelle est également approximativement zéro).\nUn masochiste peut utiliser le tableau des coefficients pour obtenir la fréquence prédite pour les différentes catégories. Les fréquences prédites, comme pour l’ANOVA à critères multiple, sont obtenus en additionnant les coefficients appropriés. Puisque, en R, le premier niveau d’une variable catégorique (facteur) en ordre alphabétique) est utilisé comme référence, l’ordonnée à l’origine (9.776733) est la valeur prédite pour le logarithme naturel de la fréquence des femelles dans la première classe d’âge (0 to 9). En effet, 9.776733 est approximativement égal à 17619, le nombre observé de femelles dans cette classe d’âge.\nPour les mâles dans la classe d’âge 80+, il faut calcule l’antilog du coefficient pour l’ordonnée à l’origine (pour les femelles dans la première classe d’âge), plus le coefficient pour sexmale (égal à la différence du log de la fréquence entre les femelles et les mâles), plus le coefficient pour la classe d’âge 80+ qui corresponds à la différence de fréquence entre cette classe d’âge et la classe d’âge de référence, plus le coefficient pour l’interaction sexmale:ageclass80+ (qui corresponds à la différence de proportion de mâles dans cette classe d’âge par rapport à la classe d’âge de référence). Ceci donne: ln(frequency)=9.776733-0.004608-1.454582-0.754343=7.5632, et la fréquence est égale à e 7.5632 =1926\nIl y a de nombreuses valeur p dans ce tableau, mais elle ne sont en général pas très utiles. Pour éprouver l’hypothèse que l’effet du sexe sur la fréquence est identique dans chaque classe d’âge (i.e. que sexe et âge sont indépendants), vous devez ajuster un modèle qui exclut cette interaction (sex:ageclass) et déterminer comment l’ajustement du modèle est affecté.\nLa fonction Anova() du package car permet de prendre un raccourci:\n\nAnova(mymodel, type = 3, test = \"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: frequency\n             LR Chisq Df Pr(&gt;Chisq)    \nsex               0.2  1     0.6657    \nageclass      21074.6  8     &lt;2e-16 ***\nsex:ageclass   1182.2  8     &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLes arguments type=3 and test=\"LR\" font en sorte que le test effectué pour comparer le modèle complet aux modèles réduits est les test de Chi carré sur le rapport de vraisemblance (Likelihood Ratio Chi-Square) à partir de la variance résiduelle, et que c’est un test partiel, et non séquentiel.\nSelon ces tests, il n’y a pas d’effet principal de sex (p=0.667) mais il y a un effet principal de ageclass et une interaction significative sex:ageclass. L’interaction significative signifie que l’effet du sexe sur la fréquence varie selon les classes d’âge, bref que le rapport des sexe varie avec l’âge. L’effet principal de ageclass signifie que la fréquence des individus varie avec l’âge dans la population recensée (i.e. que certaines classes d’âge sont plus populeuses que d’autres). L’absence d’un effet principal du sexe suggère qu’il y a approximativement le même nombre de mâles et femelles dans l’échantillon (quoique, puisqu’il y a une interaction, vous devez être prudents en faisant cette déclaration. C’est “vrai” au total, mais semble incorrect pour certaines classes d’âge).",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "37-model_freq.html#tester-une-hypothèse-extrinsèque",
    "href": "37-model_freq.html#tester-une-hypothèse-extrinsèque",
    "title": "\n10  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n10.5 Tester une hypothèse extrinsèque",
    "text": "10.5 Tester une hypothèse extrinsèque\nLe test d’indépendance ci-dessus éprouve une hypothèse intrinsèque parce que les proportions utilisées pour calculer les valeurs attendues et tester l’indépendance sont celles observées (i.e. la proportion des mâles et femelles dans tout l’échantillon, et la proportion des individus dans chaque classe d’âge).\nPour éprouver l’hypothèse (extrinsèque) que le rapport des sexes est 1:1 pour les individus les plus jeunes (ageclass 0-9), on doit produire le tableau 2X2 des fréquences observées et attendues. Les fréquences attendues sont obtenues simplement en divisant le total des mâles et femelles par 2.\nCode R pour créer et analyser un tableau de contingence 2X2 et éprouver une hypothèse extrinsèque\n\n### Produce a table of obs vs exp for 0-9 age class\nPopn0.9 &lt;- rbind(c(17578, 17578), c(17619, 17538))\n### Run X2 test on above table\nchisq.test(Popn0.9, correct = F) ### X2 without Yates\nchisq.test(Popn0.9) ### X2 with Yates\n\n\n\n\n\n\n\nExercice\n\n\n\nÉprouvez l’hypothèse nulle que la proportion de mâles et femelles à la naissance est égale. Que concluez-vous? Croyez-vous que ces données sont appropriées pour tester cette hypothèse?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nchisq.test(Popn0.9, correct = F)\n\n\n    Pearson's Chi-squared test\n\ndata:  Popn0.9\nX-squared = 0.093309, df = 1, p-value = 0.76\n\nchisq.test(Popn0.9)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  Popn0.9\nX-squared = 0.088758, df = 1, p-value = 0.7658\n\n\n\n\n\nNotez que pour un tableau 2X2, on devrait utiliser une correction de Yates ou un test de Fisher. Le test de Fisher ne pouvant être utilisé lorsque l’échantillon dépasse 200, on utilise la correction de Yates. Selon cette analyse, on accepte l’hypothèse nulle que le rapport des sexes est 1:1à la naissance. Ceci dit, ces données ne sont pas très appropriées pour éprouver l’hypothèse car la première classe d’âge est trop grossière. Il est possible que le rapport des sexes à la naissance soit différent de 1:1 mais que la mortalité différentielle des deux sexes compense au cours des 9 premières années (par exemple si il y a plus de mâles à la naissance, mais que les jeunes garçons ont une survie plus faible au cours de leurs 9 premières années). Dans un tel cas, le rapport des sexes n’est PAS de 1:1 à la naissance, mais on accepte l’hypothèse nulle à partir des données dans la classe d’âge 0-9.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "37-model_freq.html#régression-de-poisson-pour-lanalyse-de-tableaux-de-contingence-à-plusieurs-critères",
    "href": "37-model_freq.html#régression-de-poisson-pour-lanalyse-de-tableaux-de-contingence-à-plusieurs-critères",
    "title": "\n10  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n10.6 Régression de Poisson pour l’analyse de tableaux de contingence à plusieurs critères",
    "text": "10.6 Régression de Poisson pour l’analyse de tableaux de contingence à plusieurs critères\nLe principe d’éprouver l’indépendance en examinant les interactions peut être utilisé avec les tableaux de contingence à plusieurs critères. Par exemple, examinons si la température (2 niveaux: base et haute) et l’éclairage (2 niveaux: bas et haut) affectent si des plantes sont infectées (2 niveaux: infecté et non-infecté) par un pathogène. On peut représenter ces données par un tableau de contingence à 3 critères (température, lumière, statut d’infection).\nL’ajustement de modèles log-linéaires à des données de fréquence implique que l’on éprouve plusieurs modèles en les comparant au modèle complet (saturé). Une série de modèles contenant tous les termes sauf une des interactions qui nous intéressent est produite, et l’ajustement de chaque modèle est comparé à celui du modèle complet. Si la réduction de la qualité d’ajustement n’est pas significative, cela implique que l’interaction manquante contribue peu à la qualité de l’ajustement. Par contre, si le modèle réduit s’ajuste nettement moins bien aux données, alors l’interaction manquante contribue beaucoup à l’ajustement du modèle complet. Comme pour les tableaux de contingence 2X2, les termes qui nous intéressent le plus sont les interactions, pas les effets principaux, si l’on teste pour l’indépendance des différents facteurs.\nLe fichier loglin.csv contient les fréquences (frequency) des plantes infectées ou non infectées (infected) à basse et haute température (temperature) à basse et haute lumière (light). Pour visualiser ces données et déterminer si le taux d’infection dépends de la lumière et de la température, on peut faire une figure mosaïque et ajuster un modèle log-linéaire:\n\nloglin &lt;- read.csv(\"data/loglin.csv\")\n# Convert from frequency form to table form for mosaic plot\nloglinTable &lt;- xtabs(frequency ~ temperature + light + infected, data = loglin)\n# Create mosaic plot to look at data\nmosaic(loglinTable, shade = TRUE)\n\n\n\nProportion de plantes infectées en fonction de la température er la lumière\n\n\n\nCette expérience contrôlée avec le même nombre de plantes à chaque niveau de lumière et de température produit une mosaïque où la surface occupée par les observations dans les quatre quadrants est égale. Ce qui nous intéresse, le taux d’infection par le pathogène, semble varier entre les quadrants (i.e. les niveaux de température et de lumière). Le rectangle rouge dans le coin en bas à gauche indique que le nombre de plantes infectées à basse température et haute lumière est plus faible qu’attendu si ces deux facteurs n’influencent pas le taux d’infection. Même chose pour les conditions de basse lumière et de haute température (coin supérieur droit). La valeur p au bas de l’échelle représente un test d’indépendance équivalent à comparer le modèle complet au modèle excluant toutes les interactions et ne contenant que les effets principaux de la température, la lumière, et le statut d’infection sur le logarithme naturel du nombre d’observations.\n\n# Fit full model\nfull.model &lt;- glm(frequency ~ temperature * light * infected, family = poisson(), data = loglin)\n# Test partial effect of terms in full model\nAnova(full.model, type = 3, test = \"LR\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: frequency\n                           LR Chisq Df Pr(&gt;Chisq)    \ntemperature                  9.1786  1  0.0024487 ** \nlight                       13.2829  1  0.0002678 ***\ninfected                     0.0000  1  0.9999999    \ntemperature:light            5.6758  1  0.0172008 *  \ntemperature:infected        29.0612  1  7.013e-08 ***\nlight:infected              20.2687  1  6.729e-06 ***\ntemperature:light:infected   1.0840  1  0.2978126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLes probabilités associées à chaque terme sont ici calculées en comparant l’ajustement du modèle complet à un modèle qui exclue seulement le terme d’intérêt. Plusieurs des termes sont ici sans véritable intérêt puisque les fréquences sont partiellement contrôlées dans notre expérience. Puisque la question biologique porte sur le taux d’infection, les seuls termes d’intérêt sont les termes d’interactions qui incluent le statut d’infection (temperature:infected, light:infected et temperature:light:infected.\n\nL’interation significative temperature:infected implique que le taux d’infection n’est pas indépendant de la température. D’ailleurs il est apparent dans la mosaïque que le taux d’infection (le nombre relatif de plantes infectées) est supérieur à haute température.\nL’interaction significative light:infected implique que le taux d’infection dépends de la lumière. La mosaïque illustre que la proportion des plantes infectées est plus élevée en basse lumière.\nL’interaction temperature:light:infected n’est pas significative. Cela implique que l’effet de la température et de la lumière sur le taux d’infection sont indépendants. Autrement dit, l’effet de la lumière sur le taux d’infection ne dépends pas de la température, et vice versa.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "37-model_freq.html#ex-glm",
    "href": "37-model_freq.html#ex-glm",
    "title": "\n10  Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson\n",
    "section": "\n10.7 Exercice",
    "text": "10.7 Exercice\nLe fichier Sturgdat contient les données qui vous permettront d’éprouver l’hypothèse que le nombre d’esturgeons capturé est indépendants du site, de l’année, et du sexe. Avant de commencer l’analyse, les données devront être réorganisées pour pouvoir ajuster un modèle log-linéaire:\n\n\n\n\n\n\nExercice\n\n\n\nOuvrez sturgdat.csv, puis utilisez la fonction table() pour obtenir les fréquence d’individus capturés par sex, location, et year . Sauvegardez ce tableau comme strugdat.table . Faites une figure mosaïque de ces données.\n\n\n\nsturgdat &lt;- read.csv(\"data/sturgdat.csv\")\n# Reorganize data from case form to table form\nsturgdat.table &lt;- with(sturgdat, table(sex, year, location))\n# display the table\nsturgdat.table\n\n, , location = CUMBERLAND  \n\n              year\nsex            1978 1979 1980\n  FEMALE         10   30   11\n  MALE           14   14    6\n\n, , location = THE_PAS     \n\n              year\nsex            1978 1979 1980\n  FEMALE          5   12   38\n  MALE           16   12   18\n\n# Create data frame while converting from table form to frequency form\nsturgdat.freq &lt;- as.data.frame(sturgdat.table)\n# display data frame\nsturgdat.freq\n\n            sex year     location Freq\n1  FEMALE       1978 CUMBERLAND     10\n2  MALE         1978 CUMBERLAND     14\n3  FEMALE       1979 CUMBERLAND     30\n4  MALE         1979 CUMBERLAND     14\n5  FEMALE       1980 CUMBERLAND     11\n6  MALE         1980 CUMBERLAND      6\n7  FEMALE       1978 THE_PAS         5\n8  MALE         1978 THE_PAS        16\n9  FEMALE       1979 THE_PAS        12\n10 MALE         1979 THE_PAS        12\n11 FEMALE       1980 THE_PAS        38\n12 MALE         1980 THE_PAS        18\n\n# Look at the data as mosaic plot\n# mosaic using the table created above\nmosaic(sturgdat.table, shade = TRUE)\n\n\n\nFréquence de femelles et males en fonction de l’année et du lieu\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nÀ partir de ces données en format de fréquence, ajustez le modèle loglinéaire complet et le tableau d’anova avec les statistique de Chi carré pour les termes du modèles. Est-ce que l’interaction triple (location:year:sex) est significative? Est-ce que le rapport des sexes varien entre les sites ou d’une année à l’autre?.\n\n\n\n# Fit full model\nfull.model &lt;- glm(Freq ~ sex * year * location, data = sturgdat.freq, family = \"poisson\")\nsummary(full.model)\n\n\nCall:\nglm(formula = Freq ~ sex * year * location, family = \"poisson\", \n    data = sturgdat.freq)\n\nCoefficients:\n                                              Estimate Std. Error z value\n(Intercept)                                    2.30259    0.31623   7.281\nsexMALE                                        0.33647    0.41404   0.813\nyear1979                                       1.09861    0.36515   3.009\nyear1980                                       0.09531    0.43693   0.218\nlocationTHE_PAS                               -0.69315    0.54772  -1.266\nsexMALE        :year1979                      -1.09861    0.52554  -2.090\nsexMALE        :year1980                      -0.94261    0.65498  -1.439\nsexMALE        :locationTHE_PAS                0.82668    0.65873   1.255\nyear1979:locationTHE_PAS                      -0.22314    0.64550  -0.346\nyear1980:locationTHE_PAS                       1.93284    0.64593   2.992\nsexMALE        :year1979:locationTHE_PAS      -0.06454    0.83986  -0.077\nsexMALE        :year1980:locationTHE_PAS      -0.96776    0.87942  -1.100\n                                              Pr(&gt;|z|)    \n(Intercept)                                    3.3e-13 ***\nsexMALE                                        0.41641    \nyear1979                                       0.00262 ** \nyear1980                                       0.82732    \nlocationTHE_PAS                                0.20569    \nsexMALE        :year1979                       0.03658 *  \nsexMALE        :year1980                       0.15011    \nsexMALE        :locationTHE_PAS                0.20950    \nyear1979:locationTHE_PAS                       0.72957    \nyear1980:locationTHE_PAS                       0.00277 ** \nsexMALE        :year1979:locationTHE_PAS       0.93875    \nsexMALE        :year1980:locationTHE_PAS       0.27114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance:  5.7176e+01  on 11  degrees of freedom\nResidual deviance: -2.6645e-15  on  0  degrees of freedom\nAIC: 77.28\n\nNumber of Fisher Scoring iterations: 3\n\nAnova(full.model, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Freq\n                  LR Chisq Df Pr(&gt;Chisq)    \nsex                 0.6698  1  0.4131256    \nyear               13.8895  2  0.0009637 ***\nlocation            1.6990  1  0.1924201    \nsex:year            4.6930  2  0.0957024 .  \nsex:location        1.6323  1  0.2013888    \nyear:location      25.2580  2  3.276e-06 ***\nsex:year:location   1.6677  2  0.4343666    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCe tableau a trois critères: sex, location et year . Donc le modèles compelt (saturé) contient 7 termes: trois effets principaux (sex, location et year), trois interactions du second degré (double) (sex:year, sex:location et year: location) et une interaction du troisième degré (triple)(sex:year:location). La déviance nulle est 57.17574, la déviance résiduelle du modèle complet est, sans surprise, 0. La déviance pouvant être attribuée à l’interaction triple est 1.6677, non significative.\nQu’est ce que cela implique? S’il y a des interactions doubles, alors elles ne dépendent pas de la troisième variable. Par exemple, si le rapport des sexe des esturgeons varie d’une année à l’autre (une interaction sex:year), alors cette tendance est la même aux 2 stations.\nPuisqu’il n’y a pas d’interaction triple, il est (statistiquement) justifié de combiner les données pour éprouver les interactions du second degré. Par exemple, pour tester l’effet sex:location, on peut combiner les années. Pour tester l’effet sex:year, on peut combiner les sites. Cette aggrégation a pour effet d’augmenter la puissance, et est analogue à la stratégie en ANOVA à critères multiples. L’approche de la régression de Poisson permet de faire l’équivalent simplement en ajustant le modèle sans l’interaction du troisième degré.\n\nAjustez le modèle en excluant l’interaction du troisième degré:\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\no2int.model &lt;- glm(Freq ~ sex + year + location + sex:year + sex:location + year:location, data = sturgdat.freq, family = \"poisson\")\nAnova(o2int.model, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Freq\n              LR Chisq Df Pr(&gt;Chisq)    \nsex             1.8691  1  0.1715807    \nyear           15.1289  2  0.0005186 ***\nlocation        1.5444  1  0.2139568    \nsex:year       15.5847  2  0.0004129 ***\nsex:location    2.1762  1  0.1401583    \nyear:location  28.3499  2  6.981e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nL’interaction sex:location n’explique pas une portion significative de la déviance, alors que les deux autres sont significatives. Le rapport des sexes ne varie pas entre les sites, mais il varie selon les années. L’interaction year:location est aussi significative (voir plus pas pour son interprétation).\nDevriez vous tenter de simplifier le modèle encore plus? Les vrais statisticiens sont divisés sur cette question. Tous s’entendent cependant sur le fait que conserver des interactions non significatives dans un modèle peut réduire la puissance. De l’autre côté, le retrait des interactions non significatives peut rendre l’interprétation plus délicate lorsque les observations ne sont pas bien balancées (i.e. il y a de la colinéarité entre les termes du modèle).\n\nAjustez le modèle sans l’interaction sex:location :\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\no2int.model2 &lt;- glm(Freq ~ sex + year + location + sex:year + year:location, data = sturgdat.freq, family = \"poisson\")\nAnova(o2int.model2, type = 3)\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Freq\n              LR Chisq Df Pr(&gt;Chisq)    \nsex             5.0970  1  0.0239677 *  \nyear           16.1226  2  0.0003155 ***\nlocation        0.2001  1  0.6546011    \nsex:year       13.9883  2  0.0009173 ***\nyear:location  26.7534  2  1.551e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nLes deux interactions sont significatives et ce modèle semble le meilleur. Ce modèle est:\n\\[ln[f_{(ijk)} ] = location + sex + year + sex:year + location:year\\]\nComment ces effets peuvent-ils être interprétés biologiquement? Souvenez vous que, comme dans les test d’indépendance, on n’est pas vraiment intéressé aux effets principaux, seulement par les interactions. Par exemple, l’effet principal de location tnous dit que le nombre total d’esturgeons capturé (le total des 2 sexes pendant les 3 années d’échantillonnage) diffère entre les 2 sites. Cela n’est pas vraiment surprenant et peu intéressant en l’absence d’information sur l’effort de pêche. Cependant, l’interaction sex:year nous dit que le rapport des sexes a changé d’une année à l’autre. Et puisque l’interaction du troisième degré n’est pas significative, on sait que ce changement dans le temps est approximativement le même dans les deux sites. Un résultat possiblement intéressant. Pourquoi? Comme l’expliquer?\nL’interaction location:year nous dit que le nombre d’esturgeons n’a pas seulementt varié d’une année à l’autre, mais que la tendance dans le temps diffère entre les deux sites. Ceci pourrait refléter une différence d’effort de pêche à un des sites durant l’une des campagnes d’échantillonnage, ou un impact à seulement un des deux sites la dernière année par exemple. Mais cette tendance est la même pour les mâles et les femelles (donc n’a pas affecté le rapport des sexes) puisque l’interaction triple n’est pas significative.",
    "crumbs": [
      "Données",
      "Modèles linéaires",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Analyse de données de fréquence: Tableaux de contingence, modèles log-linéaires et régression de Poisson</span>"
    ]
  },
  {
    "objectID": "901-bibliographie.html",
    "href": "901-bibliographie.html",
    "title": "Références",
    "section": "",
    "text": "Paquets R\nCe livre a utilisé les paquets R (excluant leurs dépendances) listé dans le tableau Table 1. Comme recommandé par l’équipe de de développement de ‘tidyverse’, seul le paquets ‘tidyverse’ est cité et non pas chacun de ses composants.\nTable 1: Paquets utilisés dans le livre\n\n\n\n\n\n\n\n\n\nPaquets\nVersion\nCitation\n\n\n\nbase\n4.4.1\nR Core Team (2024)\n\n\nboot\n1.3.30\n\nA. C. Davison et D. V. Hinkley (1997); Angelo Canty et B. D. Ripley (2024)\n\n\n\ncar\n3.1.2\nFox et Weisberg (2019a)\n\n\neffects\n4.2.2\n\nFox (2003); Fox et Hong (2009); Fox et Weisberg (2018); Fox et Weisberg (2019b)\n\n\n\nemoji\n15.0\nHvitfeldt (2022)\n\n\ngrateful\n0.2.4\nFrancisco Rodriguez-Sanchez et Connor P. Jackson (2023)\n\n\nknitr\n1.47\n\nXie (2014); Xie (2015); Xie (2024)\n\n\n\nlme4\n1.1.35.3\nBates et al. (2015)\n\n\nlmPerm\n2.1.0\nWheeler et Torchiano (2016)\n\n\nlmtest\n0.9.40\nZeileis et Hothorn (2002)\n\n\nmultcomp\n1.4.25\nHothorn et al. (2008)\n\n\nMuMIn\n1.47.5\nBartoń (2023)\n\n\nperformance\n0.12.0\nLüdecke et al. (2021)\n\n\npwr\n1.3.0\nChampely (2020)\n\n\nquestionr\n0.7.8\nBarnier et al. (2023)\n\n\nreshape\n0.8.9\n(reshape?)\n\n\nrmarkdown\n2.27\n\nXie et al. (2018); Xie et al. (2020); Allaire et al. (2024)\n\n\n\nsimpleboot\n1.1.7\nPeng (2019)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\nvcd\n1.4.12\n\nMeyer et al. (2006); Zeileis et al. (2007); Meyer et al. (2023)\n\n\n\nvcdExtra\n0.8.5\nFriendly (2023)",
    "crumbs": [
      "Données",
      "Références"
    ]
  },
  {
    "objectID": "901-bibliographie.html#bibliographie",
    "href": "901-bibliographie.html#bibliographie",
    "title": "Références",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\nA. C. Davison, and D. V. Hinkley. 1997. Bootstrap methods and their\napplications. Cambridge University Press, Cambridge.\n\n\nAllaire, J., Y. Xie, C. Dervieux, J. McPherson, J. Luraschi, K. Ushey,\nA. Atkins, H. Wickham, J. Cheng, W. Chang, and R. Iannone. 2024. rmarkdown: Dynamic documents for r.\n\n\nAngelo Canty, and B. D. Ripley. 2024. boot:\nBootstrap r (s-plus) functions.\n\n\nBarnier, J., F. Briatte, and J. Larmarange. 2023. questionr: Functions to make surveys processing\neasier.\n\n\nBartoń, K. 2023. MuMIn:\nMulti-model inference.\n\n\nBates, D., M. Mächler, B. Bolker, and S. Walker. 2015. Fitting linear\nmixed-effects models using lme4. Journal\nof Statistical Software 67:1–48.\n\n\nChampely, S. 2020. pwr: Basic functions for power analysis.\n\n\nDouglas, A. 2023. An introduction to\nr.\n\n\nFox, J. 2003. Effect\ndisplays in R for generalised linear models. Journal of\nStatistical Software 8:1–27.\n\n\nFox, J., and J. Hong. 2009. Effect displays in\nR for multinomial and proportional-odds logit models:\nExtensions to the effects package.\nJournal of Statistical Software 32:1–24.\n\n\nFox, J., and S. Weisberg. 2018. Visualizing fit and lack of\nfit in complex regression models with predictor effect plots and partial\nresiduals. Journal of Statistical Software 87:1–27.\n\n\nFox, J., and S. Weisberg. 2019a. An\nR companion to applied regression. Third. Sage,\nThousand Oaks CA.\n\n\nFox, J., and S. Weisberg. 2019b. An\nr companion to applied regression. 3rd edition. Sage, Thousand Oaks\nCA.\n\n\nFrancisco Rodriguez-Sanchez, and Connor P. Jackson. 2023. grateful: Facilitate citation of r packages.\n\n\nFriendly, M. 2023. vcdExtra: “vcd” extensions and additions.\n\n\nHothorn, T., F. Bretz, and P. Westfall. 2008. Simultaneous inference in\ngeneral parametric models. Biometrical Journal 50:346–363.\n\n\nHvitfeldt, E. 2022. emoji: Data and function to work with emojis.\n\n\nLüdecke, D., M. S. Ben-Shachar, I. Patil, P. Waggoner, and D. Makowski.\n2021. performance: An R package for\nassessment, comparison and testing of statistical models. Journal of\nOpen Source Software 6:3139.\n\n\nMeyer, D., A. Zeileis, and K. Hornik. 2006. The strucplot framework:\nVisualizing multi-way contingency tables with vcd. Journal of\nStatistical Software 17:1–48.\n\n\nMeyer, D., A. Zeileis, K. Hornik, and M. Friendly. 2023. vcd: Visualizing categorical data.\n\n\nPeng, R. D. 2019. simpleboot: Simple bootstrap routines.\n\n\nR Core Team. 2024. R:\nA language and environment for statistical computing. R Foundation\nfor Statistical Computing, Vienna, Austria.\n\n\nWheeler, B., and M. Torchiano. 2016. lmPerm: Permutation tests for linear models.\n\n\nWickham, H., M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François,\nG. Grolemund, A. Hayes, L. Henry, J. Hester, M. Kuhn, T. L. Pedersen, E.\nMiller, S. M. Bache, K. Müller, J. Ooms, D. Robinson, D. P. Seidel, V.\nSpinu, K. Takahashi, D. Vaughan, C. Wilke, K. Woo, and H. Yutani. 2019.\nWelcome to the tidyverse. Journal of Open Source Software\n4:1686.\n\n\nXie, Y. 2014. knitr: A comprehensive tool\nfor reproducible research in R. in V. Stodden, F.\nLeisch, and R. D. Peng, editors. Implementing reproducible computational\nresearch. Chapman; Hall/CRC.\n\n\nXie, Y. 2015. Dynamic documents with\nR and knitr. 2nd edition. Chapman; Hall/CRC, Boca\nRaton, Florida.\n\n\nXie, Y. 2024. knitr: A general-purpose package for dynamic\nreport generation in r.\n\n\nXie, Y., J. J. Allaire, and G. Grolemund. 2018. R markdown: The definitive\nguide. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y., C. Dervieux, and E. Riederer. 2020. R markdown\ncookbook. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nZeileis, A., and T. Hothorn. 2002. Diagnostic checking in\nregression relationships. R News 2:7–10.\n\n\nZeileis, A., D. Meyer, and K. Hornik. 2007. Residual-based shadings\nfor visualizing (conditional) independence. Journal of Computational\nand Graphical Statistics 16:507–525.",
    "crumbs": [
      "Données",
      "Références"
    ]
  },
  {
    "objectID": "902-donnees.html",
    "href": "902-donnees.html",
    "title": "Annexe A — Données utilisées dans le livre",
    "section": "",
    "text": "check what is done for BIO8940",
    "crumbs": [
      "Données",
      "Annexes",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Données utilisées dans le livre</span>"
    ]
  }
]