[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "On the R-way to hell",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#the-aim-of-this-book",
    "href": "index.html#the-aim-of-this-book",
    "title": "On the R-way to hell",
    "section": "The aim of this book",
    "text": "The aim of this book\nThe aim of this book is two-fold:\n\nintroduce you to R, a powerful and flexible interactive environment for statistical computing and research.\nintroduce you to (or reacquaint you with) statistical analysis done in R.\n\nR in itself is not difficult to learn, but as with learning any new language (spoken or computer) the initial learning curve can be steep and somewhat daunting. It is not intended to cover everything (neither with R not with statistics) but simply to help you climb the initial learning curve (potentially faster) and provide you with the basic skills (and confidence!) needed to start your own journey with R and with specific analysis.",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#multilingual-book",
    "href": "index.html#multilingual-book",
    "title": "On the R-way to hell",
    "section": "Multilingual book",
    "text": "Multilingual book\nThe book is provided as a multilingual book breaking that language barrier and potentially allow to facilitate the learn of R and its mainly english-speaking environment. We are always looking for volunteers to help developed the book further and add more languages to the growing list. Please contact us if you want to help\nOn the web version of the book, use  in the navigation bar to switch from one language to another. After switching to your preferred language, you can of course also download the pdf and epub versions in this language if you want to using .\nList of languages:\n\nenglish (publish but need polishing)\nfrench (in development, waiting for english to be polished)\nspanish (in development, waiting for english to be polished)",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-use-this-book",
    "href": "index.html#how-to-use-this-book",
    "title": "On the R-way to hell",
    "section": "How to use this book",
    "text": "How to use this book\n\nFor the best experience we recommend that you read the web version of this book which you can find https://biostats-uottawa.github.io/R/en.\nThe web version includes a navbar at the top of the page where you can toggle the sidebars on and off , search through the book , change the page color  and suggest revisions if you spot a typo or mistake . You can also download  a pdf and epub versions of the book.\nWe use a few typographical conventions throughout this book.\nR code and the resulting output are presented in code blocks in our book.\n\n2 + 2\n\n[1] 4\n\n\nFunctions in the text are shown with brackets at the end using code font, i.e. mean() or sd() etc.\nObjects are shown using code font without the round brackets, i.e. obj1, obj2 etc.\nR packages in the text are shown using code font and followed by the 📦 icon, i.e. tidyverse 📦.\nA series of actions required to access menu commands in RStudio or VSCode are identified as File -&gt; New File -&gt; R Script which translates to ‘click on the File menu, then click on New File and then select R Script’.\nWhen we refer to IDE (Integrated Development Environment software) later in the text we mean either RStudio of VScode.\nWhen we refer to .[Rq]md, we mean either R markdown (.Rmd) or Quarto (.qmd) documents and would generally talk of R markdown documents when referring to either .Rmd or .qmd files.\nThe manual tries to highlight some part of the text using the following boxes and icons.\n\n\n\n\n\n\nExercises\n\n\n\nStuff for you to do\n\n\n\n\n\n\n\n\nSolutions\n\n\n\nR code and explanations\n\n\n\n\n\n\n\n\nWarning\n\n\n\nwarnings\n\n\n\n\n\n\n\n\nImportant\n\n\n\nimportant points\n\n\n\n\n\n\n\n\nNote\n\n\n\nnotes",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#sec-who",
    "href": "index.html#sec-who",
    "title": "On the R-way to hell",
    "section": "Who are we ?",
    "text": "Who are we ?\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nJulien Martin is a Professor at the University of Ottawa working on Evolutionary Ecology and has discovered R with version 1.8.1 and teaches R since v2.4.0.\n\n\n: uOttawa page, lab page\n\n\n: jgamartin\n\n\n: juliengamartin",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#thanks",
    "href": "index.html#thanks",
    "title": "On the R-way to hell",
    "section": "Thanks",
    "text": "Thanks\nThis book started as a fork on github from the excellent An introduction to R book by Douglas, Roos, Mancini, Couto and Lusseau (Douglas 2023). It was forked on April 23rd, 2023 from Alexd106 github repository then modified and updated following my own needs and teaching perspective of R. This also a part of a multilingual R book project to improve equity and diversity. It started with a french translation and was/will be extended to many more languages.",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#image-credits",
    "href": "index.html#image-credits",
    "title": "On the R-way to hell",
    "section": "Image credits",
    "text": "Image credits\nPhotos, images and screenshots are from Julien Martin except when indicated in caption.\nCover image was generated via Nightcafe Ai Art generator. Favicon and hex sticker were created from the cover image.\nCover image can be seen here\n\n\n\n\n\n\nNote\n\n\n\nseveral screenshot are currently by Alex Douglas and are being redone to abide by the previous statement",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "On the R-way to hell",
    "section": "License",
    "text": "License\nI share this modified version of the [original book][https://intro2r.com/] under the license License Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International.\n\n\nLicense Creative Commons\n\nIf you teach R, feel free to use some or all of the content in this book to help support your own students. The only thing I ask is that you acknowledge the original source and authors. If you find this book useful or have any comments or suggestions I would love to hear from you (contact info).",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#citing-the-book",
    "href": "index.html#citing-the-book",
    "title": "On the R-way to hell",
    "section": "Citing the book",
    "text": "Citing the book\nJulien Martin. (2024). On the R-way to hell. A multilingual introduction to R book. Version: 0.6.0 (2024-04-22).DOI: 10.5281/zenodo.10929585",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "index.html#hex-sticker",
    "href": "index.html#hex-sticker",
    "title": "On the R-way to hell",
    "section": "Hex Sticker",
    "text": "Hex Sticker\n\n\n\n\n\n\n\n\nDouglas, A. 2023. An introduction to r.",
    "crumbs": [
      "Data",
      "Preface"
    ]
  },
  {
    "objectID": "01-start.html",
    "href": "01-start.html",
    "title": "1  Getting started",
    "section": "",
    "text": "Some R pointers\nGood luck and don’t forget to have fun.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#some-r-pointers",
    "href": "01-start.html#some-r-pointers",
    "title": "1  Getting started",
    "section": "",
    "text": "Use R often and use it regularly. This will help build and maintain all important momentum.\nLearning R is not a memory test. One of advantage of a scripting language is that you will always have your (well annotated) code to refer back to when you forget how to do something.\nYou don’t need to know everything about R to be productive.\nIf you get stuck, search online, it’s not cheating and writing a good search query is a skill in itself.\nIf you find yourself staring at code for hours trying to figure out why it’s not working then walk away for a few minutes.\nIn R there are many ways to tackle a particular problem. If your code does what you want it to do in a reasonable time and robustly then don’t worry about it.\nR is just a tool to help you answer your interesting questions. Don’t lose sight of what’s important - your research question(s) and your data. No amount of skill using R will help if your data collection is fundamentally flawed or your question vague.\nRecognize that there will be times when things will get a little tough or frustrating. Try to accept these periods as part of the natural process of learning a new skill (we’ve all been there) and remember, the time and energy you invest now will be more than payed back in the not too distant future.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#installation",
    "href": "01-start.html#installation",
    "title": "1  Getting started",
    "section": "\n1.1 Installation",
    "text": "1.1 Installation\n\n1.1.1 Installing R\nTo get up and running the first thing you need to do is install R. R is freely available for Windows, Mac and Linux operating systems from the Comprehensive R Archive Network (CRAN) website. For Windows and Mac users we suggest you download and install the pre-compiled binary versions. There are reasonably comprehensive instruction to install R for each OS (Windows,Mac or linux ).\nWhichever operating system you’re using, once you have installed R you need to check its working properly. The easiest way to do this is to start R by double clicking on the R icon (Windows or Mac) or by typing R into the Console (Linux). You should see the R Console and you should be able to type R commands into the Console after the command prompt &gt;. Try typing the following R code and then press enter:\n\nplot(1)\n\n\n\n\n\n\nFigure 1.1: Most amazing plot, just useful to test R\n\n\n\n\nA plot with a single point in the center should appear. If you see this, you’re good to go. If not then we suggest you make a note of any errors produced and then use your preferred search engine to troubleshoot.\n\n1.1.2 Installing an IDE\nWe strongly recommend to use an Integrated Development Environment (IDE) software to work with R. One simple and extremely popular IDE is RStudio. An alternative to RStudio is Visual Studio Code, or VSCode. An IDE can be thought of as an add-on to R which provides a more user-friendly interface, incorporating the R Console, a script editor and other useful functionality (like R markdown and Git Hub integration).\n\n\n\n\n\n\nCaution\n\n\n\nYou must install R before you install an IDE (see Section 1.1.1 for details).\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we refer to IDE later in the text we mean either RStudio of VScode\n\n\n\n1.1.2.1 RStudio\nRStudio is freely available for Windows, Mac and Linux operating systems and can be downloaded from the RStudio site. You should select the ‘RStudio Desktop’ version.\n\n1.1.2.2 VSCode\nVSCode is freely available for Windows, Mac and Linux operating systems and can be downloaded from the VS Code site.\nIn addition you need to install the R extension to VSCode. To make VSCode a true powerhouse for working with R we strongly recommend you to also install:\n\n\nradian: A modern R console that corrects many limitations of the official R terminal and supports many features such as syntax highlighting and auto-completion.\n\nVSCode-R-Debugger: A VS Code extension to support R debugging capabilities.\n\nhttpgd: An R package 📦 to provide a graphics device that asynchronously serves SVG graphics via HTTP and WebSockets.\n\n1.1.2.3 Alternatives to RStudio and VSCode\nRather than using an ‘all in one’ IDE many people choose to use R and a separate script editor to write and execute R code. If you’re not familiar with what a script editor is, you can think of it as a bit like a word processor but specifically designed for writing code. Happily, there are many script editors freely available so feel free to download and experiment until you find one you like. Some script editors are only available for certain operating systems and not all are specific to R. Suggestions for script editors are provided below. Which one you choose is up to you: one of the great things about R is that YOU get to choose how you want to use R.\n\n1.1.2.3.1 Advanced text editors\nA light yet efficient way to work with R is using advanced text editors such as:\n\n\nAtom (all operating systems)\n\nBBedit (Mac OS)\n\ngedit (Linux; comes with most Linux distributions)\n\nMacVim (Mac OS)\n\nNano (Linux)\n\nNotepad++ (Windows)\n\nSublime Text (all operating systems)\n\nvim and its extension NVim-R (Linux)\n\n1.1.2.3.2 Integrated development environments\nThese environments are more powerful than simple text editors, and are similar to RStudio:\n\n\nEmacs and its extension Emacs Speaks Statistics (all operating systems)\n\nRKWard (Linux)\n\nTinn-R (Windows)",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-orient",
    "href": "01-start.html#sec-orient",
    "title": "1  Getting started",
    "section": "\n1.2 IDE orientation",
    "text": "1.2 IDE orientation\n\n1.2.1 RStudio\nWhen you open R studio for the first time you should see the following layout (it might look slightly different on a Windows computer).\n\n\n\n\n\n\n\nFigure 1.2: R studio main window\n\n\n\n\nThe large window (aka pane) on the left is the Console window. The window on the top right is the Environment / History / Connections pane and the bottom right window is the Files / Plots / Packages / Help / Viewer window. We will discuss each of these panes in turn below. You can customise the location of each pane by clicking on the ‘Tools’ menu then selecting Global Options –&gt; Pane Layout. You can resize the panes by clicking and dragging the middle of the window borders in the direction you want. There are a plethora of other ways to customise RStudio.\n\n1.2.1.1 Console\nThe Console is the workhorse of R. This is where R evaluates all the code you write. You can type R code directly into the Console at the command line prompt, &gt;. For example, if you type 2 + 2 into the Console you should obtain the answer 4 (reassuringly). Don’t worry about the [1] at the start of the line for now.\n\n\n\n\n\n\n\nFigure 1.3: R studio console view\n\n\n\n\nHowever, once you start writing more R code this becomes rather cumbersome. Instead of typing R code directly into the Console a better approach is to create an R script. An R script is just a plain text file with a .R file extension which contains your lines of R code. These lines of code are then sourced into the R Console line by line. To create a new R script click on the ‘File’ menu then select New File –&gt; R Script.\n\n\n\n\n\n\n\nFigure 1.4: R studio creating a new script file\n\n\n\n\nNotice that you have a new window (called the Source pane) in the top left of RStudio and the Console is now in the bottom left position. The new window is a script editor and where you will write your code.\n\n\n\n\n\n\n\nFigure 1.5: R studio main view with a new script\n\n\n\n\nTo source your code from your script editor to the Console simply place your cursor on the line of code and then click on the ‘Run’ button in the top right of the script editor pane.\n\n\n\n\n\n\n\nFigure 1.6: R studio run button\n\n\n\n\nYou should see the result in the Console window. If clicking on the ‘Run’ button starts to become tiresome you can use the keyboard shortcut ‘ctrl + enter’ (on Windows and Linux) or ‘cmd + enter’ (on Mac). You can save your R scripts as a .R file by selecting the ‘File’ menu and clicking on save. Notice that the file name in the tab will turn red to remind you that you have unsaved changes. To open your R script in RStudio select the ‘File’ menu and then ‘Open File…’. Finally, its worth noting that although R scripts are saved with a .R extension they are actually just plain text files which can be opened with any text editor.\n\n1.2.1.2 Environment/History/Connections\nThe Environment / History / Connections window shows you lots of useful information. You can access each component by clicking on the appropriate tab in the pane.\n\nThe ‘Environment’ tab displays all the objects you have created in the current (global) environment. These objects can be things like data you have imported or functions you have written. Objects can be displayed as a List or in Grid format by selecting your choice from the drop down button on the top right of the window. If you’re in the Grid format you can remove objects from the environment by placing a tick in the empty box next to the object name and then click on the broom icon. There’s also an ‘Import Dataset’ button which will import data saved in a variety of file formats. However, we would suggest that you don’t use this approach to import your data as it’s not reproducible and therefore not robust (see Chapter 3 for more details).\nThe ‘History’ tab contains a list of all the commands you have entered into the R Console. You can search back through your history for the line of code you have forgotten, send selected code back to the Console or Source window. We usually never use this as we always refer back to our R script.\nThe ‘Connections’ tab allows you to connect to various data sources such as external databases.\n\n1.2.1.3 Files/Plots/Packages/Help/Viewer\n\nThe ‘Files’ tab lists all external files and directories in the current working directory on your computer. It works like file explorer (Windows) or Finder (Mac). You can open, copy, rename, move and delete files listed in the window.\nThe ‘Plots’ tab is where all the plots you create in R are displayed (unless you tell R otherwise). You can ‘zoom’ into the plots to make them larger using the magnifying glass button, and scroll back through previously created plots using the arrow buttons. There is also the option of exporting plots to an external file using the ‘Export’ drop down menu. Plots can be exported in various file formats such as jpeg, png, pdf, tiff or copied to the clipboard (although you are probably better off using the appropriate R functions to do this - see [Chapter 4 for more details).\nThe ‘Packages’ tab lists all of the packages that you have installed on your computer. You can also install new packages and update existing packages by clicking on the ‘Install’ and ‘Update’ buttons respectively.\nThe ‘Help’ tab displays the R help documentation for any function. We will go over how to view the help files and how to search for help in Chapter 2).\nThe ‘Viewer’ tab displays local web content such as web graphics generated by some packages.\n\n1.2.2 VSCode\n\n\n\n\n\n\n\nFigure 1.7: VSCode window overview\n\n\n\n\n\n1.2.2.1 Left panel\nContains :\n\nFile manager and file outline\nR support including R environment/ R search / R help / install packages\nGithub interaction\n\n\n\n\n\n\n\n\n\n\n(a) file pane\n\n\n\n\n\n\n\n\n\n(b) git pane\n\n\n\n\n\n\n\n\n\n(c) R pane\n\n\n\n\n\n\nFigure 1.8: VS Code left panel\n\n\n\n1.2.2.2 Editor tabs\nIncludes:\n\nplot panel (with history and navigation)\nedition of scripts\npreview panels\n\n\n\n\n\n\n\n\nFigure 1.9: VSCode editor tabs and preview panels\n\n\n\n\n\n1.2.2.3 Terminal window\nContains:\n\nthe terminal allowing to have an R session or any other type of terminals needed (bash/tmux/). It can be split and run multiple sessions at the same time\na problems pane highlighting both grammar and coding problems\n\n\n\n\n\n\n\n\nFigure 1.10: VSCode terminal window",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-work-d",
    "href": "01-start.html#sec-work-d",
    "title": "1  Getting started",
    "section": "\n1.3 Working directories",
    "text": "1.3 Working directories\nThe working directory is the default location where R will look for files you want to load and where it will put any files you save. One of the great things about using RStudio Projects is that when you open a project it will automatically set your working directory to the appropriate location. You can check the file path of your working directory by using either getwd() or here() functions.\n\ngetwd()\n\n[1] \"/home/julien/Documents/courses/biostats/R_way/lang/en\"\n\n\nIn the example above, the working directory is a folder called ‘R_way’ which is a subfolder of “biostats’ in the ‘courses’ folder which in turn is in a ‘Documents’ folder located in the ‘julien’ folder which itself is in the ‘home’ folder. On a Windows based computer our working directory would also include a drive letter (i.e. C:\\home\\julien\\Documents\\courses\\biostats\\R_way).\nIf you weren’t using an IDE then you would have to set your working directory using the setwd() function at the start of every R script (something we did for many years).\n\nsetwd(\"/home/julien/Documents/courses/biostats/R_way/\")\n\nHowever, the problem with setwd() is that it uses an absolute file path which is specific to the computer you are working on. If you want to send your script to someone else (or if you’re working on a different computer) this absolute file path is not going to work on your friend/colleagues computer as their directory configuration will be different (you are unlikely to have a directory structure /home/julien/Documents/courses/biostats/ on your computer). This results in a project that is not self-contained and not easily portable. IDEs solves this problem by allowing you to use relative file paths which are relative to the Root project directory. The Root project directory is just the directory that contains the .Rproj file in Rstudio (first_project.Rproj in our case) or the base folder of your workspace in VScode. If you want to share your analysis with someone else, all you need to do is copy the entire project directory and send to your to your collaborator. They would then just need to open the project file and any R scripts that contain references to relative file paths will just work. For example, let’s say that you’ve created a subdirectory called data in your Root project directory that contains a csv delimited datile called mydata.csv (we will cover directory structures below in Section 1.4). To import this datile in an RStudio project using the read.csv() function (don’t worry about this now, we will cover this in much more detail in Chapter 3) all you need to include in your R script is\ndat &lt;- read.csv(\"data/mydata.csv\")\nBecause the file path data/mydata.csv is relative to the project directory it doesn’t matter where you collaborator saves the project directory on their computer it will still work.\nIf you weren’t using an RStudio project or VScode workspace then you would need to either set the working directory providing the full path to your directory or specify the full path of the data file. Neither option would be reproducible on other computers.\nsetwd(\"/home/julien/Documents/courses/biostats/R_way\")\n\ndat &lt;- read.csv(\"data/mydata.csv\")\nor\ndat &lt;- read.csv(\"/home/julien/Documents/courses/biostats/R_way/data/mydata.csv\")\nFor those of you who want to take the notion of relative file paths a step further, take a look at the here() function in the here package. The here() function allows you to build file paths for any file relative to the project root directory that are also operating system agnostic (works on a Mac, Windows or Linux machine). For example, to import our mydata.csv file from the data directory just use:\nlibrary(here) # you may need to install the here package first\ndat &lt;- read.csv(here(\"data\", \"mydata.csv\"))",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-dir-struc",
    "href": "01-start.html#sec-dir-struc",
    "title": "1  Getting started",
    "section": "\n1.4 Directory structure",
    "text": "1.4 Directory structure\nIn addition to using RStudio Projects, it’s also really good practice to structure your working directory in a consistent and logical way to help both you and your collaborators. We frequently use the following directory structure in our R based projects.\n\n\n\n\n\n\nroot\nrootdot01\nroot-&gt;dot01\ndot1\nroot-&gt;dot1\ndata\ndatadot21\ndata-&gt;dot21\nfunctions\nfunctionsdot22\nfunctions-&gt;dot22\noutputs\noutputsdot23\noutputs-&gt;dot23\nscripts\nscriptsdot24\nscripts-&gt;dot24\nwd\nyour working directoryLOT1\nraw processed metadataLOT2\nR functionsLOT4\nanalysis scripts R markdown documentsLOT3\npdf html figuresdot01-&gt;wd\ndot1-&gt;data\ndot2\ndot1-&gt;dot2\ndot2-&gt;functions\ndot3\ndot2-&gt;dot3\ndot3-&gt;outputs\ndot4\ndot3-&gt;dot4\ndot4-&gt;scripts\ndot21-&gt;LOT1\ndot22-&gt;LOT2\ndot23-&gt;LOT3\ndot24-&gt;LOT4\n\n\n\n\nFigure 1.11: Recommended directory structure for analysis with R\n\n\n\n\nIn our working directory we have the following directories:\n\nRoot - This is your project directory containing your .Rproj file. We tend to keep all the R scripts or [Rq]md document necessary for the analysis / report in this root folder or in the scripts folder when there are too many.\ndata - We store all our data in this directory. The subdirectory called data contains raw data files and only raw data files. These files should be treated as read only and should not be changed in any way. If you need to process/clean/modify your data do this in R (not MS Excel) as you can document (and justify) any changes made. Any processed data should be saved to a separate file and stored in the processed_data subdirectory. Information about data collection methods, details of data download and any other useful metadata should be saved in a text document (see README text files below) in the metadata subdirectory.\nfunctions - This is an optional directory where we save all of the custom R functions we’ve written for the current analysis. These can then be sourced into R using the source() function.\nscripts - An optional directory where we save our R markdown documents and/or the main R scripts we have written for the current project are saved here if not in the root folder.\noutput - Outputs from our R scripts such as plots, HTML files and data summaries are saved in this directory. This helps us and our collaborators distinguish what files are outputs and which are source files.\n\nOf course, the structure described above is just what works for us most of the time and should be viewed as a starting point for your own needs. We tend to have a fairly consistent directory structure across our projects as this allows us to quickly orientate ourselves when we return to a project after a while. Having said that, different projects will have different requirements so we happily add and remove directories as required.\nYou can create your directory structure using Windows Explorer (or Finder on a Mac) or within your IDE by clicking on the ‘New folder’ button in the ‘Files’ pane.\nAn alternative approach is to use the dir.create() functions in the R Console.\n# create directory called 'data'\ndir.create(\"data\")",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-rsprojs",
    "href": "01-start.html#sec-rsprojs",
    "title": "1  Getting started",
    "section": "\n1.5 Projects organisation",
    "text": "1.5 Projects organisation\nAs with most things in life, when it comes to dealing with data and data analysis things are so much simpler if you’re organized. Clear project organisation makes it easier for both you (especially the future you) and your collaborators to make sense of what you’ve done. There’s nothing more frustrating than coming back to a project months (sometimes years) later and have to spend days (or weeks) figuring out where everything is, what you did and why you did it. A well documented project that has a consistent and logical structure increases the likelihood that you can pick up where you left off with minimal fuss no matter how much time has passed. In addition, it’s much easier to write code to automate tasks when files are well organized and are sensibly named. This is even more relevant nowadays as it’s never been easier to collect vast amounts of data which can be saved across 1000’s or even 100,000’s of separate data files. Lastly, having a well organized project reduces the risk of introducing bugs or errors into your workflow and if they do occur (which inevitably they will at some point), it makes it easier to track down these errors and deal with them efficiently.\nThere are also a few simple steps you can take right at the start of any project to help keep things shipshape.\nA great way of keeping things organized is to use RStudio Projects or VSCode workspaces, referred after as project. A project keeps all of your R scripts, R markdown documents, R functions and data together in one place. The nice thing about project is that each has its own directory, history and source documents so different analyses that you are working on are kept completely separate from each other. This means that you can very easily switch between projects without fear of them interfering with each other.\n\n1.5.1 RStudio\nTo create a project, open RStudio and select File -&gt; New Project... from the menu. You can create either an entirely new project, a project from an existing directory or a version controlled project (see the Chapter 7 for further details about this). In this Chapter we will create a project in a new directory.\n\n\n\n\n\n\n\nFigure 1.12: R Studio creating a Project step 1\n\n\n\n\nYou can also create a new project by clicking on the ‘Project’ button in the top right of RStudio and selecting ‘New Project…’\n\n\n\n\n\n\n\nFigure 1.13: R Studio creating a Project step 2\n\n\n\n\nIn the next window select ‘New Project’.\n\n\n\n\n\n\n\nFigure 1.14: R Studio creating a Project step 3\n\n\n\n\nNow enter the name of the directory you want to create in the ‘Directory name:’ field (we’ll call it first_project for this Chapter). If you want to change the location of the directory on your computer click the ‘Browse…’ button and navigate to where you would like to create the directory. We always tick the ‘Open in new session’ box as well. Finally, hit the ‘Create Project’ to create the new project.\n\n\n\n\n\n\n\nFigure 1.15: R Studio creating a Project step 4\n\n\n\n\nOnce your new project has been created you will now have a new folder on your computer that contains an RStudio project file called first_project.Rproj. This .Rproj file contains various project options (but you shouldn’t really interact with it) and can also be used as a shortcut for opening the project directly from the file system (just double click on it). You can check this out in the ‘Files’ tab in RStudio (or in Finder if you’re on a Mac or File Explorer in Windows).\n\n\n\n\n\n\n\nFigure 1.16: R Studio creating a Project final step\n\n\n\n\nThe last thing we suggest you do is select Tools -&gt; Project Options... from the menu. Click on the ‘General’ tab on the left hand side and then change the values for ‘Restore .RData into workspace at startup’ and ‘Save workspace to .RData on exit’ from ‘Default’ to ‘No’. This ensures that every time you open your project you start with a clean R session. You don’t have to do this (many people don’t) but we prefer to start with a completely clean workspace whenever we open our projects to avoid any potential conflicts with things we have done in previous sessions (sometimes leading to surprising results and headaches figuring out the problem). The downside to this is that you will need to rerun your R code every time you open your project.\n\n\n\n\n\n\n\nFigure 1.17: R Studio creating a Project changing options\n\n\n\n\nNow that you have an RStudio project set up you can start creating R scripts (or R markdown /Quarto documents, see Chapter 6) or whatever you need to complete you project. All of the R scripts will now be contained within the RStudio project and saved in the project folder.\n\n1.5.2 VSCode\nworkspace are similar to RStudio projects. You however need to create a new folder with a R file (or text file) and save as workspace.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-file_names",
    "href": "01-start.html#sec-file_names",
    "title": "1  Getting started",
    "section": "\n1.6 File names",
    "text": "1.6 File names\nWhat you name your files matters more than you might think. Naming files is also more difficult than you think. The key requirement for a ‘good’ file name is that it’s informative whilst also being relatively short. This is not always an easy compromise and often requires some thought. Ideally you should try to avoid the following!\n\n\n\n\n\n\n\nFigure 1.18: File renaming song (source:https://xkcd.com/1459/)\n\n\n\n\nAlthough there’s not really a recognized standard approach to naming files (actually there is, just not everyone uses it), there are a couple of things to bear in mind.\n\nAvoid using spaces in file names by replacing them with underscores or hyphens. Why does this matter? One reason is that some command line software (especially many bioinformatic tools) won’t recognise a file name with a space and you’ll have to go through all sorts of shenanigans using escape characters to make sure spaces are handled correctly. Even if you don’t think you will ever use command line software you may be doing so indirectly. Take R markdown for example, if you want to render an R markdown document to pdf using the rmarkdown 📦 package you will actually be using a command line \\(\\LaTeX\\) engine under the hood. Another good reason not to use spaces in file names is that it makes searching for file names (or parts of file names) using regular expressions in R (or any other language) much more difficult.\nAvoid using special characters (i.e. @£$%^&*(:/)) in your file names.\nIf you are versioning your files with sequential numbers (i.e. file1, file2, file3 …). If you plan to have more than 9 files you should use 01, 02, 03, …, 10 as this will ensure the files are listed in the correct order. If you plan to have more than 99 files then use 001, 002, 003, …\nFor dates, use the ISO 8601 format YYYY-MM-DD (or YYYYMMDD) to ensure your files are listed in proper chronological order.\nNever use the word final in any file name - it extremely rarely is!\n\nWhatever file naming convention you decide to use, try to adopt early, stick with it and be consistent.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#sec-proj_doc",
    "href": "01-start.html#sec-proj_doc",
    "title": "1  Getting started",
    "section": "\n1.7 Script documentation",
    "text": "1.7 Script documentation\nA quick note or two about writing R code and creating R scripts. Unless you’re doing something really quick and dirty we suggest that you always write your R code as an R script. R scripts are what make R so useful. Not only do you have a complete record of your analysis, from data manipulation, visualisation and statistical analysis, you can also share this code (and data) with friends, colleagues and importantly when you submit and publish your research to a journal. With this in mind, make sure you include in your R script all the information required to make your work reproducible (author names, dates, sampling design etc). This information could be included as a series of comments # or, even better, by mixing executable code with narrative into an R markdown document (Chapter 6). It’s also good practice to include the output of the sessionInfo() function at the end of any script which prints the R version, details of the operating system and also loaded packages. A really good alternative is to use the session_info() function from the xfun 📦 package for a more concise summary of our session environment.\nHere’s an example of including meta-information at the start of an R script\n# Title: Time series analysis of lasagna consumption\n\n# Purpose : This script performs a time series analyses on\n#           lasagna meals kids want to have each week.\n#           Data consists of counts of (dreamed) lasagna meals per week\n#           collected from 24 kids at the \"Food-dreaming\" school\n#           between 2042 and 2056.\n\n# data file: lasagna_dreams.csv\n\n# Author: A. Stomach\n# Contact details: a.stomach@food.uni.com\n\n# Date script created: Fri Mar 29 17:06:44 2010 -----------\n# Date script last modified: Thu Dec 12 16:07:12 2019 ----\n\n# package dependencies\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nprint(\"put your lovely R code here\")\n\n# good practice to include session information\n\nxfun::session_info()\nThis is just one example and there are no hard and fast rules so feel free to develop a system that works for you. A really useful shortcut in RStudio is to automatically include a time and date stamp in your R script. To do this, write ts where you want to insert your time stamp in your R script and then press the ‘shift + tab’ keys. RStudio will convert ts into the current date and time and also automatically comment out this line with a #. Another really useful RStudio shortcut is to comment out multiple lines in your script with a # symbol. To do this, highlight the lines of text you want to comment and then press ‘ctrl + shift + c’ (or ‘cmd + shift + c’ on a mac). To uncomment the lines just use ‘ctrl + shift + c’ again.\nIn addition to including metadata in your R scripts it’s also common practice to create a separate text file to record important information. By convention these text files are named README. We often include a README file in the directory where we keep our raw data. In this file we include details about when data were collected (or downloaded), how data were collected, information about specialised equipment, preservation methods, type and version of any machines used (i.e. sequencing equipment) etc. You can create a README file for your project in RStudio by clicking on the File -&gt; New File -&gt; Text File menu.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#r-style-guide",
    "href": "01-start.html#r-style-guide",
    "title": "1  Getting started",
    "section": "\n1.8 R style guide",
    "text": "1.8 R style guide\nHow you write your code is more or less up to you although your goal should be to make it as easy to read as possible (for you and others). Whilst there are no rules (and no code police), we encourage you to get into the habit of writing readable R code by adopting a particular style. We suggest that you follow Google’s R style guide whenever possible. This style guide will help you decide where to use spaces, how to indent code and how to use square [ ] and curly { } brackets amongst other things.\nTo help you with code formatting:\n\nVSCode there is an embedded formatter in the R extension for VSCode. You can just use the keyboard shortcut to reformat the code nicely and automatically.\nRStudio you can install the styler 📦 package which includes an RStudio add-in to allow you to automatically restyle selected code (or entire files and projects) with the click of your mouse. You can find more information about the styler 📦 package including how to install here. Once installed, you can highlight the code you want to restyle, click on the ‘Addins’ button at the top of RStudio and select the ‘Style Selection’ option. Here is an example of poorly formatted R code\n\n\n\n\n\n\n\n\nFigure 1.19: Poorly styled code\n\n\n\n\nNow highlight the code and use the styler 📦 package to reformat\n\n\n\n\n\n\n\nFigure 1.20: Styling code with styler\n\n\n\n\nTo produce some nicely formatted code\n\n\n\n\n\n\n\nFigure 1.21: Nicely styled code",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#backing-up-projects",
    "href": "01-start.html#backing-up-projects",
    "title": "1  Getting started",
    "section": "\n1.9 Backing up projects",
    "text": "1.9 Backing up projects\nDon’t be that person who loses hard won (and often expensive) data and analyses. Don’t be that person who thinks it’ll never happen to me - it will! Always think of the absolute worst case scenario, something that makes you wake up in a cold sweat at night, and do all you can to make sure this never happens. Just to be clear, if you’re relying on copying your precious files to an external hard disk or USB stick this is NOT an effective backup strategy. These things go wrong all the time as you lob them into your rucksack or ‘bag for life’ and then lug them between your office and home. Even if you do leave them plugged into your computer what happens when the building burns down (we did say worst case!)?\nIdeally, your backups should be offsite and incremental. Happily there are numerous options for backing up your files. The first place to look is in your own institute. Most (all?) Universities have some form of network based storage that should be easily accessible and is also underpinned by a comprehensive disaster recovery plan. Other options include cloud based services such as Google Drive and Dropbox (to name but a few), but make sure you’re not storing sensitive data on these services and are comfortable with the often eye watering privacy policies.\nWhilst these services are pretty good at storing files, they don’t really help with incremental backups. Finding previous versions of files often involves spending inordinate amounts of time trawling through multiple files named ‘final.doc’, ‘final_v2.doc’ and ‘final_usethisone.doc’ etc until you find the one you were looking for. The best way we know for both backing up files and managing different versions of files is to use Git and GitHub. To find out more about how you can use RStudio, Git and GitHub together see Chapter 7.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "01-start.html#citing-r-and-r-packages",
    "href": "01-start.html#citing-r-and-r-packages",
    "title": "1  Getting started",
    "section": "\n1.10 Citing R and R packages",
    "text": "1.10 Citing R and R packages\nMany people have invested huge amounts of time and energy making R the great piece of software you’re now using. If you use R in your work (and we hope you do) please remember to give appropriate credit by citing not only R but also all the packages you used. To get the most up to date citation for R you can use the citation() function.\n\ncitation()\n\nTo cite R in publications use:\n\n  R Core Team (2024). _R: A Language and Environment for Statistical\n  Computing_. R Foundation for Statistical Computing, Vienna, Austria.\n  &lt;https://www.R-project.org/&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {R: A Language and Environment for Statistical Computing},\n    author = {{R Core Team}},\n    organization = {R Foundation for Statistical Computing},\n    address = {Vienna, Austria},\n    year = {2024},\n    url = {https://www.R-project.org/},\n  }\n\nWe have invested a lot of time and effort in creating R, please cite it\nwhen using it for data analysis. See also 'citation(\"pkgname\")' for\nciting R packages.\n\n\nIf you want to cite a particular package you’ve used for your data analysis, you can also use the citation() function to get the info.\n\ncitation(package = \"here\")\n\nTo cite package 'here' in publications use:\n\n  Müller K (2020). _here: A Simpler Way to Find Your Files_. R package\n  version 1.0.1, &lt;https://CRAN.R-project.org/package=here&gt;.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {here: A Simpler Way to Find Your Files},\n    author = {Kirill Müller},\n    year = {2020},\n    note = {R package version 1.0.1},\n    url = {https://CRAN.R-project.org/package=here},\n  }\n\n\nIn our view the most useful tool for citation is the package grateful 📦 which allow you to generate the citing information in a file, as well as creating either a sentence or a table citing all packages used. This should become the standard in any manuscript honestly. See Table 1 for an example output produced with grateful.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Getting started</span>"
    ]
  },
  {
    "objectID": "02-basics.html",
    "href": "02-basics.html",
    "title": "2  Some R basics",
    "section": "",
    "text": "2.1 Important considerations\nWe provide screenshot of RStudio but everything is really similar when using VSCode.\nBefore we continue, here are a few things to bear in mind as you work through this Chapter:",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#important-considerations",
    "href": "02-basics.html#important-considerations",
    "title": "2  Some R basics",
    "section": "",
    "text": "R is case sensitive i.e. A is not the same as a and anova is not the same as Anova.\nAnything that follows a # symbol is interpreted as a comment and ignored by R. Comments should be used liberally throughout your code for both your own information and also to help your collaborators. Writing comments is a bit of an art and something that you will become more adept at as your experience grows.\nIn R, commands are generally separated by a new line. You can also use a semicolon ; to separate your commands but we strongly recommend to avoid using it.\nIf a continuation prompt + appears in the console after you execute your code this means that you haven’t completed your code correctly. This often happens if you forget to close a bracket and is especially common when nested brackets are used ((((some command))). Just finish the command on the new line and fix the typo or hit escape on your keyboard (see point below) and fix.\nIn general, R is fairly tolerant of extra spaces inserted into your code, in fact using spaces is actively encouraged. However, spaces should not be inserted into operators i.e. &lt;- should not read &lt; - (note the space). See the style guide for advice on where to place spaces to make your code more readable.\nIf your console ‘hangs’ and becomes unresponsive after running a command you can often get yourself out of trouble by pressing the escape key (esc) on your keyboard or clicking on the stop icon in the top right of your console. This will terminate most current operations.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#first-step-in-the-console",
    "href": "02-basics.html#first-step-in-the-console",
    "title": "2  Some R basics",
    "section": "\n2.2 First step in the console",
    "text": "2.2 First step in the console\nIn Chapter 1, we learned about the R Console and creating scripts and Projects. We also saw how you write your R code in a script and then source this code into the console to get it to run (if you’ve forgotten how to do this, pop back to the console section (-Section 1.2.1.1) to refresh your memory). Writing your code in a script means that you’ll always have a permanent record of everything you’ve done (provided you save your script) and also allows you to make loads of comments to remind your future self what you’ve done. So, while you’re working through this Chapter we suggest that you create a new script (or RStudio Project) to write your code as you follow along.\nAs we saw in Chapter 1, at a basic level we can use R much as you would use a calculator. We can type an arithmetic expression into our script, then source it into the console and receive a result. For example, if we type the expression 1 + 1 and then source this line of code we get the answer 2 (😃!)\n\n1 + 1\n\n[1] 2\n\n\nThe [1] in front of the result tells you that the observation number at the beginning of the line is the first observation. This is not much help in this example, but can be quite useful when printing results with multiple lines (we’ll see an example below). The other obvious arithmetic operators are -, *, / for subtraction, multiplication and division respectively. Matrix multiplication operator is %*%. R follows the usual mathematical convention of order of operations. For example, the expression 2 + 3 * 4 is interpreted to have the value 2 + (3 * 4) = 14, not (2 + 3) * 4 = 20. There are a huge range of mathematical functions in R, some of the most useful include; log(), log10(), exp(), sqrt().\n\nlog(1) # logarithm to base e\n\n[1] 0\n\nlog10(1) # logarithm to base 10\n\n[1] 0\n\nexp(1) # natural antilog\n\n[1] 2.718282\n\nsqrt(4) # square root\n\n[1] 2\n\n4^2 # 4 to the power of 2\n\n[1] 16\n\npi # not a function but useful\n\n[1] 3.141593\n\n\nIt’s important to realize that when you run code as we’ve done above, the result of the code (or value) is only displayed in the console. Whilst this can sometimes be useful it is usually much more practical to store the value(s) in aN object.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#objects-in-r",
    "href": "02-basics.html#objects-in-r",
    "title": "2  Some R basics",
    "section": "\n2.3 Objects in R",
    "text": "2.3 Objects in R\nAt the heart of almost everything you will do (or ever likely to do) in R is the concept that everything in R is an object. These objects can be almost anything, from a single number or character string (like a word) to highly complex structures like the output of a plot, a summary of your statistical analysis or a set of R commands that perform a specific task. Understanding how you create objects and assign values to objects is key to understanding R.\n\n2.3.1 Creating objects\nTo create an object we simply give the object a name. We can then assign a value to this object using the assignment operator &lt;- (sometimes called the gets operator). The assignment operator is a composite symbol comprised of a ‘less than’ symbol &lt; and a hyphen - .\n\nmy_obj &lt;- 32\n\nIn the code above, we created an object called my_obj and assigned it a value of the number 32 using the assignment operator (in our head we always read this as ‘my_obj is 32’). You can also use = instead of &lt;- to assign values but this is bad practice since it can lead to confusion later on when programming in R (see Chapter 5) and we would discourage you from using this notation.\nTo view the value of the object you simply type the name of the object\n\nmy_obj\n\n[1] 32\n\n\nNow that we’ve created this object, R knows all about it and will keep track of it during this current R session. All of the objects you create will be stored in the current workspace and you can view all the objects in your workspace in RStudio by clicking on the ‘Environment’ tab in the top right hand pane.\n\n\n\n\n\n\n\n\nIf you click on the down arrow on the ‘List’ icon in the same pane and change to ‘Grid’ view RStudio will show you a summary of the objects including the type (numeric - it’s a number), the length (only one value in this object), its ‘physical’ size and its value (48 in this case). In VSCode, go on the R extension pane, and you can obtain the same information.\n\n\n\n\n\n\n\n\nThere are many different types of values that you can assign to an object. For example\n\nmy_obj2 &lt;- \"R is cool\"\n\nHere we have created an object called my_obj2 and assigned it a value of R is cool which is a character string. Notice that we have enclosed the string in quotes. If you forget to use the quotes you will receive an error message.\nOur workspace now contains both objects we’ve created so far with my_obj2 listed as type character.\n\n\n\n\n\n\n\n\nTo change the value of an existing object we simply reassign a new value to it. For example, to change the value of my_obj2 from \"R is cool\" to the number 1024\n\nmy_obj2 &lt;- 1024\n\nNotice that the Type has changed to numeric and the value has changed to 1024 in the environment\n\n\n\n\n\n\n\n\nOnce we have created a few objects, we can do stuff with our objects. For example, the following code creates a new object my_obj3 and assigns it the value of my_obj added to my_obj2 which is 1072 (48 + 1024 = 1072).\n\nmy_obj3 &lt;- my_obj + my_obj2\nmy_obj3\n\n[1] 1056\n\n\nNotice that to display the value of my_obj3 we also need to write the object’s name. The above code works because the values of both my_obj and my_obj2 are numeric (i.e. a number). If you try to do this with objects with character values (character class) you will receive an error\n\nchar_obj &lt;- \"hello\"\nchar_obj2 &lt;- \"world!\"\nchar_obj3 &lt;- char_obj + char_obj2\n# Error in char_obj+char_obj2:non-numeric argument to binary operator\n\nThe error message is essentially telling you that either one or both of the objects char_obj and char_obj2 is not a number and therefore cannot be added together.\nWhen you first start learning R, dealing with errors and warnings can be frustrating as they’re often difficult to understand (what’s an argument? what’s a binary operator?). One way to find out more information about a particular error is to search for a generalised version of the error message. For the above error try searching ‘non-numeric argument to binary operator error + r’ or even ‘common r error messages’.\nAnother error message that you’ll get quite a lot when you first start using R is Error: object 'XXX' not found. As an example, take a look at the code below\nmy_obj &lt;- 48\nmy_obj4 &lt;- my_obj + no_obj\n# Error: object 'no_obj' not found\nR returns an error message because we haven’t created (defined) the object no_obj yet. Another clue that there’s a problem with this code is that, if you check your environment, you’ll see that object my_obj4 has not been created.\n\n2.3.2 Naming objects\nNaming your objects is one of the most difficult things you will do in R. Ideally your object names should be kept both short and informative which is not always easy. If you need to create objects with multiple words in their name then use either an underscore or a dot between words or capitalise the different words. We prefer the underscore format and never include uppercase in names (called snake_case)\noutput_summary &lt;- \"my analysis\" # recommended#\noutput.summary &lt;- \"my analysis\"\noutputSummary &lt;- \"my analysis\"\nThere are also a few limitations when it come to giving objects names. An object name cannot start with a number or a dot followed by a number (i.e. 2my_variable or .2my_variable). You should also avoid using non-alphanumeric characters in your object names (i.e. &, ^, /, ! etc). In addition, make sure you don’t name your objects with reserved words (i.e. TRUE, NA) and it’s never a good idea to give your object the same name as a built-in function. One that crops up more times than we can remember is\ndata &lt;- read.table(\"mydatafile\", header = TRUE)\nYes, data() is a function in R to load or list available data sets from packages.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#sec-funcs",
    "href": "02-basics.html#sec-funcs",
    "title": "2  Some R basics",
    "section": "\n2.4 Using functions in R",
    "text": "2.4 Using functions in R\nUp until now we’ve been creating simple objects by directly assigning a single value to an object. It’s very likely that you’ll soon want to progress to creating more complicated objects as your R experience grows and the complexity of your tasks increase. Happily, R has a multitude of functions to help you do this. You can think of a function as an object which contains a series of instructions to perform a specific task. The base installation of R comes with many functions already defined or you can increase the power of R by installing one of the 10,000’s of packages now available. Once you get a bit more experience with using R you may want to define your own functions to perform tasks that are specific to your goals (more about this in Chapter 5).\nThe first function we will learn about is the c() function. The c() function is short for concatenate and we use it to join together a series of values and store them in a data structure called a vector (more on vectors in Chapter 3).\n\nmy_vec &lt;- c(2, 3, 1, 6, 4, 3, 3, 7)\n\nIn the code above we’ve created an object called my_vec and assigned it a value using the function c(). There are a couple of really important points to note here. Firstly, when you use a function in R, the function name is always followed by a pair of round brackets even if there’s nothing contained between the brackets. Secondly, the argument(s) of a function are placed inside the round brackets and are separated by commas. You can think of an argument as way of customising the use or behaviour of a function. In the example above, the arguments are the numbers we want to concatenate. Finally, one of the tricky things when you first start using R is to know which function to use for a particular task and how to use it. Thankfully each function will always have a help document associated with it which will explain how to use the function (more on this later Section 2.6) and a quick web search will also usually help you out.\nTo examine the value of our new object we can simply type out the name of the object as we did before\n\nmy_vec\n\n[1] 2 3 1 6 4 3 3 7\n\n\nNow that we’ve created a vector we can use other functions to do useful stuff with this object. For example, we can calculate the mean, variance, standard deviation and number of elements in our vector by using the mean(), var(), sd() and length() functions\n\nmean(my_vec) # returns the mean of my_vec\n\n[1] 3.625\n\nvar(my_vec) # returns the variance of my_vec\n\n[1] 3.982143\n\nsd(my_vec) # returns the standard deviation of my_vec\n\n[1] 1.995531\n\nlength(my_vec) # returns the number of elements in my_vec\n\n[1] 8\n\n\nIf we wanted to use any of these values later on in our analysis we can just assign the resulting value to another object\n\nvec_mean &lt;- mean(my_vec) # returns the mean of my_vec\nvec_mean\n\n[1] 3.625\n\n\nSometimes it can be useful to create a vector that contains a regular sequence of values in steps of one. Here we can make use of a shortcut using the : symbol.\n\nmy_seq &lt;- 1:10 # create regular sequence\nmy_seq\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nmy_seq2 &lt;- 10:1 # in decending order\nmy_seq2\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nOther useful functions for generating vectors of sequences include the seq() and rep() functions. For example, to generate a sequence from 1 to 5 in steps of 0.5\n\nmy_seq2 &lt;- seq(from = 1, to = 5, by = 0.5)\nmy_seq2\n\n[1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nHere we’ve used the arguments from = and to = to define the limits of the sequence and the by = argument to specify the increment of the sequence. Play around with other values for these arguments to see their effect.\nThe rep() function allows you to replicate (repeat) values a specified number of times. To repeat the value 2, 10 times\n\nmy_seq3 &lt;- rep(2, times = 10) # repeats 2, 10 times\nmy_seq3\n\n [1] 2 2 2 2 2 2 2 2 2 2\n\n\nYou can also repeat non-numeric values\n\nmy_seq4 &lt;- rep(\"abc\", times = 3) # repeats ‘abc’ 3 times\nmy_seq4\n\n[1] \"abc\" \"abc\" \"abc\"\n\n\nor each element of a series\n\nmy_seq5 &lt;- rep(1:5, times = 3) # repeats the series 1 to\n# 5, 3 times\nmy_seq5\n\n [1] 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n\n\nor elements of a series\n\nmy_seq6 &lt;- rep(1:5, each = 3) # repeats each element of the\n# series 3 times\nmy_seq6\n\n [1] 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5\n\n\nWe can also repeat a non-sequential series\n\nmy_seq7 &lt;- rep(c(3, 1, 10, 7), each = 3) # repeats each\n# element of the\n# series 3 times\nmy_seq7\n\n [1]  3  3  3  1  1  1 10 10 10  7  7  7\n\n\nNote in the code above how we’ve used the c() function inside the rep() function. Nesting functions allows us to build quite complex commands within a single line of code and is a very common practice when using R. However, care needs to be taken as too many nested functions can make your code quite difficult for others to understand (or yourself some time in the future!). We could rewrite the code above to explicitly separate the two different steps to generate our vector. Either approach will give the same result, you just need to use your own judgement as to which is more readable.\n\nin_vec &lt;- c(3, 1, 10, 7)\nmy_seq7 &lt;- rep(in_vec, each = 3) # repeats each element of\n# the series 3 times\nmy_seq7\n\n [1]  3  3  3  1  1  1 10 10 10  7  7  7",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#sec-vectors",
    "href": "02-basics.html#sec-vectors",
    "title": "2  Some R basics",
    "section": "\n2.5 Working with vectors",
    "text": "2.5 Working with vectors\nManipulating, summarising and sorting data using R is an important skill to master but one which many people find a little confusing at first. We’ll go through a few simple examples here using vectors to illustrate some important concepts but will build on this in much more detail in Chapter 3 where we will look at more complicated (and useful) data structures.\n\n2.5.1 Extracting elements\nTo extract (also known as indexing or subscripting) one or more values (more generally known as elements) from a vector we use the square bracket [ ] notation. The general approach is to name the object you wish to extract from, then a set of square brackets with an index of the element you wish to extract contained within the square brackets. This index can be a position or the result of a logical test.\nPositional index\nTo extract elements based on their position we simply write the position inside the [ ]. For example, to extract the 3rd value of my_vec\n\nmy_vec # remind ourselves what my_vec looks like\n\n[1] 2 3 1 6 4 3 3 7\n\nmy_vec[3] # extract the 3rd value\n\n[1] 1\n\n# if you want to store this value in another object\nval_3 &lt;- my_vec[3]\nval_3\n\n[1] 1\n\n\nNote that the positional index starts at 1 rather than 0 like some other other programming languages (i.e. Python).\nWe can also extract more than one value by using the c() function inside the square brackets. Here we extract the 1st, 5th, 6th and 8th element from the my_vec object\n\nmy_vec[c(1, 5, 6, 8)]\n\n[1] 2 4 3 7\n\n\nOr we can extract a range of values using the : notation. To extract the values from the 3rd to the 8th elements\n\nmy_vec[3:8]\n\n[1] 1 6 4 3 3 7\n\n\n\n2.5.1.1 Logical index\nAnother really useful way to extract data from a vector is to use a logical expression as an index. For example, to extract all elements with a value greater than 4 in the vector my_vec\n\nmy_vec[my_vec &gt; 4]\n\n[1] 6 7\n\n\nHere, the logical expression is my_vec &gt; 4 and R will only extract those elements that satisfy this logical condition. So how does this actually work? If we look at the output of just the logical expression without the square brackets you can see that R returns a vector containing either TRUE or FALSE which correspond to whether the logical condition is satisfied for each element. In this case only the 4th and 8th elements return a TRUE as their value is greater than 4.\n\nmy_vec &gt; 4\n\n[1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\n\nSo what R is actually doing under the hood is equivalent to\n\nmy_vec[c(FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE, TRUE)]\n\n[1] 6 7\n\n\nand only those element that are TRUE will be extracted.\nIn addition to the &lt; and &gt; operators you can also use composite operators to increase the complexity of your expressions. For example the expression for ‘greater or equal to’ is &gt;=. To test whether a value is equal to a value we need to use a double equals symbol == and for ‘not equal to’ we use != (the ! symbol means ‘not’).\n\nmy_vec[my_vec &gt;= 4] # values greater or equal to 4\n\n[1] 6 4 7\n\nmy_vec[my_vec &lt; 4] # values less than 4\n\n[1] 2 3 1 3 3\n\nmy_vec[my_vec &lt;= 4] # values less than or equal to 4\n\n[1] 2 3 1 4 3 3\n\nmy_vec[my_vec == 4] # values equal to 4\n\n[1] 4\n\nmy_vec[my_vec != 4] # values not equal to 4\n\n[1] 2 3 1 6 3 3 7\n\n\nWe can also combine multiple logical expressions using Boolean expressions. In R the & symbol means AND and the | symbol means OR. For example, to extract values in my_vec which are less than 6 AND greater than 2\n\nval26 &lt;- my_vec[my_vec &lt; 6 & my_vec &gt; 2]\nval26\n\n[1] 3 4 3 3\n\n\nor extract values in my_vec that are greater than 6 OR less than 3\n\nval63 &lt;- my_vec[my_vec &gt; 6 | my_vec &lt; 3]\nval63\n\n[1] 2 1 7\n\n\n\n2.5.2 Replacing elements\nWe can change the values of some elements in a vector using our [ ] notation in combination with the assignment operator &lt;-. For example, to replace the 4th value of our my_vec object from 6 to 500\n\nmy_vec[4] &lt;- 500\nmy_vec\n\n[1]   2   3   1 500   4   3   3   7\n\n\nWe can also replace more than one value or even replace values based on a logical expression\n\n# replace the 6th and 7th element with 100\nmy_vec[c(6, 7)] &lt;- 100\nmy_vec\n\n[1]   2   3   1 500   4 100 100   7\n\n# replace element that are less than or equal to 4 with 1000\nmy_vec[my_vec &lt;= 4] &lt;- 1000\nmy_vec\n\n[1] 1000 1000 1000  500 1000  100  100    7\n\n\n\n2.5.3 Ordering elements\nIn addition to extracting particular elements from a vector we can also order the values contained in a vector. To sort the values from lowest to highest value we can use the sort() function\n\nvec_sort &lt;- sort(my_vec)\nvec_sort\n\n[1]    7  100  100  500 1000 1000 1000 1000\n\n\nTo reverse the sort, from highest to lowest, we can either include the decreasing = TRUE argument when using the sort() function\n\nvec_sort2 &lt;- sort(my_vec, decreasing = TRUE)\nvec_sort2\n\n[1] 1000 1000 1000 1000  500  100  100    7\n\n\nor first sort the vector using the sort() function and then reverse the sorted vector using the rev() function. This is another example of nesting one function inside another function.\n\nvec_sort3 &lt;- rev(sort(my_vec))\nvec_sort3\n\n[1] 1000 1000 1000 1000  500  100  100    7\n\n\nWhilst sorting a single vector is fun, perhaps a more useful task would be to sort one vector according to the values of another vector. To do this we should use the order() function in combination with [ ]. To demonstrate this let’s create a vector called height containing the height of 5 different people and another vector called p.names containing the names of these people (so Joanna is 180 cm, Charlotte is 155 cm etc)\n\nheight &lt;- c(180, 155, 160, 167, 181)\nheight\n\n[1] 180 155 160 167 181\n\np.names &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\np.names\n\n[1] \"Joanna\"    \"Charlotte\" \"Helen\"     \"Karen\"     \"Amy\"      \n\n\nOur goal is to order the people in p.names in ascending order of their height. The first thing we’ll do is use the order() function with the height variable to create a vector called height_ord\n\nheight_ord &lt;- order(height)\nheight_ord\n\n[1] 2 3 4 1 5\n\n\nOK, what’s going on here? The first value, 2, (remember ignore [1]) should be read as ‘the smallest value of height is the second element of the height vector’. If we check this by looking at the height vector above, you can see that element 2 has a value of 155, which is the smallest value. The second smallest value in height is the 3rd element of height, which when we check is 160 and so on. The largest value of height is element 5 which is 181. Now that we have a vector of the positional indices of heights in ascending order (height_ord), we can extract these values from our p.names vector in this order\n\nnames_ord &lt;- p.names[height_ord]\nnames_ord\n\n[1] \"Charlotte\" \"Helen\"     \"Karen\"     \"Joanna\"    \"Amy\"      \n\n\nYou’re probably thinking ‘what’s the use of this?’ Well, imagine you have a dataset which contains two columns of data and you want to sort each column. If you just use sort() to sort each column separately, the values of each column will become uncoupled from each other. By using the ‘order()’ on one column, a vector of positional indices is created of the values of the column in ascending order This vector can be used on the second column, as the index of elements which will return a vector of values based on the first column. In all honestly, when you have multiple related vectors you need to use a data.frame type of object (see Chapter 3) instead of multiple independent vectors.\n\n2.5.4 Vectorisation\nOne of the great things about R functions is that most of them are vectorised. This means that the function will operate on all elements of a vector without needing to apply the function on each element separately. For example, to multiple each element of a vector by 5 we can simply use\n\n# create a vector\nmy_vec2 &lt;- c(3, 5, 7, 1, 9, 20)\n\n# multiply each element by 5\nmy_vec2 * 5\n\n[1]  15  25  35   5  45 100\n\n\nOr we can add the elements of two or more vectors\n\n# create a second vector\nmy_vec3 &lt;- c(17, 15, 13, 19, 11, 0)\n\n# add both vectors\nmy_vec2 + my_vec3\n\n[1] 20 20 20 20 20 20\n\n# multiply both vectors\nmy_vec2 * my_vec3\n\n[1] 51 75 91 19 99  0\n\n\nHowever, you must be careful when using vectorisation with vectors of different lengths as R will quietly recycle the elements in the shorter vector rather than throw a wobbly (error).\n\n# create a third vector\nmy_vec4 &lt;- c(1, 2)\n\n# add both vectors - quiet recycling!\nmy_vec2 + my_vec4\n\n[1]  4  7  8  3 10 22\n\n\n\n2.5.5 Missing data\nIn R, missing data is usually represented by an NA symbol meaning ‘Not Available’. Data may be missing for a whole bunch of reasons, maybe your machine broke down, maybe you broke down, maybe the weather was too bad to collect data on a particular day etc etc. Missing data can be a pain in the proverbial both from an R perspective and also a statistical perspective. From an R perspective missing data can be problematic as different functions deal with missing data in different ways. For example, let’s say we collected air temperature readings over 10 days, but our thermometer broke on day 2 and again on day 9 so we have no data for those days\n\ntemp &lt;- c(7.2, NA, 7.1, 6.9, 6.5, 5.8, 5.8, 5.5, NA, 5.5)\ntemp\n\n [1] 7.2  NA 7.1 6.9 6.5 5.8 5.8 5.5  NA 5.5\n\n\nWe now want to calculate the mean temperature over these days using the mean() function\n\nmean_temp &lt;- mean(temp)\nmean_temp\n\n[1] NA\n\n\nIf a vector has a missing value then the only possible value to return when calculating a mean is NA. R doesn’t know that you perhaps want to ignore the NA values (R can’t read your mind - yet!). If we look at the help file (using ?mean - see the next section Section 2.6 for more details) associated with the mean() function we can see there is an argument na.rm = which is set to FALSE by default.\n\nna.rm - a logical value indicating whether NA values should be stripped before the computation proceeds.\n\nIf we change this argument to na.rm = TRUE when we use the mean() function this will allow us to ignore the NA values when calculating the mean\n\nmean_temp &lt;- mean(temp, na.rm = TRUE)\nmean_temp\n\n[1] 6.2875\n\n\nIt’s important to note that the NA values have not been removed from our temp object (that would be bad practice), rather the mean() function has just ignored them. The point of the above is to highlight how we can change the default behaviour of a function using an appropriate argument. The problem is that not all functions will have an na.rm = argument, they might deal with NA values differently. However, the good news is that every help file associated with any function will always tell you how missing data are handled by default.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#sec-help",
    "href": "02-basics.html#sec-help",
    "title": "2  Some R basics",
    "section": "\n2.6 Getting help",
    "text": "2.6 Getting help\nThis book is intended as a relatively brief introduction to R and as such you will soon be using functions and packages that go beyond this scope of this introductory text. Fortunately, one of the strengths of R is its comprehensive and easily accessible help system and wealth of online resources where you can obtain further information.\n\n2.6.1 R help\nTo access R’s built-in help facility to get information on any function simply use the help() function. For example, to open the help page for our friend the mean() function.\nhelp(\"mean\")\nor you can use the equivalent shortcut\n?mean\nthe help page is displayed in the ‘Help’ tab in the Files pane (usually in the bottom right of RStudio)\n\n\n\n\n\n\n\n\nAdmittedly the help files can seem anything but helpful when you first start using R. This is probably because they’re written in a very concise manner and the language used is often quite technical and full of jargon. Having said that, you do get used to this and will over time even come to appreciate a certain beauty in their brevity (honest!). One of the great things about the help files is that they all have a very similar structure regardless of the function. This makes it easy to navigate through the file to find exactly what you need.\nThe first line of the help document contains information such as the name of the function and the package where the function can be found. There are also other headings that provide more specific information such as\n\n\n\n\n\n\nHeadings\nDescription\n\n\n\nDescription:\ngives a brief description of the function and what it does.\n\n\nUsage:\ngives the name of the arguments associated with the function and possible default values.\n\n\nArguments:\nprovides more detail regarding each argument and what they do.\n\n\nDetails:\ngives further details of the function if required.\n\n\nValue:\nif applicable, gives the type and structure of the object returned by the function or the operator.\n\n\nSee Also:\nprovides information on other help pages with similar or related content.\n\n\nExamples:\ngives some examples of using the function.\n\n\n\nThe Examples are are really helpful, all you need to do is copy and paste them into the console to see what happens. You can also access examples at any time by using the example() function (i.e. example(\"mean\"))\nThe help() function is useful if you know the name of the function. If you’re not sure of the name, but can remember a key word then you can search R’s help system using the help.search() function.\nhelp.search(\"mean\")\nor you can use the equivalent shortcut\n??mean\nThe results of the search will be displayed in RStudio under the ‘Help’ tab as before. The help.search() function searches through the help documentation, code demonstrations and package vignettes and displays the results as clickable links for further exploration.\n\n\n\n\n\n\n\n\nAnother useful function is apropos(). This function can be used to list all functions containing a specified character string. For example, to find all functions with mean in their name\n\napropos(\"mean\")\n\n [1] \".colMeans\"     \".rowMeans\"     \"colMeans\"      \"kmeans\"       \n [5] \"mean\"          \"mean_temp\"     \"mean.Date\"     \"mean.default\" \n [9] \"mean.difftime\" \"mean.POSIXct\"  \"mean.POSIXlt\"  \"rowMeans\"     \n[13] \"vec_mean\"      \"weighted.mean\"\n\n\nYou can then bring up the help file for the relevant function.\nhelp(\"kmeans\")\nAnother function is RSiteSearch() which enables you to search for keywords and phrases in function help pages and vignettes for all CRAN packages. This function allows you to access the search engine of the R website https://www.r-project.org/search.html directly from the Console with the results displayed in your web browser.\nRSiteSearch(\"regression\")\n\n2.6.2 Other sources of help\nThere really has never been a better time to start learning R. There are a plethora of freely available online resources ranging from whole courses to subject specific tutorials and mailing lists. There are also plenty of paid for options if that’s your thing but unless you’ve money to burn there really is no need to part with your hard earned cash. Some resources we have found helpful are listed below.\n\n2.6.2.1 General R resources\n\n\nR-Project: User contributed documentation\n\nThe R Journal: Journal of the R project for statistical computing\n\nSwirl: An R package that teaches you R from within R\nRStudio’s printable cheatsheets\n\nRseek A custom Google search for R-related sites\n\n2.6.2.2 Getting help\n\nGoogle it!: Try Googling any error messages you get. It’s not cheating and everyone does it! You’ll be surprised how many other people have probably had the same problem and solved it.\nStack Overflow: There are many thousands of questions relevant to R on Stack Overflow. Here are the most popular ones, ranked by vote. Make sure you search for similar questions before asking your own, and make sure you include a reproducible example to get the most useful advice. A reproducible example is a minimal example that lets others who are trying to help you to see the error themselves.\n\n2.6.2.3 R markdown resources\n\nBasic markdown and R markdown reference\nA good markdown reference\nA good 10-minute markdown tutorial\nRStudio’s R markdown cheatsheet\nR markdown reference sheet\n\nThe R markdown documentation including a getting started guide, a gallery of demos, and several articles for more advanced usage.\n\nThe knitr website has lots of useful reference material about how knitr works.\n\n2.6.2.4 Git and GitHub resources\n\n\nHappy Git: Great resource for using Git and GitHub\n\nVersion control with RStudio: RStudio document for using version control\n\nUsing Git from RStudio: Good 10 minute guide\n\nThe R Class: In depth guide to using Git and GitHub with RStudio\n\n2.6.2.5 R programming\n\n\nR Programming for Data Science: In depth guide to R programming\n\nR for Data Science: Fantastic book, tidyverse orientated",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#saving-stuff-in-r",
    "href": "02-basics.html#saving-stuff-in-r",
    "title": "2  Some R basics",
    "section": "\n2.7 Saving stuff in R",
    "text": "2.7 Saving stuff in R\nYour approach to saving work in R and RStudio depends on what you want to save. Most of the time the only thing you will need to save is the R code in your script(s). Remember your script is a reproducible record of everything you’ve done so all you need to do is open up your script in a new RStudio session and source it into the R Console and you’re back to where you left off.\nUnless you’ve followed our suggestion about changing the default settings for RStudio Projects (see Section 1.5) you will be asked whether you want to save your workspace image every time you exit RStudio. We suggest that 99.9% of the time that you don’t want do this. By starting with a clean RStudio session each time we come back to our analysis we can be sure to avoid any potential conflicts with things we’ve done in previous sessions.\nThere are, however, some occasions when saving objects you’ve created in R is useful. For example, let’s say you’re creating an object that takes hours (even days) of computational time to generate. It would be extremely inconvenient to have to wait all this time each time you come back to your analysis (although we would suggest exporting this to an external file is a better solution). In this case we can save this object as an external .RData file which we can load back into RStudio the next time we want to use it. To save an object to an .RData file you can use the save() function (notice we don’t need to use the assignment operator here)\nsave(nameOfObject, file = \"name_of_file.RData\")\nor if you want to save all of the objects in your workspace into a single .RData file use the save.image() function\nsave.image(file = \"name_of_file.RData\")\nTo load your .RData file back into RStudio use the load() function\nload(file = \"name_of_file.RData\")",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "02-basics.html#sec-packages",
    "href": "02-basics.html#sec-packages",
    "title": "2  Some R basics",
    "section": "\n2.8 R packages",
    "text": "2.8 R packages\nThe base installation of R comes with many useful packages as standard. These packages will contain many of the functions you will use on a daily basis. However, as you start using R for more diverse projects (and as your own use of R evolves) you will find that there comes a time when you will need to extend R’s capabilities. Happily, many thousands of R users have developed useful code and shared this code as installable packages. You can think of a package as a collection of functions, data and help files collated into a well defined standard structure which you can download and install in R. These packages can be downloaded from a variety of sources but the most popular are CRAN, Bioconductor and GitHub. Currently, CRAN hosts over 15000 packages and is the official repository for user contributed R packages. Bioconductor provides open source software oriented towards bioinformatics and hosts over 1800 R packages. GitHub is a website that hosts git repositories for all sorts of software and projects (not just R). Often, cutting edge development versions of R packages are hosted on GitHub so if you need all the new bells and whistles then this may be an option. However, a potential downside of using the development version of an R package is that it might not be as stable as the version hosted on CRAN (it’s in development!) and updating packages won’t be automatic.\n\n2.8.1 Using packages\nOnce you have installed a package onto your computer it is not immediately available for you to use. To use a package you first need to load the package by using the library() function. For example, to load the remotes 📦 package you previously installed\nlibrary(remotes)\nThe library() function will also load any additional packages required and may print out additional package information. It is important to realize that every time you start a new R session (or restore a previously saved session) you need to load the packages you will be using. We tend to put all our library() statements required for our analysis near the top of our R scripts to make them easily accessible and easy to add to as our code develops. If you try to use a function without first loading the relevant R package you will receive an error message that R could not find the function. For example, if you try to use the install_github() function without loading the remotes 📦 package first you will receive the following error\ninstall_github(\"tidyverse/dplyr\")\n\n# Error in install_github(\"tidyverse/dplyr\") :\n#  could not find function \"install_github\"\nSometimes it can be useful to use a function without first using the library() function. If, for example, you will only be using one or two functions in your script and don’t want to load all of the other functions in a package then you can access the function directly by specifying the package name followed by two colons and then the function name\nremotes::install_github(\"tidyverse/dplyr\")\nThis is how we were able to use the install() and install_github() functions above without first loading the packages BiocManager 📦 and remotes 📦 . Most of the time we recommend using the library() function.\n\n2.8.2 Installing R packages\n\n2.8.2.1 CRAN packages\nTo install a package from CRAN you can use the install.packages() function. For example if you want to install the remotes package enter the following code into the Console window of RStudio (note: you will need a working internet connection to do this)\ninstall.packages(\"remotes\", dependencies = TRUE)\nYou may be asked to select a CRAN mirror, just select ‘0-cloud’ or a mirror near to your location. The dependencies = TRUE argument ensures that additional packages that are required will also be installed.\nIt’s good practice to regularly update your previously installed packages to get access to new functionality and bug fixes. To update CRAN packages you can use the update.packages() function (you will need a working internet connection for this)\nupdate.packages(ask = FALSE)\nThe ask = FALSE argument avoids having to confirm every package download which can be a pain if you have many packages installed.\n\n2.8.2.2 Bioconductor packages\nTo install packages from Bioconductor the process is a little different. You first need to install the BiocManager 📦 package. You only need to do this once unless you subsequently reinstall or upgrade R\ninstall.packages(\"BiocManager\", dependencies = TRUE)\nOnce the BiocManager📦 package has been installed you can either install all of the ‘core’ Bioconductor packages with\nBiocManager::install()\nor install specific packages such as the GenomicRanges 📦 and edgeR 📦 packages\nBiocManager::install(c(\"GenomicRanges\", \"edgeR\"))\nTo update Bioconductor packages just use the BiocManager::install() function again\nBiocManager::install(ask = FALSE)\nAgain, you can use the ask = FALSE argument to avoid having to confirm every package download.\n\n2.8.2.3 GitHub packages\nThere are multiple options for installing packages hosted on GitHub. Perhaps the most efficient method is to use the install_github() function from the remotes 📦 package (you installed this package previously (Section 2.8.2.1)). Before you use the function you will need to know the GitHub username of the repository owner and also the name of the repository. For example, the development version of dplyr 📦 from Hadley Wickham is hosted on the tidyverse GitHub account and has the repository name ‘dplyr’ (just search for ‘github dplyr’). To install this version from GitHub use\nremotes::install_github(\"tidyverse/dplyr\")\nThe safest way (that we know of) to update a package installed from GitHub is to just reinstall it using the above command.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Some R basics</span>"
    ]
  },
  {
    "objectID": "03-data.html",
    "href": "03-data.html",
    "title": "3  Data",
    "section": "",
    "text": "3.1 Data types\nUnderstanding the different types of data and how R deals with these data is important. The temptation is to glaze over and skip these technical details, but beware, this can come back to bite you somewhere unpleasant if you don’t pay attention. We’ve already seen an example (Section 2.3.1) of this when we tried (and failed) to add two character objects together using the + operator.\nR has six basic types of data; numeric, integer, logical, complex and character. The keen eyed among you will notice we’ve only listed five data types here, the final data type is raw which we won’t cover as it’s not useful 99.99% of the time. We also won’t cover complex numbers, but will let you imagine that part!\nR is (usually) able to automatically distinguish between different classes of data by their nature and the context in which they’re used although you should bear in mind that R can’t actually read your mind and you may have to explicitly tell R how you want to care a data type. You can find out the type (or class) of any object using the class() function.\nnum &lt;- 2.2\nclass(num)\n\n[1] \"numeric\"\n\nchar &lt;- \"hello\"\nclass(char)\n\n[1] \"character\"\n\nlogi &lt;- TRUE\nclass(logi)\n\n[1] \"logical\"\nAlternatively, you can ask if an object is a specific class using using a logical test. The is.[classOfData]() family of functions will return either a TRUE or a FALSE.\nis.numeric(num)\n\n[1] TRUE\n\nis.character(num)\n\n[1] FALSE\n\nis.character(char)\n\n[1] TRUE\n\nis.logical(logi)\n\n[1] TRUE\nIt can sometimes be useful to be able to change the class of a variable using the as.[className]() family of coercion functions, although you need to be careful when doing this as you might receive some unexpected results (see what happens below when we try to convert a character string to a numeric).\n# coerce numeric to character\nclass(num)\n\n[1] \"numeric\"\n\nnum_char &lt;- as.character(num)\nnum_char\n\n[1] \"2.2\"\n\nclass(num_char)\n\n[1] \"character\"\n\n# coerce character to numeric!\nclass(char)\n\n[1] \"character\"\n\nchar_num &lt;- as.numeric(char)\n\nWarning: NAs introduced by coercion\nHere’s a summary table of some of the logical test and coercion functions available to you.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#data-types",
    "href": "03-data.html#data-types",
    "title": "3  Data",
    "section": "",
    "text": "Numeric data are numbers that contain a decimal. Actually they can also be whole numbers but we’ll gloss over that.\nIntegers are whole numbers (those numbers without a decimal point).\nLogical data take on the value of either TRUE or FALSE. There’s also another special type of logical called NA to represent missing values.\nCharacter data are used to represent string values. You can think of character strings as something like a word (or multiple words). A special type of character string is a factor, which is a string but with additional attributes (like levels or an order). We’ll cover factors later.\n\n\n\n\n\n\n\n\n\n\nType\nLogical test\nCoercing\n\n\n\nCharacter\nis.character\nas.character\n\n\nNumeric\nis.numeric\nas.numeric\n\n\nLogical\nis.logical\nas.logical\n\n\nFactor\nis.factor\nas.factor\n\n\nComplex\nis.complex\nas.complex",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#data-structures",
    "href": "03-data.html#data-structures",
    "title": "3  Data",
    "section": "\n3.2 Data structures",
    "text": "3.2 Data structures\nNow that you’ve been introduced to some of the most important classes of data in R, let’s have a look at some of main structures that we have for storing these data.\n\n3.2.1 Scalars and vectors\nPerhaps the simplest type of data structure is the vector. You’ve already been introduced to vectors in Section 2.4 although some of the vectors you created only contained a single value. Vectors that have a single value (length 1) are called scalars. Vectors can contain numbers, characters, factors or logicals, but the key thing to remember is that all the elements inside a vector must be of the same class. In other words, vectors can contain either numbers, characters or logical but not mixtures of these types of data. There is one important exception to this, you can include NA (remember this is special type of logical) to denote missing data in vectors with other data types.\n\n\n\n\n\n\n\n\n\n3.2.2 Matrices and arrays\nAnother useful data structure used in many disciplines such as population ecology, theoretical and applied statistics is the matrix. A matrix is simply a vector that has additional attributes called dimensions. Arrays are just multidimensional matrices. Again, matrices and arrays must contain elements all of the same data class.\n\n\n\n\n\n\n\n\nA convenient way to create a matrix or an array is to use the matrix() and array() functions respectively. Below, we will create a matrix from a sequence 1 to 16 in four rows (nrow = 4) and fill the matrix row-wise (byrow = TRUE) rather than the default column-wise. When using the array() function we define the dimensions using the dim = argument, in our case 2 rows, 4 columns in 2 different matrices.\n\nmy_mat &lt;- matrix(1:16, nrow = 4, byrow = TRUE)\nmy_mat\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n\nmy_array &lt;- array(1:16, dim = c(2, 4, 2))\nmy_array\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]    9   11   13   15\n[2,]   10   12   14   16\n\n\nSometimes it’s also useful to define row and column names for your matrix but this is not a requirement. To do this use the rownames() and colnames() functions.\n\nrownames(my_mat) &lt;- c(\"A\", \"B\", \"C\", \"D\")\ncolnames(my_mat) &lt;- c(\"a\", \"b\", \"c\", \"d\")\nmy_mat\n\n   a  b  c  d\nA  1  2  3  4\nB  5  6  7  8\nC  9 10 11 12\nD 13 14 15 16\n\n\nOnce you’ve created your matrices you can do useful stuff with them and as you’d expect, R has numerous built in functions to perform matrix operations. Some of the most common are given below. For example, to transpose a matrix we use the transposition function t()\n\nmy_mat_t &lt;- t(my_mat)\nmy_mat_t\n\n  A B  C  D\na 1 5  9 13\nb 2 6 10 14\nc 3 7 11 15\nd 4 8 12 16\n\n\nTo extract the diagonal elements of a matrix and store them as a vector we can use the diag() function\n\nmy_mat_diag &lt;- diag(my_mat)\nmy_mat_diag\n\n[1]  1  6 11 16\n\n\nThe usual matrix addition, multiplication etc can be performed. Note the use of the %*% operator to perform matrix multiplication.\n\nmat.1 &lt;- matrix(c(2, 0, 1, 1), nrow = 2) # notice that the matrix has been filled\nmat.1 # column-wise by default\n\n     [,1] [,2]\n[1,]    2    1\n[2,]    0    1\n\nmat.2 &lt;- matrix(c(1, 1, 0, 2), nrow = 2)\nmat.2\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    1    2\n\nmat.1 + mat.2 # matrix addition\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    1    3\n\nmat.1 * mat.2 # element by element products\n\n     [,1] [,2]\n[1,]    2    0\n[2,]    0    2\n\nmat.1 %*% mat.2 # matrix multiplication\n\n     [,1] [,2]\n[1,]    3    2\n[2,]    1    2\n\n\n\n3.2.3 Lists\nThe next data structure we will quickly take a look at is a list. Whilst vectors and matrices are constrained to contain data of the same type, lists are able to store mixtures of data types. In fact we can even store other data structures such as vectors and arrays within a list or even have a list of a list. This makes for a very flexible data structure which is ideal for storing irregular or non-rectangular data (see Chapter 5 for an example).\nTo create a list we can use the list() function. Note how each of the three list elements are of different classes (character, logical, and numeric) and are of different lengths.\n\nlist_1 &lt;- list(\n  c(\"black\", \"yellow\", \"orange\"),\n  c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE),\n  matrix(1:6, nrow = 3)\n)\nlist_1\n\n[[1]]\n[1] \"black\"  \"yellow\" \"orange\"\n\n[[2]]\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n[[3]]\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nElements of the list can be named during the construction of the list\n\nlist_2 &lt;- list(\n  colours = c(\"black\", \"yellow\", \"orange\"),\n  evaluation = c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE),\n  time = matrix(1:6, nrow = 3)\n)\nlist_2\n\n$colours\n[1] \"black\"  \"yellow\" \"orange\"\n\n$evaluation\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n$time\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\nor after the list has been created using the names() function\n\nnames(list_1) &lt;- c(\"colours\", \"evaluation\", \"time\")\nlist_1\n\n$colours\n[1] \"black\"  \"yellow\" \"orange\"\n\n$evaluation\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE\n\n$time\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n\n\n3.2.4 Data frames\nBy far the most commonly used data structure to store data in is the data frame. A data frame is a powerful two-dimensional object made up of rows and columns which looks superficially very similar to a matrix. However, whilst matrices are restricted to containing data all of the same type, data frames can contain a mixture of different types of data. Typically, in a data frame each row corresponds to an individual observation and each column corresponds to a different measured or recorded variable. This setup may be familiar to those of you who use LibreOffice Calc or Microsoft Excel to manage and store your data. Perhaps a useful way to think about data frames is that they are essentially made up of a bunch of vectors (columns) with each vector containing its own data type but the data type can be different between vectors.\nAs an example, the data frame below contains the results of an experiment to determine the effect of parental care (with or without) of unicorns (Unicornus magnificens) on offsprings growth under 3 different food availability regime. The data frame has 8 variables (columns) and each row represents an individual unicorn. The variables care and food are factors (categorical variables). The p_care variable has 2 levels (care and no_care) and the food level variable has 3 levels (low, medium and high). The variables height, weight, mane_size and fluffyness are numeric and the variable horn_rings is an integer representing the number of rings on the horn. Although the variable block has numeric values, these do not really have any order and could also be treated as a factor (i.e. they could also have been called A and B).\n\n\n\nTable 3.1: Imported unicorn data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_care\nfood\nblock\nheight\nweight\nmane_size\nfluffyness\nhorn_rings\n\n\n\ncare\nmedium\n1\n7.5\n7.62\n11.7\n31.9\n1\n\n\ncare\nmedium\n1\n10.7\n12.14\n14.1\n46.0\n10\n\n\ncare\nmedium\n1\n11.2\n12.76\n7.1\n66.7\n10\n\n\ncare\nmedium\n1\n10.4\n8.78\n11.9\n20.3\n1\n\n\ncare\nmedium\n1\n10.4\n13.58\n14.5\n26.9\n4\n\n\ncare\nmedium\n1\n9.8\n10.08\n12.2\n72.7\n9\n\n\nno_care\nlow\n2\n3.7\n8.10\n10.5\n60.5\n6\n\n\nno_care\nlow\n2\n3.2\n7.45\n14.1\n38.1\n4\n\n\nno_care\nlow\n2\n3.9\n9.19\n12.4\n52.6\n9\n\n\nno_care\nlow\n2\n3.3\n8.92\n11.6\n55.2\n6\n\n\nno_care\nlow\n2\n5.5\n8.44\n13.5\n77.6\n9\n\n\nno_care\nlow\n2\n4.4\n10.60\n16.2\n63.3\n6\n\n\n\n\n\n\n\n\nThere are a couple of important things to bear in mind about data frames. These types of objects are known as rectangular data (or tidy data) as each column must have the same number of observations. Also, any missing data should be recorded as an NA just as we did with our vectors.\nWe can construct a data frame from existing data objects such as vectors using the data.frame() function. As an example, let’s create three vectors p.height, p.weight and p.names and include all of these vectors in a data frame object called dataf.\n\np.height &lt;- c(180, 155, 160, 167, 181)\np.weight &lt;- c(65, 50, 52, 58, 70)\np.names &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndataf &lt;- data.frame(height = p.height, weight = p.weight, names = p.names)\ndataf\n\n  height weight     names\n1    180     65    Joanna\n2    155     50 Charlotte\n3    160     52     Helen\n4    167     58     Karen\n5    181     70       Amy\n\n\nYou’ll notice that each of the columns are named with variable name we supplied when we used the data.frame() function. It also looks like the first column of the data frame is a series of numbers from one to five. Actually, this is not really a column but the name of each row. We can check this out by getting R to return the dimensions of the dataf object using the dim() function. We see that there are 5 rows and 3 columns.\n\ndim(dataf) # 5 rows and 3 columns\n\n[1] 5 3\n\n\nAnother really useful function which we use all the time is str() which will return a compact summary of the structure of the data frame object (or any object for that matter).\n\nstr(dataf)\n\n'data.frame':   5 obs. of  3 variables:\n $ height: num  180 155 160 167 181\n $ weight: num  65 50 52 58 70\n $ names : chr  \"Joanna\" \"Charlotte\" \"Helen\" \"Karen\" ...\n\n\nThe str() function gives us the data frame dimensions and also reminds us that dataf is a data.frame type object. It also lists all of the variables (columns) contained in the data frame, tells us what type of data the variables contain and prints out the first five values. We often copy this summary and place it in our R scripts with comments at the beginning of each line so we can easily refer back to it whilst writing our code. We showed you how to comment blocks in RStudio Section 1.7.\nAlso notice that R has automatically decided that our p.names variable should be a character (chr) class variable when we first created the data frame. Whether this is a good idea or not will depend on how you want to use this variable in later analysis. If we decide that this wasn’t such a good idea we can change the default behaviour of the data.frame() function by including the argument stringsAsFactors = TRUE. Now our strings are automatically converted to factors.\n\np.height &lt;- c(180, 155, 160, 167, 181)\np.weight &lt;- c(65, 50, 52, 58, 70)\np.names &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndataf &lt;- data.frame(\n  height = p.height, weight = p.weight, names = p.names,\n  stringsAsFactors = TRUE\n)\nstr(dataf)\n\n'data.frame':   5 obs. of  3 variables:\n $ height: num  180 155 160 167 181\n $ weight: num  65 50 52 58 70\n $ names : Factor w/ 5 levels \"Amy\",\"Charlotte\",..: 4 2 3 5 1",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#importing-data",
    "href": "03-data.html#importing-data",
    "title": "3  Data",
    "section": "\n3.3 Importing data",
    "text": "3.3 Importing data\nAlthough creating data frames from existing data structures is extremely useful, by far the most common approach is to create a data frame by importing data from an external file. To do this, you’ll need to have your data correctly formatted and saved in a file format that R is able to recognize. Fortunately for us, R is able to recognize a wide variety of file formats, although in reality you’ll probably end up only using two or three regularly.\n\n3.3.1 Saving files to import\nThe easiest method of creating a data file to import into R is to enter your data into a spreadsheet using either Microsoft Excel or LibreOffice Calc and save the spreadsheet as a comma delimited file. We prefer LibreOffice Calc as it’s open source, platform independent and free but MS Excel is OK too (but see here for some gotchas). Here’s the data from the petunia experiment we discussed previously displayed in LibreOffice. If you want to follow along you can download the data file (‘unicorn.xlsx’) from Appendix A.\n\n\n\n\n\n\n\n\nFor those of you unfamiliar with the tab delimited file format it simply means that data in different columns are separated with a ‘,’ character and is usually saved as a file with a ‘.csv’ extension.\nTo save a spreadsheet as a comma delimited file in LibreOffice Calc select File -&gt; Save as ... from the main menu. You will need to specify the location you want to save your file in the ‘Save in folder’ option and the name of the file in the ‘Name’ option. In the drop down menu located above the ‘Save’ button change the default ‘All formats’ to ‘Text CSV (.csv)’.\n\n\n\n\n\n\n\n\nClick the Save button and then select the ‘Use Text CSV Format’ option. Click on OK to save the file.\n\n\n\n\n\n\n\n\nThere are a couple of things to bear in mind when saving files to import into R which will make your life easier in the long run. Keep your column headings (if you have them) short and informative. Also avoid spaces in your column headings by replacing them with an underscore or a dot (i.e. replace mane size with mane size or mane.size) and avoid using special characters (i.e. leaf area (mm^2) or uppercase to simpy your life). Remember, if you have missing data in your data frame (empty cells) you should use an NA to represent these missing values or have an empty cell. This will keep the data frame tidy.\n\n3.3.2 Import functions\nOnce you’ve saved your data file in a suitable format we can now read this file into R. The workhorse function for importing data into R is the read.table() function (we discuss some alternatives later in the chapter). The read.table() function is a very flexible function with a shed load of arguments (see ?read.table) but it’s quite simple to use. Let’s import a comma delimited file called unicorns.csv which contains the data we saw previously in this Chapter (Section 3.2.4) and assign it to an object called unicorns. The file is located in a data directory which itself is located in our root directory (Section 1.4). The first row of the data contains the variable (column) names. To use the read.table() function to import this file\n\nunicorns &lt;- read.table(\n  file = \"data/unicorns.csv\", header = TRUE, sep = \",\", dec = \".\",\n  stringsAsFactors = TRUE\n)\n\nThere are a few things to note about the above command. First, the file path and the filename (including the file extension) needs to be enclosed in either single or double quotes (i.e. the data/unicorns.txt bit) as the read.table() function expects this to be a character string. If your working directory is already set to the directory which contains the file, you don’t need to include the entire file path just the filename. In the example above, the file path is separated with a single forward slash /. This will work regardless of the operating system you are using and we recommend you stick with this. However, Windows users may be more familiar with the single backslash notation and if you want to keep using this you will need to include them as double backslashes.\n\n\n\n\n\n\nWarning\n\n\n\nNote though that the double backslash notation will not work on computers using Mac OSX or Linux operating systems. We thus strongly discourage it since it is not reproducible\n\n\nThe header = TRUE argument specifies that the first row of your data contains the variable names (i.e. food, block etc). If this is not the case you can specify header = FALSE (actually, this is the default value so you can omit this argument entirely). The sep = \",\" argument tells R what is file delimiter.\nOther useful arguments include dec = and na.strings =. The dec = argument allows you to change the default character (.) used for a decimal point. This is useful if you’re in a country where decimal places are usually represented by a comma (i.e. dec = \",\"). The na.strings = argument allows you to import data where missing values are represented with a symbol other than NA. This can be quite common if you are importing data from other statistical software such as Minitab which represents missing values as a * (na.strings = \"*\").\nHonestly, from the read.table() a series of predefined functions are available. They are all using read.table() but define format specific options. We can simply read.csv()to read a csv file, with “,” separation and “.” for decimals. In countries were “,” is used for decimals, csv files use “;” as a separator. In this case using read.csv2() would be needed. When working with tab delimited files, the functions read.delim() and read.delim2() can be used with “.” and “,” as decimal respectively.\nAfter importing our data into R , to see the contents of the data frame we could just type the name of the object as we have done previously. BUT before you do that, think about why you’re doing this. If your data frame is anything other than tiny, all you’re going to do is fill up your Console with data. It’s not like you can easily check whether there are any errors or that your data has been imported correctly. A much better solution is to use our old friend the str() function to return a compact and informative summary of your data frame.\n\nstr(unicorns)\n\n'data.frame':   96 obs. of  8 variables:\n $ p_care    : Factor w/ 2 levels \"care\",\"no_care\": 1 1 1 1 1 1 1 1 1 1 ...\n $ food      : Factor w/ 3 levels \"high\",\"low\",\"medium\": 3 3 3 3 3 3 3 3 3 3 ...\n $ block     : int  1 1 1 1 1 1 1 1 2 2 ...\n $ height    : num  7.5 10.7 11.2 10.4 10.4 9.8 6.9 9.4 10.4 12.3 ...\n $ weight    : num  7.62 12.14 12.76 8.78 13.58 ...\n $ mane_size : num  11.7 14.1 7.1 11.9 14.5 12.2 13.2 14 10.5 16.1 ...\n $ fluffyness: num  31.9 46 66.7 20.3 26.9 72.7 43.1 28.5 57.8 36.9 ...\n $ horn_rings: int  1 10 10 1 4 9 7 6 5 8 ...\n\n\nHere we see that unicorns is a ‘data.frame’ object which contains 96 rows and 8 variables (columns). Each of the variables are listed along with their data class and the first 10 values. As we mentioned previously in this Chapter, it can be quite convenient to copy and paste this into your R script as a comment block for later reference.\nNotice also that your character string variables (care and food) have been imported as factors because we used the argument stringsAsFactors = TRUE. If this is not what you want you can prevent this by using the stringsAsFactors = FALSE or from R version 4.0.0 you can just leave out this argument as stringsAsFactors = FALSE is the default.\n\nunicorns &lt;- read.delim(file = \"data/unicorns.txt\")\nstr(unicorns)\n\n'data.frame':   96 obs. of  8 variables:\n $ p_care    : chr  \"care\" \"care\" \"care\" \"care\" ...\n $ food      : chr  \"medium\" \"medium\" \"medium\" \"medium\" ...\n $ block     : int  1 1 1 1 1 1 1 1 2 2 ...\n $ height    : num  7.5 10.7 11.2 10.4 10.4 9.8 6.9 9.4 10.4 12.3 ...\n $ weight    : num  7.62 12.14 12.76 8.78 13.58 ...\n $ mane_size : num  11.7 14.1 7.1 11.9 14.5 12.2 13.2 14 10.5 16.1 ...\n $ fluffyness: num  31.9 46 66.7 20.3 26.9 72.7 43.1 28.5 57.8 36.9 ...\n $ horn_rings: int  1 10 10 1 4 9 7 6 5 8 ...\n\n\nIf we just wanted to see the names of our variables (columns) in the data frame we can use the names() function which will return a character vector of the variable names.\n\nnames(unicorns)\n\n[1] \"p_care\"     \"food\"       \"block\"      \"height\"     \"weight\"    \n[6] \"mane_size\"  \"fluffyness\" \"horn_rings\"\n\n\nYou can even import spreadsheet files from MS Excel or other statistics software directly into R but our advice is that this should generally be avoided if possible as it just adds a layer of uncertainty between you and your data. In our opinion it’s almost always better to export your spreadsheets as tab or comma delimited files and then import them into R using one of the read.table() derivative function. If you’re hell bent on directly importing data from other software you will need to install the foreign package which has functions for importing Minitab, SPSS, Stata and SAS files. For MS Excel and LO Calc spreadsheets, there are a few packages that can be used.\n\n3.3.3 Common import frustrations\nIt’s quite common to get a bunch of really frustrating error messages when you first start importing data into R. Perhaps the most common is\nError in file(file, \"rt\") : cannot open the connection\nIn addition: Warning message:\nIn file(file, \"rt\") :\n  cannot open file 'unicorns.txt': No such file or directory\nThis error message is telling you that R cannot find the file you are trying to import. It usually rears its head for one of a couple of reasons (or all of them!). The first is that you’ve made a mistake in the spelling of either the filename or file path. Another common mistake is that you have forgotten to include the file extension in the filename (i.e. .txt). Lastly, the file is not where you say it is or you’ve used an incorrect file path. Using RStudio Projects (Section 1.5) and having a logical directory structure (Section 1.4) goes a long way to avoiding these types of errors.\nAnother really common mistake is to forget to include the header = TRUE argument when the first row of the data contains variable names. For example, if we omit this argument when we import our unicorns.txt file everything looks OK at first (no error message at least)\n\nunicorns_bad &lt;- read.table(file = \"data/unicorns.txt\", sep = \"\\t\")\n\nbut when we take a look at our data frame using str()\n\nstr(unicorns_bad)\n\n'data.frame':   97 obs. of  8 variables:\n $ V1: chr  \"p_care\" \"care\" \"care\" \"care\" ...\n $ V2: chr  \"food\" \"medium\" \"medium\" \"medium\" ...\n $ V3: chr  \"block\" \"1\" \"1\" \"1\" ...\n $ V4: chr  \"height\" \"7.5\" \"10.7\" \"11.2\" ...\n $ V5: chr  \"weight\" \"7.62\" \"12.14\" \"12.76\" ...\n $ V6: chr  \"mane_size\" \"11.7\" \"14.1\" \"7.1\" ...\n $ V7: chr  \"fluffyness\" \"31.9\" \"46\" \"66.7\" ...\n $ V8: chr  \"horn_rings\" \"1\" \"10\" \"10\" ...\n\n\nWe can see an obvious problem, all of our variables have been imported as factors and our variables are named V1, V2, V3 … V8. The problem happens because we haven’t told the read.table() function that the first row contains the variable names and so it treats them as data. As soon as we have a single character string in any of our data vectors, R treats the vectors as character type data (remember all elements in a vector must contain the same type of data (Section 3.2.1)).\nThis is just one more argument to use read.csv() or read.delim() function with appropriate default values for arguments.\n\n3.3.4 Other import options\nThere are numerous other functions to import data from a variety of sources and formats. Most of these functions are contained in packages that you will need to install before using them. We list a couple of the more useful packages and functions below.\nThe fread() function from the read.table package is great for importing large data files quickly and efficiently (much faster than the read.table() function). One of the great things about the fread() function is that it will automatically detect many of the arguments you would normally need to specify (like sep = etc). One of the things you will need to consider though is that the fread() function will return a data.table object not a data.frame as would be the case with the read.table() function. This is usually not a problem as you can pass a data.table object to any function that only accepts data.frame objects. To learn more about the differences between data.table and data.frame objects see here.\n\nlibrary(read.table)\nall_data &lt;- fread(file = \"data/unicorns.txt\")\n\nVarious functions from the readr package are also very efficient at reading in large data files. The readr package is part of the ‘tidyverse’ collection of packages and provides many equivalent functions to base R for importing data. The readr functions are used in a similar way to the read.table() or read.csv() functions and many of the arguments are the same (see ?readr::read_table for more details). There are however some differences. For example, when using the read_table() function the header = TRUE argument is replaced by col_names = TRUE and the function returns a tibble class object which is the tidyverse equivalent of a data.frame object (see here for differences).\n\nlibrary(readr)\n# import white space delimited files\nall_data &lt;- read_table(file = \"data/unicorns.txt\", col_names = TRUE)\n\n# import comma delimited files\nall_data &lt;- read_csv(file = \"data/unicorns.txt\")\n\n# import tab delimited files\nall_data &lt;- read_delim(file = \"data/unicorns.txt\", delim = \"\\t\")\n\n# or use\nall_data &lt;- read_tsv(file = \"data/unicorns.txt\")\n\nIf your data file is ginormous, then the ff and bigmemory packages may be useful as they both contain import functions that are able to store large data in a memory efficient manner. You can find out more about these functions here and here.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#wrangling-data-frames",
    "href": "03-data.html#wrangling-data-frames",
    "title": "3  Data",
    "section": "\n3.4 Wrangling data frames",
    "text": "3.4 Wrangling data frames\nNow that you’re able to successfully import your data from an external file into R our next task is to do something useful with our data. Working with data is a fundamental skill which you’ll need to develop and get comfortable with as you’ll likely do a lot of it during any project. The good news is that R is especially good at manipulating, summarising and visualising data. Manipulating data (often known as data wrangling or munging) in R can at first seem a little daunting for the new user but if you follow a few simple logical rules then you’ll quickly get the hang of it, especially with some practice.\nLet’s remind ourselves of the structure of the unicorns data frame we imported in the previous section.\n\nunicorns &lt;- read.table(file = \"data/unicorns.txt\", header = TRUE, sep = \"\\t\")\nstr(unicorns)\n\n'data.frame':   96 obs. of  8 variables:\n $ p_care    : chr  \"care\" \"care\" \"care\" \"care\" ...\n $ food      : chr  \"medium\" \"medium\" \"medium\" \"medium\" ...\n $ block     : int  1 1 1 1 1 1 1 1 2 2 ...\n $ height    : num  7.5 10.7 11.2 10.4 10.4 9.8 6.9 9.4 10.4 12.3 ...\n $ weight    : num  7.62 12.14 12.76 8.78 13.58 ...\n $ mane_size : num  11.7 14.1 7.1 11.9 14.5 12.2 13.2 14 10.5 16.1 ...\n $ fluffyness: num  31.9 46 66.7 20.3 26.9 72.7 43.1 28.5 57.8 36.9 ...\n $ horn_rings: int  1 10 10 1 4 9 7 6 5 8 ...\n\n\nTo access the data in any of the variables (columns) in our data frame we can use the $ notation. For example, to access the height variable in our unicorns data frame we can use unicorns$height. This tells R that the height variable is contained within the data frame unicorns.\n\nunicorns$height\n\n [1]  7.5 10.7 11.2 10.4 10.4  9.8  6.9  9.4 10.4 12.3 10.4 11.0  7.1  6.0  9.0\n[16]  4.5 12.6 10.0 10.0  8.5 14.1 10.1  8.5  6.5 11.5  7.7  6.4  8.8  9.2  6.2\n[31]  6.3 17.2  8.0  8.0  6.4  7.6  9.7 12.3  9.1  8.9  7.4  3.1  7.9  8.8  8.5\n[46]  5.6 11.5  5.8  5.6  5.3  7.5  4.1  3.5  8.5  4.9  2.5  5.4  3.9  5.8  4.5\n[61]  8.0  1.8  2.2  3.9  8.5  8.5  6.4  1.2  2.6 10.9  7.2  2.1  4.7  5.0  6.5\n[76]  2.6  6.0  9.3  4.6  5.2  3.9  2.3  5.2  2.2  4.5  1.8  3.0  3.7  2.4  5.7\n[91]  3.7  3.2  3.9  3.3  5.5  4.4\n\n\nThis will return a vector of the height data. If we want we can assign this vector to another object and do stuff with it, like calculate a mean or get a summary of the variable using the summary() function.\n\nf_height &lt;- unicorns$height\nmean(f_height)\n\n[1] 6.839583\n\nsummary(f_height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.200   4.475   6.450   6.840   9.025  17.200 \n\n\nOr if we don’t want to create an additional object we can use functions ‘on-the-fly’ to only display the value in the console.\n\nmean(unicorns$height)\n\n[1] 6.839583\n\nsummary(unicorns$height)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.200   4.475   6.450   6.840   9.025  17.200 \n\n\nJust as we did with vectors (Section 2.5), we also can access data in data frames using the square bracket [ ] notation. However, instead of just using a single index, we now need to use two indexes, one to specify the rows and one for the columns. To do this, we can use the notation my_data[rows, columns] where rows and columns are indexes and my_data is the name of the data frame. Again, just like with our vectors our indexes can be positional or the result of a logical test.\n\n3.4.1 Positional indexes\nTo use positional indexes we simple have to write the position of the rows and columns we want to extract inside the [ ]. For example, if for some reason we wanted to extract the first value (1st row ) of the height variable (4th column)\n\nunicorns[1, 4]\n\n[1] 7.5\n\n# this would give you the same\nunicorns$height[1]\n\n[1] 7.5\n\n\nWe can also extract values from multiple rows or columns by specifying these indexes as vectors inside the [ ]. To extract the first 10 rows and the first 4 columns we simple supply a vector containing a sequence from 1 to 10 for the rows index (1:10) and a vector from 1 to 4 for the column index (1:4).\n\nunicorns[1:10, 1:4]\n\n   p_care   food block height\n1    care medium     1    7.5\n2    care medium     1   10.7\n3    care medium     1   11.2\n4    care medium     1   10.4\n5    care medium     1   10.4\n6    care medium     1    9.8\n7    care medium     1    6.9\n8    care medium     1    9.4\n9    care medium     2   10.4\n10   care medium     2   12.3\n\n\nOr for non sequential rows and columns then we can supply vectors of positions using the c() function. To extract the 1st, 5th, 12th, 30th rows from the 1st, 3rd, 6th and 8th columns\n\nunicorns[c(1, 5, 12, 30), c(1, 3, 6, 8)]\n\n   p_care block mane_size horn_rings\n1    care     1      11.7          1\n5    care     1      14.5          4\n12   care     2      12.6          6\n30   care     2      11.6          5\n\n\nAll we are doing in the two examples above is creating vectors of positions for the rows and columns that we want to extract. We have done this by using the skills we developed in Section 2.4 when we generated vectors using the c() function or using the : notation.\nBut what if we want to extract either all of the rows or all of the columns? It would be extremely tedious to have to generate vectors for all rows or for all columns. Thankfully R has a shortcut. If you don’t specify either a row or column index in the [ ] then R interprets it to mean you want all rows or all columns. For example, to extract the first 4 rows and all of the columns in the unicorns data frame\n\nunicorns[1:4, ]\n\n  p_care   food block height weight mane_size fluffyness horn_rings\n1   care medium     1    7.5   7.62      11.7       31.9          1\n2   care medium     1   10.7  12.14      14.1       46.0         10\n3   care medium     1   11.2  12.76       7.1       66.7         10\n4   care medium     1   10.4   8.78      11.9       20.3          1\n\n\nor all of the rows and the first 3 columns1.\nunicorns[, 1:3]\n\n\n    p_care   food block\n1     care medium     1\n2     care medium     1\n3     care medium     1\n4     care medium     1\n5     care medium     1\n92 no_care    low     2\n93 no_care    low     2\n94 no_care    low     2\n95 no_care    low     2\n96 no_care    low     2\n\n\nWe can even use negative positional indexes to exclude certain rows and columns. As an example, lets extract all of the rows except the first 85 rows and all columns except the 4th, 7th and 8th columns. Notice we need to use -() when we generate our row positional vectors. If we had just used -1:85 this would actually generate a regular sequence from -1 to 85 which is not what we want (we can of course use -1:-85).\n\nunicorns[-(1:85), -c(4, 7, 8)]\n\n    p_care food block weight mane_size\n86 no_care  low     1   6.01      17.6\n87 no_care  low     1   9.93      12.0\n88 no_care  low     1   7.03       7.9\n89 no_care  low     2   9.10      14.5\n90 no_care  low     2   9.05       9.6\n91 no_care  low     2   8.10      10.5\n92 no_care  low     2   7.45      14.1\n93 no_care  low     2   9.19      12.4\n94 no_care  low     2   8.92      11.6\n95 no_care  low     2   8.44      13.5\n96 no_care  low     2  10.60      16.2\n\n\nIn addition to using a positional index for extracting particular columns (variables) we can also name the variables directly when using the square bracket [ ] notation. For example, let’s extract the first 5 rows and the variables care, food and mane_size. Instead of using unicorns[1:5, c(1, 2, 6)] we can instead use\n\nunicorns[1:5, c(\"p_care\", \"food\", \"mane_size\")]\n\n  p_care   food mane_size\n1   care medium      11.7\n2   care medium      14.1\n3   care medium       7.1\n4   care medium      11.9\n5   care medium      14.5\n\n\nWe often use this method in preference to the positional index for selecting columns as it will still give us what we want even if we’ve changed the order of the columns in our data frame for some reason.\n\n3.4.2 Logical indexes\nJust as we did with vectors, we can also extract data from our data frame based on a logical test. We can use all of the logical operators that we used for our vector examples so if these have slipped your mind maybe have a look at Section 2.5.1.1 and refresh your memory. Let’s extract all rows where height is greater than 12 and extract all columns by default (remember, if you don’t include a column index after the comma it means all columns).\n\nbig_unicorns &lt;- unicorns[unicorns$height &gt; 12, ]\nbig_unicorns\n\n   p_care   food block height weight mane_size fluffyness horn_rings\n10   care medium     2   12.3  13.48      16.1       36.9          8\n17   care   high     1   12.6  18.66      18.6       54.0          9\n21   care   high     1   14.1  19.12      13.1      113.2         13\n32   care   high     2   17.2  19.20      10.9       89.9         14\n38   care    low     1   12.3  11.27      13.7       28.7          5\n\n\nNotice in the code above that we need to use the unicorns$height notation for the logical test. If we just named the height variable without the name of the data frame we would receive an error telling us R couldn’t find the variable height. The reason for this is that the height variable only exists inside the unicorns data frame so you need to tell R exactly where it is.\nbig_unicorns &lt;- unicorns[height &gt; 12, ]\nError in `[.data.frame`(unicorns, height &gt; 12, ) : \n  object 'height' not found\nSo how does this work? The logical test is unicorns$height &gt; 12 and R will only extract those rows that satisfy this logical condition. If we look at the output of just the logical condition you can see this returns a vector containing TRUE if height is greater than 12 and FALSE if height is not greater than 12.\n\nunicorns$height &gt; 12\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n[13] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n[25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[37] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nSo our row index is a vector containing either TRUE or FALSE values and only those rows that are TRUE are selected.\nOther commonly used operators are shown below\n\nunicorns[unicorns$height &gt;= 6, ] # values greater or equal to 6\n\nunicorns[unicorns$height &lt;= 6, ] # values less than or equal to 6\n\nunicorns[unicorns$height == 8, ] # values  equal to 8\n\nunicorns[unicorns$height != 8, ] # values  not equal to 8\n\nWe can also extract rows based on the value of a character string or factor level. Let’s extract all rows where the food level is equal to high (again we will output all columns). Notice that the double equals == sign must be used for a logical test and that the character string must be enclosed in either single or double quotes (i.e. \"high\").\n\nfood_high &lt;- unicorns[unicorns$food == \"high\", ]\nrbind(head(food_high, n = 10), tail(food_high, n = 10))\n\n    p_care food block height weight mane_size fluffyness horn_rings\n17    care high     1   12.6  18.66      18.6       54.0          9\n18    care high     1   10.0  18.07      16.9       90.5          3\n19    care high     1   10.0  13.29      15.8      142.7         12\n20    care high     1    8.5  14.33      13.2       91.4          5\n21    care high     1   14.1  19.12      13.1      113.2         13\n22    care high     1   10.1  15.49      12.6       77.2         12\n23    care high     1    8.5  17.82      20.5       54.4          3\n24    care high     1    6.5  17.13      24.1      147.4          6\n25    care high     2   11.5  23.89      14.3      101.5         12\n26    care high     2    7.7  14.77      17.2      104.5          4\n71 no_care high     1    7.2  15.21      15.9      135.0         14\n72 no_care high     1    2.1  19.15      15.6      176.7          6\n73 no_care high     2    4.7  13.42      19.8      124.7          5\n74 no_care high     2    5.0  16.82      17.3      182.5         15\n75 no_care high     2    6.5  14.00      10.1      126.5          7\n76 no_care high     2    2.6  18.88      16.4      181.5         14\n77 no_care high     2    6.0  13.68      16.2      133.7          2\n78 no_care high     2    9.3  18.75      18.4      181.1         16\n79 no_care high     2    4.6  14.65      16.7       91.7         11\n80 no_care high     2    5.2  17.70      19.1      181.1          8\n\n\nOr we can extract all rows where food level is not equal to medium (using !=) and only return columns 1 to 4.\n\nfood_not_medium &lt;- unicorns[unicorns$food != \"medium\", 1:4]\nrbind(head(food_not_medium, n = 10), tail(food_not_medium, n = 10))\n\n    p_care food block height\n17    care high     1   12.6\n18    care high     1   10.0\n19    care high     1   10.0\n20    care high     1    8.5\n21    care high     1   14.1\n22    care high     1   10.1\n23    care high     1    8.5\n24    care high     1    6.5\n25    care high     2   11.5\n26    care high     2    7.7\n87 no_care  low     1    3.0\n88 no_care  low     1    3.7\n89 no_care  low     2    2.4\n90 no_care  low     2    5.7\n91 no_care  low     2    3.7\n92 no_care  low     2    3.2\n93 no_care  low     2    3.9\n94 no_care  low     2    3.3\n95 no_care  low     2    5.5\n96 no_care  low     2    4.4\n\n\nWe can increase the complexity of our logical tests by combining them with Boolean expressions just as we did for vector objects. For example, to extract all rows where height is greater or equal to 6 AND food is equal to medium AND care is equal to no_care we combine a series of logical expressions with the & symbol.\n\nlow_no_care_heigh6 &lt;- unicorns[unicorns$height &gt;= 6 & unicorns$food == \"medium\" &\n  unicorns$p_care == \"no_care\", ]\nlow_no_care_heigh6\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n51 no_care medium     1    7.5  13.60      13.6      122.2         11\n54 no_care medium     1    8.5  10.04      12.3      113.6          4\n61 no_care medium     2    8.0  11.43      12.6       43.2         14\n\n\nTo extract rows based on an ‘OR’ Boolean expression we can use the | symbol. Let’s extract all rows where height is greater than 12.3 OR less than 2.2.\n\nheight2.2_12.3 &lt;- unicorns[unicorns$height &gt; 12.3 | unicorns$height &lt; 2.2, ]\nheight2.2_12.3\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n17    care   high     1   12.6  18.66      18.6       54.0          9\n21    care   high     1   14.1  19.12      13.1      113.2         13\n32    care   high     2   17.2  19.20      10.9       89.9         14\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n68 no_care   high     1    1.2  18.24      16.6      148.1          7\n72 no_care   high     1    2.1  19.15      15.6      176.7          6\n86 no_care    low     1    1.8   6.01      17.6       46.2          4\n\n\nAn alternative method of selecting parts of a data frame based on a logical expression is to use the subset() function instead of the [ ]. The advantage of using subset() is that you no longer need to use the $ notation when specifying variables inside the data frame as the first argument to the function is the name of the data frame to be subsetted. The disadvantage is that subset() is less flexible than the [ ] notation.\n\ncare_med_2 &lt;- subset(unicorns, p_care == \"care\" & food == \"medium\" & block == 2)\ncare_med_2\n\n   p_care   food block height weight mane_size fluffyness horn_rings\n9    care medium     2   10.4  10.48      10.5       57.8          5\n10   care medium     2   12.3  13.48      16.1       36.9          8\n11   care medium     2   10.4  13.18      11.1       56.8         12\n12   care medium     2   11.0  11.56      12.6       31.3          6\n13   care medium     2    7.1   8.16      29.6        9.7          2\n14   care medium     2    6.0  11.22      13.0       16.4          3\n15   care medium     2    9.0  10.20      10.8       90.1          6\n16   care medium     2    4.5  12.55      13.4       14.4          6\n\n\nAnd if you only want certain columns you can use the select = argument.\n\nuni_p_care &lt;- subset(unicorns, p_care == \"care\" & food == \"medium\" & block == 2,\n  select = c(\"p_care\", \"food\", \"mane_size\")\n)\nuni_p_care\n\n   p_care   food mane_size\n9    care medium      10.5\n10   care medium      16.1\n11   care medium      11.1\n12   care medium      12.6\n13   care medium      29.6\n14   care medium      13.0\n15   care medium      10.8\n16   care medium      13.4\n\n\n\n3.4.3 Ordering data frames\nRemember when we used the function order() to order one vector based on the order of another vector (way back in Section 2.5.3). This comes in very handy if you want to reorder rows in your data frame. For example, if we want all of the rows in the data frame unicorns to be ordered in ascending value of height and output all columns by default.\n\nheight_ord &lt;- unicorns[order(unicorns$height), ]\nhead(height_ord, n = 10)\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n68 no_care   high     1    1.2  18.24      16.6      148.1          7\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n86 no_care    low     1    1.8   6.01      17.6       46.2          4\n72 no_care   high     1    2.1  19.15      15.6      176.7          6\n63 no_care medium     2    2.2  10.70      15.3       97.1          7\n84 no_care    low     1    2.2   9.97       9.6       63.1          2\n82 no_care    low     1    2.3   7.28      13.8       32.8          6\n89 no_care    low     2    2.4   9.10      14.5       78.7          8\n56 no_care medium     1    2.5  14.85      17.5       77.8         10\n69 no_care   high     1    2.6  16.57      17.1      141.1          3\n\n\nWe can also order by descending order of a variable (i.e. mane_size) using the decreasing = TRUE argument.\n\nmane_size_ord &lt;- unicorns[order(unicorns$mane_size, decreasing = TRUE), ]\nhead(mane_size_ord, n = 10)\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n70 no_care   high     1   10.9  17.22      49.2      189.6         17\n13    care medium     2    7.1   8.16      29.6        9.7          2\n24    care   high     1    6.5  17.13      24.1      147.4          6\n65 no_care   high     1    8.5  22.53      20.8      166.9         16\n23    care   high     1    8.5  17.82      20.5       54.4          3\n66 no_care   high     1    8.5  17.33      19.8      184.4         12\n73 no_care   high     2    4.7  13.42      19.8      124.7          5\n80 no_care   high     2    5.2  17.70      19.1      181.1          8\n17    care   high     1   12.6  18.66      18.6       54.0          9\n49 no_care medium     1    5.6  11.03      18.6       49.9          8\n\n\nWe can even order data frames based on multiple variables. For example, to order the data frame unicorns in ascending order of both block and height.\n\nblock_height_ord &lt;- unicorns[order(unicorns$block, unicorns$height), ]\nhead(block_height_ord, n = 10)\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n68 no_care   high     1    1.2  18.24      16.6      148.1          7\n86 no_care    low     1    1.8   6.01      17.6       46.2          4\n72 no_care   high     1    2.1  19.15      15.6      176.7          6\n84 no_care    low     1    2.2   9.97       9.6       63.1          2\n82 no_care    low     1    2.3   7.28      13.8       32.8          6\n56 no_care medium     1    2.5  14.85      17.5       77.8         10\n69 no_care   high     1    2.6  16.57      17.1      141.1          3\n87 no_care    low     1    3.0   9.93      12.0       56.6          6\n53 no_care medium     1    3.5  12.93      16.6      109.3          3\n88 no_care    low     1    3.7   7.03       7.9       36.7          5\n\n\nWhat if we wanted to order unicorns by ascending order of block but descending order of height? We can use a simple trick by adding a - symbol before the unicorns$height variable when we use the order() function. This will essentially turn all of the height values negative which will result in reversing the order. Note, that this trick will only work with numeric variables.\n\nblock_revheight_ord &lt;- unicorns[order(unicorns$block, -unicorns$height), ]\nrbind(head(block_revheight_ord, n = 10), tail(block_revheight_ord, n = 10))\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n21    care   high     1   14.1  19.12      13.1      113.2         13\n17    care   high     1   12.6  18.66      18.6       54.0          9\n38    care    low     1   12.3  11.27      13.7       28.7          5\n3     care medium     1   11.2  12.76       7.1       66.7         10\n70 no_care   high     1   10.9  17.22      49.2      189.6         17\n2     care medium     1   10.7  12.14      14.1       46.0         10\n4     care medium     1   10.4   8.78      11.9       20.3          1\n5     care medium     1   10.4  13.58      14.5       26.9          4\n22    care   high     1   10.1  15.49      12.6       77.2         12\n18    care   high     1   10.0  18.07      16.9       90.5          3\n64 no_care medium     2    3.9  12.97      17.0       97.5          5\n93 no_care    low     2    3.9   9.19      12.4       52.6          9\n91 no_care    low     2    3.7   8.10      10.5       60.5          6\n94 no_care    low     2    3.3   8.92      11.6       55.2          6\n92 no_care    low     2    3.2   7.45      14.1       38.1          4\n42    care    low     2    3.1   8.74      16.1       39.1          3\n76 no_care   high     2    2.6  18.88      16.4      181.5         14\n89 no_care    low     2    2.4   9.10      14.5       78.7          8\n63 no_care medium     2    2.2  10.70      15.3       97.1          7\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n\n\nIf we wanted to do the same thing with a factor (or character) variable like food we would need to use the function xtfrm() for this variable inside our order() function.\n\nblock_revheight_ord &lt;- unicorns[order(-xtfrm(unicorns$food), unicorns$height), ]\nrbind(head(block_revheight_ord, n = 10), tail(block_revheight_ord, n = 10))\n\n    p_care   food block height weight mane_size fluffyness horn_rings\n62 no_care medium     2    1.8  10.47      11.8      120.8          9\n63 no_care medium     2    2.2  10.70      15.3       97.1          7\n56 no_care medium     1    2.5  14.85      17.5       77.8         10\n53 no_care medium     1    3.5  12.93      16.6      109.3          3\n58 no_care medium     2    3.9   9.07       9.6       90.4          7\n64 no_care medium     2    3.9  12.97      17.0       97.5          5\n52 no_care medium     1    4.1  12.58      13.9      136.6         11\n16    care medium     2    4.5  12.55      13.4       14.4          6\n60 no_care medium     2    4.5  13.68      14.8      125.5          9\n55 no_care medium     1    4.9   6.89       8.2       52.9          3\n29    care   high     2    9.2  13.26      11.3      108.0          9\n78 no_care   high     2    9.3  18.75      18.4      181.1         16\n18    care   high     1   10.0  18.07      16.9       90.5          3\n19    care   high     1   10.0  13.29      15.8      142.7         12\n22    care   high     1   10.1  15.49      12.6       77.2         12\n70 no_care   high     1   10.9  17.22      49.2      189.6         17\n25    care   high     2   11.5  23.89      14.3      101.5         12\n17    care   high     1   12.6  18.66      18.6       54.0          9\n21    care   high     1   14.1  19.12      13.1      113.2         13\n32    care   high     2   17.2  19.20      10.9       89.9         14\n\n\nNotice that the food variable has been reverse ordered alphabetically and height has been ordered by increasing values within each level of food.\nIf we wanted to order the data frame by food but this time order it from low -&gt; medium -&gt; high instead of the default alphabetically (high, low, medium), we need to first change the order of our levels of the food factor in our data frame using the factor() function. Once we’ve done this we can then use the order() function as usual. Note, if you’re reading the pdf version of this book, the output has been truncated to save space.\n\nunicorns$food &lt;- factor(unicorns$food,\n  levels = c(\"low\", \"medium\", \"high\")\n)\nfood_ord &lt;- unicorns[order(unicorns$food), ]\nrbind(head(food_ord, n = 10), tail(food_ord, n = 10))\n\n    p_care food block height weight mane_size fluffyness horn_rings\n33    care  low     1    8.0   6.88       9.3       16.1          4\n34    care  low     1    8.0  10.23      11.9       88.1          4\n35    care  low     1    6.4   5.97       8.7        7.3          2\n36    care  low     1    7.6  13.05       7.2       47.2          8\n37    care  low     1    9.7   6.49       8.1       18.0          3\n38    care  low     1   12.3  11.27      13.7       28.7          5\n39    care  low     1    9.1   8.96       9.7       23.8          3\n40    care  low     1    8.9  11.48      11.1       39.4          7\n41    care  low     2    7.4  10.89      13.3        9.5          5\n42    care  low     2    3.1   8.74      16.1       39.1          3\n71 no_care high     1    7.2  15.21      15.9      135.0         14\n72 no_care high     1    2.1  19.15      15.6      176.7          6\n73 no_care high     2    4.7  13.42      19.8      124.7          5\n74 no_care high     2    5.0  16.82      17.3      182.5         15\n75 no_care high     2    6.5  14.00      10.1      126.5          7\n76 no_care high     2    2.6  18.88      16.4      181.5         14\n77 no_care high     2    6.0  13.68      16.2      133.7          2\n78 no_care high     2    9.3  18.75      18.4      181.1         16\n79 no_care high     2    4.6  14.65      16.7       91.7         11\n80 no_care high     2    5.2  17.70      19.1      181.1          8\n\n\n\n3.4.4 Adding columns and rows\nSometimes it’s useful to be able to add extra rows and columns of data to our data frames. There are multiple ways to achieve this (as there always is in R!) depending on your circumstances. To simply append additional rows to an existing data frame we can use the rbind() function and to append columns the cbind() function. Let’s create a couple of test data frames to see this in action using our old friend the data.frame() function.\n\n# rbind for rows\ndf1 &lt;- data.frame(\n  id = 1:4, height = c(120, 150, 132, 122),\n  weight = c(44, 56, 49, 45)\n)\ndf1\n\n  id height weight\n1  1    120     44\n2  2    150     56\n3  3    132     49\n4  4    122     45\n\ndf2 &lt;- data.frame(\n  id = 5:6, height = c(119, 110),\n  weight = c(39, 35)\n)\ndf2\n\n  id height weight\n1  5    119     39\n2  6    110     35\n\ndf3 &lt;- data.frame(\n  id = 1:4, height = c(120, 150, 132, 122),\n  weight = c(44, 56, 49, 45)\n)\ndf3\n\n  id height weight\n1  1    120     44\n2  2    150     56\n3  3    132     49\n4  4    122     45\n\ndf4 &lt;- data.frame(location = c(\"UK\", \"CZ\", \"CZ\", \"UK\"))\ndf4\n\n  location\n1       UK\n2       CZ\n3       CZ\n4       UK\n\n\nWe can use the rbind() function to append the rows of data in df2 to the rows in df1 and assign the new data frame to df_rcomb.\n\ndf_rcomb &lt;- rbind(df1, df2)\ndf_rcomb\n\n  id height weight\n1  1    120     44\n2  2    150     56\n3  3    132     49\n4  4    122     45\n5  5    119     39\n6  6    110     35\n\n\nAnd cbind to append the column in df4 to the df3 data frame and assign to df_ccomb`.\n\ndf_ccomb &lt;- cbind(df3, df4)\ndf_ccomb\n\n  id height weight location\n1  1    120     44       UK\n2  2    150     56       CZ\n3  3    132     49       CZ\n4  4    122     45       UK\n\n\nAnother situation when adding a new column to a data frame is useful is when you want to perform some kind of transformation on an existing variable. For example, say we wanted to apply a log10 transformation on the height variable in the df_rcomb data frame we created above. We could just create a separate variable to contains these values but it’s good practice to create this variable as a new column inside our existing data frame so we keep all of our data together. Let’s call this new variable height_log10.\n\n# log10 transformation\ndf_rcomb$height_log10 &lt;- log10(df_rcomb$height)\ndf_rcomb\n\n  id height weight height_log10\n1  1    120     44     2.079181\n2  2    150     56     2.176091\n3  3    132     49     2.120574\n4  4    122     45     2.086360\n5  5    119     39     2.075547\n6  6    110     35     2.041393\n\n\nThis situation also crops up when we want to convert an existing variable in a data frame from one data class to another data class. For example, the id variable in the df_rcomb data frame is numeric type data (use the str() or class() functions to check for yourself). If we wanted to convert the id variable to a factor to use later in our analysis we can create a new variable called Fid in our data frame and use the factor() function to convert the id variable.\n\n# convert to a factor\ndf_rcomb$Fid &lt;- factor(df_rcomb$id)\ndf_rcomb\n\n  id height weight height_log10 Fid\n1  1    120     44     2.079181   1\n2  2    150     56     2.176091   2\n3  3    132     49     2.120574   3\n4  4    122     45     2.086360   4\n5  5    119     39     2.075547   5\n6  6    110     35     2.041393   6\n\nstr(df_rcomb)\n\n'data.frame':   6 obs. of  5 variables:\n $ id          : int  1 2 3 4 5 6\n $ height      : num  120 150 132 122 119 110\n $ weight      : num  44 56 49 45 39 35\n $ height_log10: num  2.08 2.18 2.12 2.09 2.08 ...\n $ Fid         : Factor w/ 6 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6\n\n\n\n3.4.5 Merging data frames\nInstead of just appending either rows or columns to a data frame we can also merge two data frames together. Let’s say we have one data frame that contains taxonomic information on some common UK rocky shore invertebrates (called taxa) and another data frame that contains information on where they are usually found on the rocky shore (called zone). We can merge these two data frames together to produce a single data frame with both taxonomic and location information. Let’s first create both of these data frames (in reality you would probably just import your different datasets).\n\ntaxa &lt;- data.frame(\n  GENUS = c(\"Patella\", \"Littorina\", \"Halichondria\", \"Semibalanus\"),\n  species = c(\"vulgata\", \"littoria\", \"panacea\", \"balanoides\"),\n  family = c(\"patellidae\", \"Littorinidae\", \"Halichondriidae\", \"Archaeobalanidae\")\n)\ntaxa\n\n         GENUS    species           family\n1      Patella    vulgata       patellidae\n2    Littorina   littoria     Littorinidae\n3 Halichondria    panacea  Halichondriidae\n4  Semibalanus balanoides Archaeobalanidae\n\nzone &lt;- data.frame(\n  genus = c(\n    \"Laminaria\", \"Halichondria\", \"Xanthoria\", \"Littorina\",\n    \"Semibalanus\", \"Fucus\"\n  ),\n  species = c(\n    \"digitata\", \"panacea\", \"parietina\", \"littoria\",\n    \"balanoides\", \"serratus\"\n  ),\n  zone = c(\"v_low\", \"low\", \"v_high\", \"low_mid\", \"high\", \"low_mid\")\n)\nzone\n\n         genus    species    zone\n1    Laminaria   digitata   v_low\n2 Halichondria    panacea     low\n3    Xanthoria  parietina  v_high\n4    Littorina   littoria low_mid\n5  Semibalanus balanoides    high\n6        Fucus   serratus low_mid\n\n\nBecause both of our data frames contains at least one variable in common (species in our case) we can simply use the merge() function to create a new data frame called taxa_zone.\n\ntaxa_zone &lt;- merge(x = taxa, y = zone)\ntaxa_zone\n\n     species        GENUS           family        genus    zone\n1 balanoides  Semibalanus Archaeobalanidae  Semibalanus    high\n2   littoria    Littorina     Littorinidae    Littorina low_mid\n3    panacea Halichondria  Halichondriidae Halichondria     low\n\n\nNotice that the merged data frame contains only the rows that have species information in both data frames. There are also two columns called GENUS and genus because the merge() function treats these as two different variables that originate from the two data frames.\nIf we want to include all data from both data frames then we will need to use the all = TRUE argument. The missing values will be included as NA.\n\ntaxa_zone &lt;- merge(x = taxa, y = zone, all = TRUE)\ntaxa_zone\n\n     species        GENUS           family        genus    zone\n1 balanoides  Semibalanus Archaeobalanidae  Semibalanus    high\n2   digitata         &lt;NA&gt;             &lt;NA&gt;    Laminaria   v_low\n3   littoria    Littorina     Littorinidae    Littorina low_mid\n4    panacea Halichondria  Halichondriidae Halichondria     low\n5  parietina         &lt;NA&gt;             &lt;NA&gt;    Xanthoria  v_high\n6   serratus         &lt;NA&gt;             &lt;NA&gt;        Fucus low_mid\n7    vulgata      Patella       patellidae         &lt;NA&gt;    &lt;NA&gt;\n\n\nIf the variable names that you want to base the merge on are different in each data frame (for example GENUS and genus) you can specify the names in the first data frame (known as x) and the second data frame (known as y) using the by.x = and by.y = arguments.\n\ntaxa_zone &lt;- merge(x = taxa, y = zone, by.x = \"GENUS\", by.y = \"genus\", all = TRUE)\ntaxa_zone\n\n         GENUS  species.x           family  species.y    zone\n1        Fucus       &lt;NA&gt;             &lt;NA&gt;   serratus low_mid\n2 Halichondria    panacea  Halichondriidae    panacea     low\n3    Laminaria       &lt;NA&gt;             &lt;NA&gt;   digitata   v_low\n4    Littorina   littoria     Littorinidae   littoria low_mid\n5      Patella    vulgata       patellidae       &lt;NA&gt;    &lt;NA&gt;\n6  Semibalanus balanoides Archaeobalanidae balanoides    high\n7    Xanthoria       &lt;NA&gt;             &lt;NA&gt;  parietina  v_high\n\n\nOr using multiple variable names.\n\ntaxa_zone &lt;- merge(\n  x = taxa, y = zone, by.x = c(\"species\", \"GENUS\"),\n  by.y = c(\"species\", \"genus\"), all = TRUE\n)\ntaxa_zone\n\n     species        GENUS           family    zone\n1 balanoides  Semibalanus Archaeobalanidae    high\n2   digitata    Laminaria             &lt;NA&gt;   v_low\n3   littoria    Littorina     Littorinidae low_mid\n4    panacea Halichondria  Halichondriidae     low\n5  parietina    Xanthoria             &lt;NA&gt;  v_high\n6   serratus        Fucus             &lt;NA&gt; low_mid\n7    vulgata      Patella       patellidae    &lt;NA&gt;\n\n\n\n3.4.6 Reshaping data frames\nReshaping data into different formats is a common task. With rectangular type data (data frames have the same number of rows in each column) there are two main data frame shapes that you will come across: the ‘long’ format (sometimes called stacked) and the ‘wide’ format. An example of a long format data frame is given below. We can see that each row is a single observation from an individual subject and each subject can have multiple rows. This results in a single column of our measurement.\n\nlong_data &lt;- data.frame(\n  subject = rep(c(\"A\", \"B\", \"C\", \"D\"), each = 3),\n  sex = rep(c(\"M\", \"F\", \"F\", \"M\"), each = 3),\n  condition = rep(c(\"control\", \"cond1\", \"cond2\"), times = 4),\n  measurement = c(\n    12.9, 14.2, 8.7, 5.2, 12.6, 10.1, 8.9,\n    12.1, 14.2, 10.5, 12.9, 11.9\n  )\n)\nlong_data\n\n   subject sex condition measurement\n1        A   M   control        12.9\n2        A   M     cond1        14.2\n3        A   M     cond2         8.7\n4        B   F   control         5.2\n5        B   F     cond1        12.6\n6        B   F     cond2        10.1\n7        C   F   control         8.9\n8        C   F     cond1        12.1\n9        C   F     cond2        14.2\n10       D   M   control        10.5\n11       D   M     cond1        12.9\n12       D   M     cond2        11.9\n\n\nWe can also format the same data in the wide format as shown below. In this format we have multiple observations from each subject in a single row with measurements in different columns (control, cond1 and cond2). This is a common format when you have repeated measurements from sampling units.\n\nwide_data &lt;- data.frame(\n  subject = c(\"A\", \"B\", \"C\", \"D\"),\n  sex = c(\"M\", \"F\", \"F\", \"M\"),\n  control = c(12.9, 5.2, 8.9, 10.5),\n  cond1 = c(14.2, 12.6, 12.1, 12.9),\n  cond2 = c(8.7, 10.1, 14.2, 11.9)\n)\nwide_data\n\n  subject sex control cond1 cond2\n1       A   M    12.9  14.2   8.7\n2       B   F     5.2  12.6  10.1\n3       C   F     8.9  12.1  14.2\n4       D   M    10.5  12.9  11.9\n\n\nWhilst there’s no inherent problem with either of these formats we will sometimes need to convert between the two because some functions will require a specific format for them to work. The most common format is the long format.\nThere are many ways to convert between these two formats but we’ll use the melt() and dcast() functions from the reshape2 package (you will need to install this package first). The melt() function is used to convert from wide to long formats. The first argument for the melt() function is the data frame we want to melt (in our case wide_data). The id.vars = c(\"subject\", \"sex\") argument is a vector of the variables you want to stack, the measured.vars = c(\"control\", \"cond1\", \"cond2\") argument identifies the columns of the measurements in different conditions, the variable.name = \"condition\" argument specifies what you want to call the stacked column of your different conditions in your output data frame and value.name = \"measurement\" is the name of the column of your stacked measurements in your output data frame.\n\nlibrary(reshape2)\nwide_data # remind ourselves what the wide format looks like\n\n  subject sex control cond1 cond2\n1       A   M    12.9  14.2   8.7\n2       B   F     5.2  12.6  10.1\n3       C   F     8.9  12.1  14.2\n4       D   M    10.5  12.9  11.9\n\n# convert wide to long\nmy_long_df &lt;- melt(\n  data = wide_data, id.vars = c(\"subject\", \"sex\"),\n  measured.vars = c(\"control\", \"cond1\", \"cond2\"),\n  variable.name = \"condition\", value.name = \"measurement\"\n)\nmy_long_df\n\n   subject sex condition measurement\n1        A   M   control        12.9\n2        B   F   control         5.2\n3        C   F   control         8.9\n4        D   M   control        10.5\n5        A   M     cond1        14.2\n6        B   F     cond1        12.6\n7        C   F     cond1        12.1\n8        D   M     cond1        12.9\n9        A   M     cond2         8.7\n10       B   F     cond2        10.1\n11       C   F     cond2        14.2\n12       D   M     cond2        11.9\n\n\nThe dcast() function is used to convert from a long format data frame to a wide format data frame. The first argument is again is the data frame we want to cast (long_data for this example). The second argument is in formula syntax. The subject + sex bit of the formula means that we want to keep these columns separate, and the ~ condition part is the column that contains the labels that we want to split into new columns in our new data frame. The value.var = \"measurement\" argument is the column that contains the measured data.\n\nlong_data # remind ourselves what the long format look like\n\n   subject sex condition measurement\n1        A   M   control        12.9\n2        A   M     cond1        14.2\n3        A   M     cond2         8.7\n4        B   F   control         5.2\n5        B   F     cond1        12.6\n6        B   F     cond2        10.1\n7        C   F   control         8.9\n8        C   F     cond1        12.1\n9        C   F     cond2        14.2\n10       D   M   control        10.5\n11       D   M     cond1        12.9\n12       D   M     cond2        11.9\n\n# convert long to wide\nmy_wide_df &lt;- dcast(\n  data = long_data, subject + sex ~ condition,\n  value.var = \"measurement\"\n)\nmy_wide_df\n\n  subject sex cond1 cond2 control\n1       A   M  14.2   8.7    12.9\n2       B   F  12.6  10.1     5.2\n3       C   F  12.1  14.2     8.9\n4       D   M  12.9  11.9    10.5",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#introduction-to-the-tidyverse",
    "href": "03-data.html#introduction-to-the-tidyverse",
    "title": "3  Data",
    "section": "\n3.5 Introduction to the tidyverse\n",
    "text": "3.5 Introduction to the tidyverse\n\nit seems it is not super tidy in here and we need to improve that",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#summarising-data-frames",
    "href": "03-data.html#summarising-data-frames",
    "title": "3  Data",
    "section": "\n3.6 Summarising data frames",
    "text": "3.6 Summarising data frames\nNow that we’re able to manipulate and extract data from our data frames our next task is to start exploring and getting to know our data. In this section we’ll start producing tables of useful summary statistics of the variables in our data frame and in the next two Chapters we’ll cover visualising our data with base R graphics and using the ggplot2 package.\nA really useful starting point is to produce some simple summary statistics of all of the variables in our unicorns data frame using the summary() function.\n\nsummary(unicorns)\n\n    p_care              food        block         height           weight      \n Length:96          low   :32   Min.   :1.0   Min.   : 1.200   Min.   : 5.790  \n Class :character   medium:32   1st Qu.:1.0   1st Qu.: 4.475   1st Qu.: 9.027  \n Mode  :character   high  :32   Median :1.5   Median : 6.450   Median :11.395  \n                                Mean   :1.5   Mean   : 6.840   Mean   :12.155  \n                                3rd Qu.:2.0   3rd Qu.: 9.025   3rd Qu.:14.537  \n                                Max.   :2.0   Max.   :17.200   Max.   :23.890  \n   mane_size       fluffyness       horn_rings    \n Min.   : 5.80   Min.   :  5.80   Min.   : 1.000  \n 1st Qu.:11.07   1st Qu.: 39.05   1st Qu.: 4.000  \n Median :13.45   Median : 70.05   Median : 6.000  \n Mean   :14.05   Mean   : 79.78   Mean   : 7.062  \n 3rd Qu.:16.45   3rd Qu.:113.28   3rd Qu.: 9.000  \n Max.   :49.20   Max.   :189.60   Max.   :17.000  \n\n\nFor numeric variables (i.e. height, weight etc) the mean, minimum, maximum, median, first (lower) quartile and third (upper) quartile are presented. For factor variables (i.e. care and food) the number of observations in each of the factor levels is given. If a variable contains missing data then the number of NA values is also reported.\nIf we wanted to summarise a smaller subset of variables in our data frame we can use our indexing skills in combination with the summary() function. For example, to summarise only the height, weight, mane_size and fluffyness variables we can include the appropriate column indexes when using the [ ]. Notice we include all rows by not specifying a row index.\n\nsummary(unicorns[, 4:7])\n\n     height           weight         mane_size       fluffyness    \n Min.   : 1.200   Min.   : 5.790   Min.   : 5.80   Min.   :  5.80  \n 1st Qu.: 4.475   1st Qu.: 9.027   1st Qu.:11.07   1st Qu.: 39.05  \n Median : 6.450   Median :11.395   Median :13.45   Median : 70.05  \n Mean   : 6.840   Mean   :12.155   Mean   :14.05   Mean   : 79.78  \n 3rd Qu.: 9.025   3rd Qu.:14.537   3rd Qu.:16.45   3rd Qu.:113.28  \n Max.   :17.200   Max.   :23.890   Max.   :49.20   Max.   :189.60  \n\n# or equivalently\n# summary(unicorns[, c(\"height\", \"weight\", \"mane_size\", \"fluffyness\")])\n\nAnd to summarise a single variable.\n\nsummary(unicorns$mane_size)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.80   11.07   13.45   14.05   16.45   49.20 \n\n# or equivalently\n# summary(unicorns[, 6])\n\nAs you’ve seen above, the summary() function reports the number of observations in each level of our factor variables. Another useful function for generating tables of counts is the table() function. The table() function can be used to build contingency tables of different combinations of factor levels. For example, to count the number of observations for each level of food\n\ntable(unicorns$food)\n\n\n   low medium   high \n    32     32     32 \n\n\nWe can extend this further by producing a table of counts for each combination of food and care factor levels.\n\ntable(unicorns$food, unicorns$p_care)\n\n        \n         care no_care\n  low      16      16\n  medium   16      16\n  high     16      16\n\n\nA more flexible version of the table() function is the xtabs() function. The xtabs() function uses a formula notation (~) to build contingency tables with the cross-classifying variables separated by a + symbol on the right hand side of the formula. xtabs() also has a useful data = argument so you don’t have to include the data frame name when specifying each variable.\n\nxtabs(~ food + p_care, data = unicorns)\n\n        p_care\nfood     care no_care\n  low      16      16\n  medium   16      16\n  high     16      16\n\n\nWe can even build more complicated contingency tables using more variables. Note, in the example below the xtabs() function has quietly coerced our block variable to a factor.\n\nxtabs(~ food + p_care + block, data = unicorns)\n\n, , block = 1\n\n        p_care\nfood     care no_care\n  low       8       8\n  medium    8       8\n  high      8       8\n\n, , block = 2\n\n        p_care\nfood     care no_care\n  low       8       8\n  medium    8       8\n  high      8       8\n\n\nAnd for a nicer formatted table we can nest the xtabs() function inside the ftable() function to ‘flatten’ the table.\n\nftable(xtabs(~ food + p_care + block, data = unicorns))\n\n               block 1 2\nfood   p_care           \nlow    care          8 8\n       no_care       8 8\nmedium care          8 8\n       no_care       8 8\nhigh   care          8 8\n       no_care       8 8\n\n\nWe can also summarise our data for each level of a factor variable. Let’s say we want to calculate the mean value of height for each of our low, meadium and high levels of food. To do this we will use the mean() function and apply this to the height variable for each level of food using the tapply() function.\n\ntapply(unicorns$height, unicorns$food, mean)\n\n     low   medium     high \n5.853125 7.012500 7.653125 \n\n\nThe tapply() function is not just restricted to calculating mean values, you can use it to apply many of the functions that come with R or even functions you’ve written yourself (see Chapter 5 for more details). For example, we can apply the sd() function to calculate the standard deviation for each level of food or even the summary() function.\n\ntapply(unicorns$height, unicorns$food, sd)\n\n     low   medium     high \n2.828425 3.005345 3.483323 \n\ntapply(unicorns$height, unicorns$food, summary)\n\n$low\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.800   3.600   5.550   5.853   8.000  12.300 \n\n$medium\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.800   4.500   7.000   7.013   9.950  12.300 \n\n$high\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.200   5.800   7.450   7.653   9.475  17.200 \n\n\nNote, if the variable you want to summarise contains missing values (NA) you will also need to include an argument specifying how you want the function to deal with the NA values. We saw an example if this in Section 2.5.5 where the mean() function returned an NA when we had missing data. To include the na.rm = TRUE argument we simply add this as another argument when using tapply().\n\ntapply(unicorns$height, unicorns$food, mean, na.rm = TRUE)\n\n     low   medium     high \n5.853125 7.012500 7.653125 \n\n\nWe can also use tapply() to apply functions to more than one factor. The only thing to remember is that the factors need to be supplied to the tapply() function in the form of a list using the list() function. To calculate the mean height for each combination of food and care factor levels we can use the list(unicorns$food, unicorns$p_care) notation.\n\ntapply(unicorns$height, list(unicorns$food, unicorns$p_care), mean)\n\n         care no_care\nlow    8.0375 3.66875\nmedium 9.1875 4.83750\nhigh   9.6000 5.70625\n\n\nAnd if you get a little fed up with having to write unicorns$ for every variable you can nest the tapply() function inside the with() function. The with() function allows R to evaluate an R expression with respect to a named data object (in this case unicorns).\n\nwith(unicorns, tapply(height, list(food, p_care), mean))\n\n         care no_care\nlow    8.0375 3.66875\nmedium 9.1875 4.83750\nhigh   9.6000 5.70625\n\n\nThe with() function also works with many other functions and can save you alot of typing!\nAnother really useful function for summarising data is the aggregate() function. The aggregate() function works in a very similar way to tapply() but is a bit more flexible.\nFor example, to calculate the mean of the variables height, weight, mane_size and fluffyness for each level of food.\n\naggregate(unicorns[, 4:7], by = list(food = unicorns$food), FUN = mean)\n\n    food   height    weight mane_size fluffyness\n1    low 5.853125  8.652812  11.14375    45.1000\n2 medium 7.012500 11.164062  13.83125    67.5625\n3   high 7.653125 16.646875  17.18125   126.6875\n\n\nIn the code above we have indexed the columns we want to summarise in the unicorns data frame using unicorns[, 4:7]. The by = argument specifies a list of factors (list(food = unicorns$food)) and the FUN = argument names the function to apply (mean in this example).\nSimilar to the tapply() function we can include more than one factor to apply a function to. Here we calculate the mean values for each combination of food and care\n\naggregate(unicorns[, 4:7], by = list(\n  food = unicorns$food,\n  p_care = unicorns$p_care\n), FUN = mean)\n\n    food  p_care  height    weight mane_size fluffyness\n1    low    care 8.03750  9.016250   9.96250   30.30625\n2 medium    care 9.18750 11.011250  13.48750   40.59375\n3   high    care 9.60000 16.689375  15.54375   98.05625\n4    low no_care 3.66875  8.289375  12.32500   59.89375\n5 medium no_care 4.83750 11.316875  14.17500   94.53125\n6   high no_care 5.70625 16.604375  18.81875  155.31875\n\n\nWe can also use the aggregate() function in a different way by using the formula method (as we did with xtabs()). On the left hand side of the formula (~) we specify the variable we want to apply the mean function on and to the right hand side our factors separated by a + symbol. The formula method also allows you to use the data = argument for convenience.\n\naggregate(height ~ food + p_care, FUN = mean, data = unicorns)\n\n    food  p_care  height\n1    low    care 8.03750\n2 medium    care 9.18750\n3   high    care 9.60000\n4    low no_care 3.66875\n5 medium no_care 4.83750\n6   high no_care 5.70625\n\n\nOne advantage of using the formula method is that we can also use the subset = argument to apply the function to subsets of the original data. For example, to calculate the mean height for each combination of the food and care levels but only for those unicorns that have less than 7 horn_rings.\n\naggregate(height ~ food + p_care, FUN = mean, subset = horn_rings &lt; 7, data = unicorns)\n\n    food  p_care   height\n1    low    care 8.176923\n2 medium    care 8.570000\n3   high    care 7.900000\n4    low no_care 3.533333\n5 medium no_care 5.316667\n6   high no_care 3.850000\n\n\nOr for only those unicorns in block 1.\n\naggregate(height ~ food + p_care, FUN = mean, subset = block == \"1\", data = unicorns)\n\n    food  p_care  height\n1    low    care  8.7500\n2 medium    care  9.5375\n3   high    care 10.0375\n4    low no_care  3.3250\n5 medium no_care  5.2375\n6   high no_care  5.9250",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#exporting-data",
    "href": "03-data.html#exporting-data",
    "title": "3  Data",
    "section": "\n3.7 Exporting data",
    "text": "3.7 Exporting data\nBy now we hope you’re getting a feel for how powerful and useful R is for manipulating and summarising data (and we’ve only really scratched the surface). One of the great benefits of doing all your data wrangling in R is that you have a permanent record of all the things you’ve done to your data. Gone are the days of making undocumented changes in Excel or Calc! By treating your data as ‘read only’ and documenting all of your decisions in R you will have made great strides towards making your analysis more reproducible and transparent to others. It’s important to realise, however, that any changes you’ve made to your data frame in R will not change the original data file you imported into R (and that’s a good thing). Happily it’s straightforward to export data frames to external files in a wide variety of formats.\n\n3.7.1 Export functions\nThe main workhorse function for exporting data frames is the write.table() function. As with the read.table() function, the write.table() function is very flexible with lots of arguments to help customise it’s behaviour. As an example, let’s take our original unicorns data frame, do some useful stuff to it and then export these changes to an external file.\nLet’s order the rows in the data frame in ascending order of height within each level food. We will also apply a square root transformation on the number of horn rings variable (horn_rings) and a log10 transformation on the height variable and save these as additional columns in our data frame (hopefully this will be somewhat familiar to you!).\n\nunicorns_df2 &lt;- unicorns[order(unicorns$food, unicorns$height), ]\nunicorns_df2$horn_rings_sqrt &lt;- sqrt(unicorns_df2$horn_rings)\nunicorns_df2$log10_height &lt;- log10(unicorns_df2$height)\nstr(unicorns_df2)\n\n'data.frame':   96 obs. of  10 variables:\n $ p_care         : chr  \"no_care\" \"no_care\" \"no_care\" \"no_care\" ...\n $ food           : Factor w/ 3 levels \"low\",\"medium\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ block          : int  1 1 1 2 1 2 2 2 1 2 ...\n $ height         : num  1.8 2.2 2.3 2.4 3 3.1 3.2 3.3 3.7 3.7 ...\n $ weight         : num  6.01 9.97 7.28 9.1 9.93 8.74 7.45 8.92 7.03 8.1 ...\n $ mane_size      : num  17.6 9.6 13.8 14.5 12 16.1 14.1 11.6 7.9 10.5 ...\n $ fluffyness     : num  46.2 63.1 32.8 78.7 56.6 39.1 38.1 55.2 36.7 60.5 ...\n $ horn_rings     : int  4 2 6 8 6 3 4 6 5 6 ...\n $ horn_rings_sqrt: num  2 1.41 2.45 2.83 2.45 ...\n $ log10_height   : num  0.255 0.342 0.362 0.38 0.477 ...\n\n\nNow we can export our new data frame unicorns_df2 using the write.table() function. The first argument is the data frame you want to export (unicorns_df2 in our example). We then give the filename (with file extension) and the file path in either single or double quotes using the file = argument. In this example we’re exporting the data frame to a file called unicorns_04_12.txt in the data directory. The col.names = TRUE argument indicates that the variable names should be written in the first row of the file and the row.names = FALSE argument stops R from including the row names in the first column of the file. Finally, the sep = \"\\t\" argument indicates that a Tab should be used as the delimiter in the exported file.\n\nwrite.csv(unicorns_df2,\n  file = \"data/unicorns_04_12.txt\", col.names = TRUE,\n  row.names = FALSE, sep = \"\\t\"\n)\n\nAs we saved the file as a tab delimited text file we could open this file in any text editor. Perhaps a more familiar option would be to open the file in Excel. First start Excel and then select File -&gt; Open .. in the main menu and then select our unicorns_04_12.txt file to open. Next. choose the ‘Tab’ option to set the delimiter and then click on the ‘Finish’ button.\n\n\n\n\n\n\n\n\nWe can of course export our files in a variety of other formats. Another popular option is to export files in csv (comma separated values) format. We can do this using the write.table() function by setting the separator argument to sep = \",\".\n\nwrite.table(unicorns_df2,\n  file = \"data/unicorns_04_12.csv\", col.names = TRUE,\n  row.names = FALSE, sep = \",\"\n)\n\nOr alternatively by using the convenience function write.csv(). Notice that we don’t need to set the sep = \",\" or col.names = TRUE arguments as these are the defaults when using the read.csv() function.\n\nwrite.csv(unicorns_df2, file = \"data/unicorns_04_12.csv\", row.names = FALSE)\n\n\n3.7.2 Other export functions\nAs with importing data files into R, there are also many alternative functions for exporting data to external files beyond the write.table() function. If you followed the ‘Other import functions’ Section 3.3.4 of this Chapter you will already have the required packages installed.\nThe fwrite() function from the read.table package is very efficient at exporting large data objects and is much faster than the write.table() function. It’s also quite simple to use as it has most of the same arguments as write.table(). To export a tab delimited text file we just need to specify the data frame name, the output file name and file path and the separator between columns.\n\nlibrary(read.table)\nfwrite(unicorns_df2, file = \"data/unicorns_04_12.txt\", sep = \"\\t\")\n\nTo export a csv delimited file it’s even easier as we don’t even need to include the sep = argument.\n\nlibrary(read.table)\nfwrite(unicorns_df2, file = \"data/unicorns_04_12.csv\")\n\nThe readr package also comes with two useful functions for quickly writing data to external files: the write_tsv() function for writing tab delimited files and the write_csv() function for saving comma separated values (csv) files.\n\nlibrary(readr)\nwrite_tsv(unicorns_df2, path = \"data/unicorns_04_12.txt\")\n\nwrite_csv(unicorns_df2, path = \"data/unicorns_04_12.csv\")",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "03-data.html#footnotes",
    "href": "03-data.html#footnotes",
    "title": "3  Data",
    "section": "",
    "text": "For space and simplicity we are just showing the first and last five rows↩︎",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html",
    "href": "04-graphics_short.html",
    "title": "4  Figures",
    "section": "",
    "text": "4.1 Simple base R plots\nThere are many functions in R to produce plots ranging from the very basic to the highly complex. It’s impossible to cover every aspect of producing graphics in R in this book so we’ll introduce you to most of the common methods of graphing data and describe how to customise your graphs later on in Section 4.5.\nThe most common high level function used to produce plots in R is (rather unsurprisingly) the plot() function. For example, let’s plot the weight of unicorns from our unicorns data frame which we imported in Section 3.3.2.\nunicorns &lt;- read.csv(file = \"data/unicorns.csv\")\n\nplot(unicorns$weight)\nR has plotted the values of weight (on the y axis) against an index since we are only plotting one variable to plot. The index is just the order of the weight values in the data frame (1 first in the data frame and 97 last). The weight variable name has been automatically included as a y axis label and the axes scales have been automatically set.\nIf we’d only included the variable weight rather than unicorns$weight, the plot() function will display an error as the variable weight only exists in the unicorns data frame object.\nplot(weight)\n## Error in plot(weight) : object 'weight' not found\nAs many of the base R plotting functions don’t have a data = argument to specify the data frame name directly we can use the with() function in combination with plot() as a shortcut.\nwith(unicorns, plot(weight))\nTo plot a scatterplot of one numeric variable against another numeric variable we just need to include both variables as arguments when using the plot() function. For example to plot fluffyness on the y axis and weight of the x axis.\nplot(x = unicorns$weight, y = unicorns$fluffyness)\nThere is an equivalent approach for these types of plots which often causes some confusion at first. You can also use the formula notation when using the plot() function. However, in contrast to the previous method the formula method requires you to specify the y axis variable first, then a ~ and then our x axis variable.\nplot(fluffyness ~ weight, data = unicorns)\nBoth of these two approaches are equivalent so we suggest that you just choose the one you prefer and go with it.\nYou can also specify the type of graph you wish to plot using the argument type =. You can plot just the points (type = \"p\", this is the default), just lines (type = \"l\"), both points and lines connected (type = \"b\"), both points and lines with the lines running through the points (type = \"o\") and empty points joined by lines (type = \"c\"). For example, let’s use our skills from Section 2.4 to generate two vectors of numbers (my_x and my_y) and then plot one against the other using different type = values to see what type of plots are produced. Don’t worry about the par(mfrow = c(2, 2)) line of code yet. We’re just using this to split the plotting device so we can fit all four plots on the same device to save some space. See Section 4.4 in the Chapter for more details about this. The top left plot is type = \"l\", the top right type = \"b\", bottom left type = \"o\" and bottom right is type = \"c\".\nmy_x &lt;- 1:10\nmy_y &lt;- seq(from = 1, to = 20, by = 2)\n\npar(mfrow = c(2, 2))\nplot(my_x, my_y, type = \"l\")\nplot(my_x, my_y, type = \"b\")\nplot(my_x, my_y, type = \"o\")\nplot(my_x, my_y, type = \"c\")\nAdmittedly the plots we’ve produced so far don’t look anything particularly special. However, the plot() function is incredibly versatile and can generate a large range of plots which you can customise to your own taste. We’ll cover how to customise ggplots in Section 4.5. As a quick aside, the plot() function is also what’s known as a generic function which means it can change its default behaviour depending on the type of object used as an argument. You will see an example of this in Section 8.1 where we use the plot() function to generate diagnostic plots of residuals from a linear model object (bet you can’t wait!).",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#ggplot2",
    "href": "04-graphics_short.html#ggplot2",
    "title": "4  Figures",
    "section": "\n4.2 ggplot2",
    "text": "4.2 ggplot2\nAs mentioned earlier ggplot grammar requires several elements to produce a graphic (Figure 4.1) and a minimum of 3 are required:\n\na data frame\na mapping system defining x and y\na geometry layer\n\nThe data and mapping are provided within the called to the ggplot() function with the data and mapping arguments. The geometry layer is added using specific functions.\nIn fact all layers are needed but default simple values of the other layers are automatically provided.\nTo redo the Figure 4.2, that contain only a scatterplot of point we can use the geom_point() function.\n\nggplot(\n  data = unicorns,\n  mapping = aes(x = weight, y = fluffyness)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nNow that we have basic understanding of ggplotwe can explore some graphics using both base R and ggplot code",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#simple-plots",
    "href": "04-graphics_short.html#simple-plots",
    "title": "4  Figures",
    "section": "\n4.3 Simple plots",
    "text": "4.3 Simple plots\n\n4.3.1 Scatterplots\nSimple type of plots really useful to have a look at the relation between 2 variables for example. Here are the code to do it using base R (Figure 4.2)\n\nplot(fluffyness ~ weight, data = unicorns)\n\nor ggplot (Figure 4.3)\n\nggplot(\n  data = unicorns,\n  mapping = aes(x = weight, y = fluffyness)\n) +\n  geom_point()\n\nOne gig advantage of ggplot for simple scatterplot is the ease with which we can add a regression, smoother (loes or gam) line to the plot using stat_smooth()function to add a statistic layer to the plot.\n\nggplot(\n  data = unicorns,\n  mapping = aes(x = weight, y = fluffyness)\n) +\n  geom_point() +\n  stat_smooth()\n\n\n\n\n\n\n\n\n4.3.2 Histograms\nFrequency histograms are useful when you want to get an idea about the distribution of values in a numeric variable. Using base R, the hist() function takes a numeric vector as its main argument. In ggplot, we need to use geom_histogram(). Let’s generate a histogram of the height values.\nWith base R\n\nhist(unicorns$height)\n\n\n\n\n\n\n\nwith ggplot2\n\nggplot(unicorns, aes(x = height)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nThe hist() and geom_histogram() function automatically creates the breakpoints (or bins) in the histogram unless you specify otherwise by using the breaks = argument. For example, let’s say we want to plot our histogram with breakpoints every 1 cm unicorns height. We first generate a sequence from zero to the maximum value of height (18 rounded up) in steps of 1 using the seq() function. We can then use this sequence with the breaks = argument. While we’re at it, let’s also replace the ugly title for something a little better using the main = argument\n\nbrk &lt;- seq(from = 0, to = 18, by = 1)\nhist(unicorns$height, breaks = brk, main = \"Unicorn height\")\n\n\n\n\n\n\n\n\nbrk &lt;- seq(from = 0, to = 18, by = 1)\nggplot(unicorns, aes(x = height)) +\n  geom_histogram(breaks = brk) +\n  ggtitle(\"Unicorn height\")\n\n\n\n\n\n\n\nYou can also display the histogram as a proportion rather than a frequency by using the freq = FALSE argument to hist() or indicating aes(y = after_stat(density)) in geom_histogram().\n\nbrk &lt;- seq(from = 0, to = 18, by = 1)\nhist(unicorns$height,\n  breaks = brk, main = \"Unicorn height\",\n  freq = FALSE\n)\nggplot(unicorns, aes(x = height)) +\n  geom_histogram(aes(y = after_stat(density)), breaks = brk) +\n  ggtitle(\"Unicorn height\")\n\nAn alternative to plotting just a straight up histogram is to add a [kernel density][kernel-dens] curve to the plot. In base R, you first need to compute the kernel density estimates using the density() and then ad the estimates to plot as a line using the lines() function.\n\ndens &lt;- density(unicorns$height)\nhist(unicorns$height,\n  breaks = brk, main = \"Unicorn height\",\n  freq = FALSE\n)\nlines(dens)\n\n\n\n\n\n\n\nWith ggplot, you can simply add the geom_density() layer to the plot\n\nggplot(unicorns, aes(x = height)) +\n  geom_histogram(aes(y = after_stat(density)), breaks = brk) +\n  geom_density() +\n  ggtitle(\"Unicorn height\")\n\n\n\n\n\n\n\n\n4.3.3 Box plots\nOK, we’ll just come and out and say it, we love boxplots and their close relation the violin plot. Boxplots (or box-and-whisker plots to give them their full name) are very useful when you want to graphically summarise the distribution of a variable, identify potential unusual values and compare distributions between different groups. The reason we love them is their ease of interpretation, transparency and relatively high data-to-ink ratio (i.e. they convey lots of information efficiently). We suggest that you try to use boxplots as much as possible when exploring your data and avoid the temptation to use the more ubiquitous bar plot (even with standard error or 95% confidence intervals bars). The problem with bar plots (aka dynamite plots) is that they hide important information from the reader such as the distribution of the data and assume that the error bars (or confidence intervals) are symmetric around the mean. Of course, it’s up to you what you do but if you’re tempted to use bar plots just search for ‘dynamite plots are evil’ or see [here][dynamite-plot1] or [here][dynamite-plot2] for a fuller discussion.\nTo create a boxplot in R we use the boxplot() function. For example, let’s create a boxplot of the variable weight from our unicorns data frame. We can also include a y axis label using the ylab = argument.\n\nboxplot(unicorns$weight, ylab = \"weight (g)\")\n\n\n\n\n\n\n\n\nggplot(unicorns, aes(y = weight)) +\n  geom_boxplot() +\n  labs(y = \"weight (g)\")\n\n\n\n\n\n\n\nThe thick horizontal line in the middle of the box is the median value of weight (around 11 g). The upper line of the box is the upper quartile (75th percentile) and the lower line is the lower quartile (25th percentile). The distance between the upper and lower quartiles is known as the inter quartile range and represents the values of weight for 50% of the data. The dotted vertical lines are called the whiskers and their length is determined as 1.5 x the inter quartile range. Data points that are plotted outside the the whiskers represent potential unusual observations. This doesn’t mean they are unusual, just that they warrant a closer look. We recommend using boxplots in combination with Cleveland dotplots to identify potential unusual observations (see the Section 4.3.5 for more details). The neat thing about boxplots is that they not only provide a measure of central tendency (the median value) they also give you an idea about the distribution of the data. If the median line is more or less in the middle of the box (between the upper and lower quartiles) and the whiskers are more or less the same length then you can be reasonably sure the distribution of your data is symmetrical.\nIf we want examine how the distribution of a variable changes between different levels of a factor we need to use the formula notation with the boxplot() function. For example, let’s plot our weight variable again, but this time see how this changes with each level of food. When we use the formula notation with boxplot() we can use the data = argument to save some typing. We’ll also introduce an x axis label using the xlab = argument.\n\nboxplot(weight ~ food,\n  data = unicorns,\n  ylab = \"Weight (g)\", xlab = \"food level\"\n)\n\n\n\n\n\n\n\n\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_boxplot() +\n  labs(y = \"Weight (g)\", x = \"food Concentration\")\n\n\n\n\n\n\n\nThe factor levels are plotted in the same order defined by our factor variable food (often alphabetically). To change the order we need to change the order of our levels of the food factor in our data frame using the factor() function and then re-plot the graph. Let’s plot our boxplot with our factor levels going from low to high.\n\nunicorns$food &lt;- factor(unicorns$food,\n  levels = c(\"low\", \"medium\", \"high\")\n)\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_boxplot() +\n  labs(y = \"Weight (g)\", x = \"food Concentration\")\n\n\n\n\n\n\n\nWe can also group our variables by two factors in the same plot. Let’s plot our weight variable but this time plot a separate box for each food and parental care treatment (p_care) combination.\n\nboxplot(weight ~ food * p_care,\n  data = unicorns,\n  ylab = \"weight (g)\", xlab = \"food level\"\n)\n\n\n\n\n\n\n\n\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_boxplot() +\n  labs(y = \"Weight (g)\", x = \"food Concentration\") +\n  facet_grid(.\n  ~ p_care)\n\n\n\n\n\n\n\nThis plot looks much better in ggplot with the use of facet_grid allowing to make similar plots as a function of a third (or even fourth) variable.\n\n4.3.4 Violin plots\nViolin plots are like a combination of a boxplot and a kernel density plot (you saw an example of a kernel density plot in the histogram section above) all rolled into one figure. We can create a violin plot in R using the vioplot() function from the vioplot package. You’ll need to first install this package using install.packages('vioplot') function as usual. The nice thing about the vioplot() function is that you use it in pretty much the same way you would use the boxplot() function. We’ll also use the argument col = \"lightblue\" to change the fill colour to light blue.\n\nlibrary(vioplot)\nvioplot(weight ~ food,\n  data = unicorns,\n  ylab = \"weight (g)\", xlab = \"food Concentration\",\n  col = \"lightblue\"\n)\n\n\n\n\n\n\n\nIn the violin plot above we have our familiar boxplot for each food level but this time the median value is represented by a white circle. Plotted around each boxplot is the kernel density plot which represents the distribution of the data for each food level.\n\nggplot(unicorns, aes(y = weight, x = food)) +\n  geom_violin() +\n  geom_boxplot(width = 0.1) +\n  labs(y = \"Weight (g)\", x = \"food Concentration\")\n\n\n\n\n\n\n\n\n4.3.5 Dot charts\nIdentifying unusual observations (aka outliers) in numeric variables is extremely important as they may influence parameter estimates in your statistical model or indicate an error in your data. A really useful (if undervalued) plot to help identify outliers is the Cleveland dotplot. You can produce a dotplot in R very simply by using the dotchart() function.\n\ndotchart(unicorns$height)\n\n\n\n\n\n\n\nIn the dotplot above the data from the height variable is plotted along the x axis and the data is plotted in the order it occurs in the unicorns data frame on the y axis (values near the top of the y axis occur later in the data frame with those lower down occurring at the beginning of the data frame). In this plot we have a single value extending to the right at about 17 cm but it doesn’t appear particularly large compared to the rest. An example of a dotplot with an unusual observation is given below.\n\n\n\n\n\n\n\n\nWe can also group the values in our height variable by a factor variable such as food using the groups = argument. This is useful for identifying unusual observations within a factor level that might be obscured when looking at all the data together.\n\ndotchart(unicorns$height, groups = unicorns$food)\n\n\n\n\n\n\n\n\n\nggdotchart(data = unicorns, x = \"height\", y = \"food\")\n\n\n\n\n\n\n\n\n4.3.6 Pairs plots\nPreviously in this Chapter we used the plot() function to create a scatterplot to explore the relationship between two numeric variables. With datasets that contain many numeric variables, it’s often handy to create multiple scatterplots to visualise relationships between all these variables. We could use the plot() function to create each of these plot individually, but a much easier way is to use the pairs() function. The pairs() function creates a multi-panel scatterplot (sometimes called a scatterplot matrix) which plots all combinations of variables. Let’s create a multi-panel scatterplot of all of the numeric variables in our unicorns data frame. Note, you may need to click on the ‘Zoom’ button in RStudio to display the plot clearly.\n\npairs(unicorns[, c(\n  \"height\", \"weight\", \"mane_size\",\n  \"fluffyness\", \"horn_rings\"\n)])\n\n\n\n\n\n\n# or we could use the equivalent\n# pairs(unicorns[, 4:8])\n\nInterpretation of the pairs plot takes a bit of getting used to. The panels on the diagonal give the variable names. The first row of plots displays the height variable on the y axis and the variables weight, mane_size, fluffyness and unicorns on the x axis for each of the four plots respectively. The next row of plots have weight on the y axis and height, mane_size, fluffyness and unicorns on the x axis. We interpret the rest of the rows in the same way with the last row displaying the unicorns variable on the y axis and the other variables on the x axis. Hopefully you’ll notice that the plots below the diagonal are the same plots as those above the diagonal just with the axis reversed.\nTo do pairs plot with ggplot, you nee the ggpairs()function from GGallypackage. The output is quite similar but you have only the lower part of the matrix of plots, you get a density plot on the diagonal and the correlations on the upper part of the plot.\n\nggpairs(unicorns[, c(\n  \"height\", \"weight\", \"mane_size\",\n  \"fluffyness\", \"horn_rings\"\n)])\n\n\n\n\n\n\n\nThe pairs() function can be tweak to do similar things and more but is more involved. Have a lok at the great help file for the pairs() function (?pairs)which provide all the details to do something like the plot below.\n\n\n\n\n\n\n\n\n\n4.3.7 Coplots\nWhen examining the relationship between two numeric variables, it is often useful to be able to determine whether a third variable is obscuring or changing any relationship. A really handy plot to use in these situations is a conditioning plot (also known as conditional scatterplot plot) which we can create in R by using the coplot() function. The coplot() function plots two variables but each plot is conditioned (|) by a third variable. This third variable can be either numeric or a factor. As an example, let’s look at how the relationship between the number of unicorns (unicorns variable) and the weight of unicorns changes dependent on mane_size. Note the coplot() function has a data = argument so no need to use the $ notation.\n\ncoplot(horn_rings ~ weight | mane_size, data = unicorns)\n\n\n\n\n\n\n\n\ngg_coplot(unicorns,\n  x = weight, y = horn_rings,\n  faceting = mane_size\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIt takes a little practice to interpret coplots. The number of unicorns is plotted on the y axis and the weight of unicorns on the x axis. The six plots show the relationship between these two variables for different ranges of leaf area. The bar plot at the top indicates the range of leaf area values for each of the plots. The panels are read from bottom left to top right along each row. For example, the bottom left panel shows the relationship between number of unicorns and weight for unicorns with the lowest range of leaf area values (approximately 5 - 11 cm2). The top right plot shows the relationship between unicorns and weight for unicorns with a leaf area ranging from approximately 16 - 50 cm2. Notice that the range of values for leaf area differs between panels and that the ranges overlap from panel to panel. The coplot() function does it’s best to split the data up to ensure there are an adequate number of data points in each panel. If you don’t want to produce plots with overlapping data in the panel you can set the overlap = argument to overlap = 0\nYou can also use the coplot() function with factor conditioning variables. With gg_coplot() you need to first set the factor as numeric before plotting and specify overlap=0. For example, we can examine the relationship between unicorns and weight variables conditioned on the factor food. The bottom left plot is the relationship between unicorns and weight for those unicorns in the low food treatment. The top left plot shows the same relationship but for unicorns in the high food treatment.\n\ncoplot(horn_rings ~ weight | food, data = unicorns)\n\n\n\n\n\n\n\n\nunicorns &lt;- mutate(unicorns, food_num = as.numeric(food))\ngg_coplot(unicorns,\n  x = weight, y = horn_rings,\n  faceting = food_num, overlap = 0\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n4.3.8 Summary of plot function\n\n\n\n\n\n\n\nGraph type\nggplot2\nBase R function\n\n\n\nscatterplot\ngeom_point()\nplot()\n\n\nfrequency histogram\ngeom_histogram()\nhist()\n\n\nboxplot\ngeom_boxplot()\nboxplot()\n\n\nCleveland dotplot\nggdotchart()\ndotchart()\n\n\nscatterplot matrix\nggpairs()\npairs()\n\n\nconditioning plot\ngg_coplot()\ncoplot()\n\n\n\nHopefully, you’re getting the idea that we can create really informative exploratory plots quite easily using either base R or ggplot graphics. Which one you use is entirely up to you (that’s the beauty of using R, you get to choose) and we happily mix and match to suit our needs. In the next section we cover how to customise your base R plots to get them to look exactly how you want.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#sec-mult-graphs",
    "href": "04-graphics_short.html#sec-mult-graphs",
    "title": "4  Figures",
    "section": "\n4.4 Multiple graphs",
    "text": "4.4 Multiple graphs\n\n4.4.1 Base R\nIn base R, one of the most common methods to plot multiple graphs is to use the main graphical function par() to split the plotting device up into a number of defined sections using the mfrow = argument. With this method, you first need to specify the number of rows and columns of plots you would like and then run the code for each plot. For example, to plot two graphs side by side we would use par(mfrow = c(1, 2)) to split the device into 1 row and two columns.\n\npar(mfrow = c(1, 2))\nplot(unicorns$weight, unicorns$fluffyness,\n  xlab = \"weight\",\n  ylab = \"Fluffyness\"\n)\nboxplot(fluffyness ~ food, data = unicorns, cex.axis = 0.6)\n\n\n\n\n\n\n\nOnce you’ve finished making your plots don’t forget to reset your plotting device back to normal with par(mfrow = c(1,1)).\n\n4.4.2 ggplot\nUsing ggplot in addition to the facet_grid() and facet_wrap functions allowing to easily repeat and organise multiple plots as a function of specific variables, there are multiple way of organising multiple ggplot together. The approach we recommend is using the package patchwork.\nFirst you will need to install (if you don’t have it yet) and make the patchwork 📦 package available.\n\ninstall.packages(\"patchwork\")\nlibrary(patchwork)\n\nAn important note: For those who have used base R to produce their figures and are familiar with using par(mfrow = c(2,2)) (which allows plotting of four figures in two rows and two columns) be aware that this does not work for ggplot2 objects. Instead you will need to use either the patchwork package or alternative packages such as gridArrange or cowplot or covert the ggplot2 objects to grobs.\nTo plot both of the plots together we need to assign each figure to a separate object and then use these objects when we use patchwork.\nSo we can generate 2 figures and assign them to objects. As you can see, the figures do not appear in the plot window. They will appear only when you call the object.\n\nfirst_figure &lt;- ggplot(\n  aes(x = height, y = fluffyness, color = food),\n  data = unicorns\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(block ~ p_care)\nsecond_figure &lt;- ggplot(\n  aes(x = weight, y = fluffyness, color = food),\n  data = unicorns\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_grid(block ~ p_care)\n\nWe have two immediate and simple options with patchwork; arrange figures on top of each other (specified with a /) or arrange figures side-by-side (specified with either a + or a |). Let’s try to plot both figures, one on top of the other.\n\nfirst_figure / second_figure\n\n\n\n\n\n\n\nPlay around: Try to create a side-by-side version of the above figure (hint: try the other operators).\nWe can take this one step further and assign nested patchwork figures to an object and use this in turn to create labels for individuals figures.\n\nnested_compare &lt;- first_figure / second_figure\n\nnested_compare +\n  plot_annotation(tag_levels = \"A\", tag_suffix = \")\")",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#sec-custom-plot",
    "href": "04-graphics_short.html#sec-custom-plot",
    "title": "4  Figures",
    "section": "\n4.5 Customising ggplots",
    "text": "4.5 Customising ggplots\nWent for a walk to be edited 🦄",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "04-graphics_short.html#sec-export-plots",
    "href": "04-graphics_short.html#sec-export-plots",
    "title": "4  Figures",
    "section": "\n4.6 Exporting plots",
    "text": "4.6 Exporting plots\nCreating plots in R is all well and good but what if you want to use these plots in your thesis, report or publication? One option is to click on the ‘Export’ button in the ‘Plots’ tab in RStudio. You can also export your plots from R to an external file by writing some code in your R script. The advantage of this approach is that you have a little more control over the output format and it also allows you to generate (or update) plots automatically whenever you run your script. You can export your plots in many different formats but the most common are, pdf, png, jpeg and tiff.\nBy default, R (and therefore RStudio) will direct any plot you create to the plot window. To save your plot to an external file you first need to redirect your plot to a different graphics device. You do this by using one of the many graphics device functions to start a new graphic device. For example, to save a plot in pdf format we will use the pdf() function. The first argument in the pdf() function is the filepath and filename of the file we want to save (don’t forget to include the .pdf extension). Once we’ve used the pdf() function we can then write all of the code we used to create our plot including any graphical parameters such as setting the margins and splitting up the plotting device. Once the code has run we need to close the pdf plotting device using the dev.off() function.\n\npdf(file = \"output/my_plot.pdf\")\npar(mar = c(4.1, 4.4, 4.1, 1.9), xaxs = \"i\", yaxs = \"i\")\nplot(unicorns$weight, unicorns$fluffyness,\n  xlab = \"weight (g)\",\n  ylab = expression(paste(\"shoot area (cm\"^\"2\", \")\")),\n  xlim = c(0, 30), ylim = c(0, 200), bty = \"l\",\n  las = 1, cex.axis = 0.8, tcl = -0.2,\n  pch = 16, col = \"dodgerblue1\", cex = 0.9\n)\ntext(x = 28, y = 190, label = \"A\", cex = 2)\ndev.off()\n\nIf we want to save this plot in png format we simply use the png() function in more or less the same way we used the pdf() function.\n\npng(\"output/my_plot.png\")\npar(mar = c(4.1, 4.4, 4.1, 1.9), xaxs = \"i\", yaxs = \"i\")\nplot(unicorns$weight, unicorns$fluffyness,\n  xlab = \"weight (g)\",\n  ylab = expression(paste(\"shoot area (cm\"^\"2\", \")\")),\n  xlim = c(0, 30), ylim = c(0, 200), bty = \"l\",\n  las = 1, cex.axis = 0.8, tcl = -0.2,\n  pch = 16, col = \"dodgerblue1\", cex = 0.9\n)\ntext(x = 28, y = 190, label = \"A\", cex = 2)\ndev.off()\n\nOther useful functions are; jpeg(), tiff() and bmp(). Additional arguments to these functions allow you to change the size, resolution and background colour of your saved images. See ?png for more details.\nggplot2 📦 provide a really useful function ggsave() function which simplify saving plots a lot but works only for ggplots.\nAfter producing a plot and seeing it in your IDE, you can simply run ggsave() with the adequate argument to save the last ggplot produced. You can of course, also, specify which plot to save.\n\nggsave(\"file.png\")\n\n\n\n\n\nWilkinson, L. 2005. The Grammar of Graphics. Springer Science & Business Media.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Figures</span>"
    ]
  },
  {
    "objectID": "05-programming.html",
    "href": "05-programming.html",
    "title": "5  Programming",
    "section": "",
    "text": "5.1 Looking behind the curtain\nA good way to start learning to program in R is to see what others have done. We can start by briefly peeking behind the curtain. With many functions in R, if you want to have a quick glance at the machinery behind the scenes, we can simply write the function name but without the ().\nNote that to view the source code of base R packages (those that come with R) requires some additional steps which we won’t cover here (see this link if you’re interested), but for most other packages that you install yourself, generally entering the function name without () will show the source code of the function.\nWhat can have a look at the function to fit a linear model lm()\nlm\n\nfunction (formula, data, subset, weights, na.action, method = \"qr\", \n    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, \n    contrasts = NULL, offset, ...) \n{\n    ret.x &lt;- x\n    ret.y &lt;- y\n    cl &lt;- match.call()\n    mf &lt;- match.call(expand.dots = FALSE)\n    m &lt;- match(c(\"formula\", \"data\", \"subset\", \"weights\", \"na.action\", \n        \"offset\"), names(mf), 0L)\n    mf &lt;- mf[c(1L, m)]\n    mf$drop.unused.levels &lt;- TRUE\n    mf[[1L]] &lt;- quote(stats::model.frame)\n    mf &lt;- eval(mf, parent.frame())\n    if (method == \"model.frame\") \n        return(mf)\n    else if (method != \"qr\") \n        warning(gettextf(\"method = '%s' is not supported. Using 'qr'\", \n            method), domain = NA)\n    mt &lt;- attr(mf, \"terms\")\n    y &lt;- model.response(mf, \"numeric\")\n    w &lt;- as.vector(model.weights(mf))\n    if (!is.null(w) && !is.numeric(w)) \n        stop(\"'weights' must be a numeric vector\")\n    offset &lt;- model.offset(mf)\n    mlm &lt;- is.matrix(y)\n    ny &lt;- if (mlm) \n        nrow(y)\n    else length(y)\n    if (!is.null(offset)) {\n        if (!mlm) \n            offset &lt;- as.vector(offset)\n        if (NROW(offset) != ny) \n            stop(gettextf(\"number of offsets is %d, should equal %d (number of observations)\", \n                NROW(offset), ny), domain = NA)\n    }\n    if (is.empty.model(mt)) {\n        x &lt;- NULL\n        z &lt;- list(coefficients = if (mlm) matrix(NA_real_, 0, \n            ncol(y)) else numeric(), residuals = y, fitted.values = 0 * \n            y, weights = w, rank = 0L, df.residual = if (!is.null(w)) sum(w != \n            0) else ny)\n        if (!is.null(offset)) {\n            z$fitted.values &lt;- offset\n            z$residuals &lt;- y - offset\n        }\n    }\n    else {\n        x &lt;- model.matrix(mt, mf, contrasts)\n        z &lt;- if (is.null(w)) \n            lm.fit(x, y, offset = offset, singular.ok = singular.ok, \n                ...)\n        else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, \n            ...)\n    }\n    class(z) &lt;- c(if (mlm) \"mlm\", \"lm\")\n    z$na.action &lt;- attr(mf, \"na.action\")\n    z$offset &lt;- offset\n    z$contrasts &lt;- attr(x, \"contrasts\")\n    z$xlevels &lt;- .getXlevels(mt, mf)\n    z$call &lt;- cl\n    z$terms &lt;- mt\n    if (model) \n        z$model &lt;- mf\n    if (ret.x) \n        z$x &lt;- x\n    if (ret.y) \n        z$y &lt;- y\n    if (!qr) \n        z$qr &lt;- NULL\n    z\n}\n&lt;bytecode: 0x5954c29dde58&gt;\n&lt;environment: namespace:stats&gt;\nWhat we see above is the underlying code for this particular function. We could copy and paste this into our own script and make any changes we deemed necessary, although tread carefully and test the changes you’ve made.\nDon’t worry overly if most of the code contained in functions doesn’t make sense immediately. This will be especially true if you are new to R, in which case it seems incredibly intimidating. Honestly, it can be intimidating even after years of R experience. To help with that, we’ll begin by making our own functions in R in the next section.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "05-programming.html#functions-in-r",
    "href": "05-programming.html#functions-in-r",
    "title": "5  Programming",
    "section": "\n5.2 Functions in R",
    "text": "5.2 Functions in R\nFunctions are the bread and butter of R, the essential sustaining elements allowing you to work with R. They’re made (most of the time) with the utmost care and attention but may end up being something of a Frankenstein’s monster - with weirdly attached limbs. But no matter how convoluted they may be they will always faithfully do the same thing.\nThis means that functions can also be very stupid.\nIf we asked you to go to the supermarket to get us some ingredients to make Balmoral chicken, even if you don’t know what the heck that is, you’d be able to guess and bring at least something back. Or you could decide to make something else. Or you could ask a chef for help. Or you could pull out your phone and search online for what Balmoral chicken is. The point is, even if we didn’t give you enough information to do the task, you’re intelligent enough to, at the very least, try to find a work around.\nIf instead, we asked a function to do the same, it would listen intently to our request, and then will simply return an error. It would then repeat this every single time we asked it to do the job when the task is not clear. The point here, is that code and functions cannot find workarounds to poorly provided information, which is great. It’s totally reliant on you, to tell it very explicitly what it needs to do step by step.\nRemember two things: the intelligence of code comes from the coder, not the computer and functions need exact instructions to work.\nTo prevent functions from being too stupid you must provide the information the function needs in order for it to work. As with the Balmoral chicken example, if we’d supplied a recipe list to the function, it would have managed just fine. We call this “fulfilling an argument”. The vast majority of functions require the user to fulfill at least one argument.\nThis can be illustrated in the pseudocode below. When we make a function we can:\n\nspecify what arguments the user must fulfill (e.g. arg1 and arg2)\nprovide default values to arguments (e.g. arg2 = TRUE)\ndefine what to do with the arguments (expression):\n\n\nmy_function &lt;- function(arg1, arg2, ...) {\n  expression\n}\n\nThe first thing to note is that we’ve used the function function() to create a new function called my_function. To walk through the above code; we’re creating a function called my_function. Within the round brackets we specify what information (i.e. arguments) the function requires to run (as many or as few as needed). These arguments are then passed to the expression part of the function. The expression can be any valid R command or set of R commands and is usually contained between a pair of braces { }. Once you run the above code, you can then use your new function by typing:\n\nmy_function(arg1, arg2)\n\nLet’s work through an example to help clear things up.\nFirst we are going to create a data frame called dishes, where columns lasagna, stovies, poutine, and tartiflette are filled with 10 random values drawn from a bag (using the rnorm() function to draw random values from a Normal distribution with mean 0 and standard deviation of 1). We also include a “problem”, for us to solve later, by including 3 NA values within the poutine column (using rep(NA, 3)).\n\ndishes &lt;- data.frame(\n  lasagna = rnorm(10),\n  stovies = rnorm(10),\n  poutine = c(rep(NA, 3), rnorm(7)),\n  tartiflette = rnorm(10)\n)\n\nLet’s say that you want to multiply the values in the variables stovies and lasagna and create a new object called stovies_lasagna. We can do this “by hand” using:\n\nstovies_lasagna &lt;- dishes$stovies * dishes$lasagna\n\nIf this was all we needed to do, we can stop here. R works with vectors, so doing these kinds of operations in R is actually much simpler than other programming languages, where this type of code might require loops (we say that R is a vectorised language). Something to keep in mind for later is that doing these kinds of operations with loops can be much slower compared to vectorisation.\nBut what if we want to repeat this multiplication many times? Let’s say we wanted to multiply columns lasagna and stovies, stovies and tartiflette, and poutine and tartiflette. In this case we could copy and paste the code, replacing the relevant information.\n\nlasagna_stovies &lt;- dishes$lasagna * dishes$stovies\nstovies_tartiflette &lt;- dishes$stovies * dishes$stovies\npoutine_tartiflette &lt;- dishes$poutine * dishes$tartiflette\n\nWhile this approach works, it’s easy to make mistakes. In fact, here we’ve “forgotten” to change stovies to tartiflette in the second line of code when copying and pasting. This is where writing a function comes in handy. If we were to write this as a function, there is only one source of potential error (within the function itself) instead of many copy-pasted lines of code (which we also cut down on by using a function).\n\n\n\n\n\n\nTip\n\n\n\nAs a rule of thumb if we have to do the same thing (by copy/paste & modify) 3 times or more, we just make a function for it.\n\n\nIn this case, we’re using some fairly trivial code where it’s maybe hard to make a genuine mistake. But what if we increased the complexity?\n\ndishes$lasagna * dishes$stovies / dishes$lasagna + (dishes$lasagna * 10^(dishes$stovies))\n-dishes$stovies - (dishes$lasagna * sqrt(dishes$stovies + 10))\n\nNow imagine having to copy and paste this three times, and in each case having to change the lasagna and stovies variables (especially if we had to do it more than three times).\nWhat we could do instead is generalize our code for x and y columns instead of naming specific dishes. If we did this, we could recycle the x * y code. Whenever we wanted to multiple columns together, we assign a dishes to either x or y. We’ll assign the multiplication to the objects lasagna_stovies and stovies_poutine so we can come back to them later.\n\n# Assign x and y values\nx &lt;- dishes$lasagna\ny &lt;- dishes$stovies\n\n# Use multiplication code\nlasagna_stovies &lt;- x * y\n\n# Assign new x and y values\nx &lt;- dishes$stovies\ny &lt;- dishes$poutine\n\n# Reuse multiplication code\nstovies_poutine &lt;- x * y\n\nThis is essentially what a function does. Let’s call our new function multiply_cols() and define it with two arguments, x and y. A function in R will simply return its last value. However, it is possible to force the function to return an earlier value if wanted/needed. Using the return() function is not strictly necessary in this example as R will automatically return the value of the last line of code in our function. We include it here to make this explicit.\n\nmultiply_cols &lt;- function(x, y) {\n  return(x * y)\n}\n\nNow that we’ve defined our function we can use it. Let’s use the function to multiple the columns lasagna and stovies and assign the result to a new object called lasagna_stovies_func\n\nlasagna_stovies_func &lt;- multiply_cols(x = dishes$lasagna, y = dishes$stovies)\nlasagna_stovies_func\n\n [1]  0.14956801  1.12215525  1.21258347 -1.30645139  0.12371740  1.42008301\n [7]  0.20130923 -2.78229845  0.26672596 -0.07330369\n\n\nIf we’re only interested in multiplying dishes$lasagna and dishes$stovies, it would be overkill to create a function to do something once. However, the benefit of creating a function is that we now have that function added to our environment which we can use as often as we like. We also have the code to create the function, meaning we can use it in completely new projects, reducing the amount of code that has to be written (and retested) from scratch each time.\nTo satisfy ourselves that the function has worked properly, we can compare the lasagna_stovies variable with our new variable lasagna_stovies_func using the identical() function. The identical() function tests whether two objects are exactly identical and returns either a TRUE or FALSE value. Use ?identical if you want to know more about this function.\n\nidentical(lasagna_stovies, lasagna_stovies_func)\n\n[1] TRUE\n\n\nAnd we confirm that the function has produced the same result as when we do the calculation manually. We recommend getting into a habit of checking that the function you’ve created works the way you think it has.\nNow let’s use our multiply_cols() function to multiply columns stovies and poutine. Notice now that argument x is given the value dishes$stoviesand y the value dishes$poutine.\n\nstovies_poutine_func &lt;- multiply_cols(x = dishes$stovies, y = dishes$poutine)\nstovies_poutine_func\n\n [1]          NA          NA          NA  0.78317895 -0.07867687  1.22337106\n [7]  0.35535748  1.93620123 -0.27024572  0.01389178\n\n\nSo far so good. All we’ve really done is wrapped the code x * y into a function, where we ask the user to specify what their x and y variables are.\nUsing the function is a bit long since we have to retype the name of the data frame for each variable. For a bit of fun we can modify the function so that, we can specify the data frame as an argument and the column names without quoting them (as in a tidyverse style).\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = prod(.)) %&gt;%\n    pull(xy)\n}\n\nFor this new version of the function, we added we added a data argument on line 1. On lines 3, we select the x and y variables provided as arguments. On line 4., we create the product of the 2 selected columns and on line 5. we extract the column we juste created. We also remove the return() function since it was not needed\nOur function is now compatible with the pipe (either native |&gt; or magrittr %&gt;%) function. However, since the function now uses the pipe from magrittr 📦 and dplyr 📦 functions, we need to load the tidyverse 📦 package for it to work.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)\nlasagna_stovies_func &lt;- dishes |&gt; multiply_cols(lasagna, stovies)\n\nNow let’s add a little bit more complexity. If you look at the output of poutine_tartiflette some of the calculations have produced NA values. This is because of those NA values we included in poutine when we created the dishes data frame. Despite these NA values, the function appeared to have worked but it gave us no indication that there might be a problem. In such cases we may prefer if it had warned us that something was wrong. How can we get the function to let us know when NA values are produced? Here’s one way.\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = {\n      .[1] * .[2]\n    }) %&gt;%\n    pull(xy)\n  if (any(is.na(temp_var))) {\n    warning(\"The function has produced NAs\")\n    return(temp_var)\n  } else {\n    return(temp_var)\n  }\n}\n\n\nstovies_poutine_func &lt;- multiply_cols(dishes, stovies, poutine)\n\nWarning in multiply_cols(dishes, stovies, poutine): The function has produced\nNAs\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)\n\nThe core of our function is still the same, but we’ve now got an extra six lines of code (lines 6-11). We’ve included some conditional statements, if (lines 6-8) and else (lines 9-11), to test whether any NAs have been produced and if they have we display a warning message to the user. The next section of this Chapter will explain how these work and how to use them.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "05-programming.html#conditional-statements",
    "href": "05-programming.html#conditional-statements",
    "title": "5  Programming",
    "section": "\n5.3 Conditional statements",
    "text": "5.3 Conditional statements\nx * y does not apply any logic. It merely takes the value of x and multiplies it by the value of y. Conditional statements are how you inject some logic into your code. The most commonly used conditional statement is if. Whenever you see an if statement, read it as ‘If X is TRUE, do a thing’. Including an else statement simply extends the logic to ‘If X is TRUE, do a thing, or else do something different’.\nBoth the if and else statements allow you to run sections of code, depending on a condition is either TRUE or FALSE. The pseudo-code below shows you the general form.\n  if (condition) {\n  Code executed when condition is TRUE\n  } else {\n  Code executed when condition is FALSE\n  }\nTo delve into this a bit more, we can use an old programmer joke to set up a problem.\n\nA programmer’s partner says: ‘Please go to the store and buy a carton of milk and if they have eggs, get six.’\nThe programmer returned with 6 cartons of milk.\nWhen the partner sees this, and exclaims ‘Why the heck did you buy six cartons of milk?’\nThe programmer replied ‘They had eggs’\n\nAt the risk of explaining a joke, the conditional statement here is whether or not the store had eggs. If coded as per the original request, the programmer should bring 6 cartons of milk if the store had eggs (condition = TRUE), or else bring 1 carton of milk if there weren’t any eggs (condition = FALSE). In R this is coded as:\n\neggs &lt;- TRUE # Whether there were eggs in the store\n\nif (eggs == TRUE) { # If there are eggs\n  n.milk &lt;- 6 # Get 6 cartons of milk\n} else { # If there are not eggs\n  n.milk &lt;- 1 # Get 1 carton of milk\n}\n\nWe can then check n.milk to see how many milk cartons they returned with.\n\nn.milk\n\n[1] 6\n\n\nAnd just like the joke, our R code has missed that the condition was to determine whether or not to buy eggs, not more milk (this is actually a loose example of the Winograd Scheme, designed to test the intelligence of artificial intelligence by whether it can reason what the intended referent of a sentence is).\nWe could code the exact same egg-milk joke conditional statement using an ifelse() function.\n\neggs &lt;- TRUE\nn.milk &lt;- ifelse(eggs == TRUE, yes = 6, no = 1)\n\nThis ifelse() function is doing exactly the same as the more fleshed out version from earlier, but is now condensed down into a single line of code. It has the added benefit of working on vectors as opposed to single values (more on this later when we introduce loops). The logic is read in the same way; “If there are eggs, assign a value of 6 to n.milk, if there isn’t any eggs, assign the value 1 to n.milk”.\nWe can check again to make sure the logic is still returning 6 cartons of milk:\n\nn.milk\n\n[1] 6\n\n\nCurrently we’d have to copy and paste code if we wanted to change if eggs were in the store or not. We learned above how to avoid lots of copy and pasting by creating a function. Just as with the simple x * y expression in our previous multiply_cols() function, the logical statements above are straightforward to code and well suited to be turned into a function. How about we do just that and wrap this logical statement up in a function?\n\nmilk &lt;- function(eggs) {\n  if (eggs == TRUE) {\n    6\n  } else {\n    1\n  }\n}\n\nWe’ve now created a function called milk() where the only argument is eggs. The user of the function specifies if eggs is either TRUE or FALSE, and the function will then use a conditional statement to determine how many cartons of milk are returned.\nLet’s quickly try:\n\nmilk(eggs = TRUE)\n\n[1] 6\n\n\nAnd the joke is maintained. Notice in this case we have actually specified that we are fulfilling the eggs argument (eggs = TRUE). In some functions, as with ours here, when a function only has a single argument we can be lazy and not name which argument we are fulfilling. In reality, it’s generally viewed as better practice to explicitly state which arguments you are fulfilling to avoid potential mistakes.\nOK, lets go back to the multiply_cols() function we created above and explain how we’ve used conditional statements to warn the user if NA values are produced when we multiple any two columns together.\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = {\n      .[1] * .[2]\n    }) %&gt;%\n    pull(xy)\n  if (any(is.na(temp_var))) {\n    warning(\"The function has produced NAs\")\n    return(temp_var)\n  } else {\n    return(temp_var)\n  }\n}\n\nIn this new version of the function we still use x * y as before but this time we’ve assigned the values from this calculation to a temporary vector called temp_var so we can use it in our conditional statements. Note, this temp_var variable is local to our function and will not exist outside of the function due something called R’s scoping rules. We then use an if statement to determine whether our temp_var variable contains any NA values. The way this works is that we first use the is.na() function to test whether each value in our temp_var variable is an NA. The is.na() function returns TRUE if the value is an NA and FALSE if the value isn’t an NA. We then nest the is.na(temp_var) function inside the function any() to test whether any of the values returned by is.na(temp_var) are TRUE. If at least one value is TRUE the any() function will return a TRUE. So, if there are any NA values in our temp_var variable the condition for the if() function will be TRUE whereas if there are no NA values present then the condition will be FALSE. If the condition is TRUE the warning() function generates a warning message for the user and then returns the temp_var variable. If the condition is FALSE the code below the else statement is executed which just returns the temp_var variable.\nSo if we run our modified multiple_columns() function on the columns dishes$stovies and dishes$poutine (which contains NAs) we will receive an warning message.\n\nstovies_poutine_func &lt;- multiply_cols(dishes, stovies, poutine)\n\nWarning in multiply_cols(dishes, stovies, poutine): The function has produced\nNAs\n\n\nWhereas if we multiple two columns that don’t contain NA values we don’t receive a warning message\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "05-programming.html#combining-logical-operators",
    "href": "05-programming.html#combining-logical-operators",
    "title": "5  Programming",
    "section": "\n5.4 Combining logical operators",
    "text": "5.4 Combining logical operators\nThe functions that we’ve created so far have been perfectly suited for what we need, though they have been fairly simplistic. Let’s try creating a function that has a little more complexity to it. We’ll make a function to determine if today is going to be a good day or not based on two criteria. The first criteria will depend on the day of the week (Friday or not) and the second will be whether or not your code is working (TRUE or FALSE). To accomplish this, we’ll be using if and else statements. The complexity will come from if statements immediately following the relevant else statement. We’ll use such conditional statements four times to achieve all combinations of it being a Friday or not, and if your code is working or not.\n\ngood.day &lt;- function(code.working, day) {\n  if (code.working == TRUE && day == \"Friday\") {\n    \"BEST.\nDAY.\nEVER.\nStop while you are ahead and go to the pub!\"\n  } else if (code.working == FALSE && day == \"Friday\") {\n    \"Oh well, but at least it's Friday! Pub time!\"\n  } else if (code.working == TRUE && day != \"Friday\") {\n    \"So close to a good day...\nshame it's not a Friday\"\n  } else if (code.working == FALSE && day != \"Friday\") {\n    \"Hello darkness.\"\n  }\n}\n\ngood.day(code.working = TRUE, day = \"Friday\")\n\n[1] \"BEST.\\nDAY.\\nEVER.\\nStop while you are ahead and go to the pub!\"\n\ngood.day(FALSE, \"Tuesday\")\n\n[1] \"Hello darkness.\"\n\n\nNotice that we never specified what to do if the day was not a Friday? That’s because, for this function, the only thing that matters is whether or not it’s Friday.\nWe’ve also been using logical operators whenever we’ve used if statements. Logical operators are the final piece of the logical conditions jigsaw. Below is a table which summarises operators. The first two are logical operators and the final six are relational operators. You can use any of these when you make your own functions (or loops).\n\n\n\n\n\n\n\n\nOperator\nTechnical Description\nWhat it means\nExample\n\n\n\n&&\nLogical AND\nBoth conditions must be met\nif(cond1 == test && cond2 == test)\n\n\n||\nLogical OR\nEither condition must be met\nif(cond1 == test || cond2 == test)\n\n\n&lt;\nLess than\nX is less than Y\nif(X &lt; Y)\n\n\n&gt;\nGreater than\nX is greater than Y\nif(X &gt; Y)\n\n\n&lt;=\nLess than or equal to\nX is less/equal to Y\nif(X &lt;= Y)\n\n\n&gt;=\nGreater than or equal to\nX is greater/equal to Y\nif(X &gt;= Y)\n\n\n==\nEqual to\nX is equal to Y\nif(X == Y)\n\n\n!=\nNot equal to\nX is not equal to Y\nif(X != Y)",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "05-programming.html#loops",
    "href": "05-programming.html#loops",
    "title": "5  Programming",
    "section": "\n5.5 Loops",
    "text": "5.5 Loops\nR is very good at performing repetitive tasks. If we want a set of operations to be repeated several times we use what’s known as a loop. When you create a loop, R will execute the instructions in the loop a specified number of times or until a specified condition is met. There are three main types of loop in R: the for loop, the while loop and the repeat loop.\nLoops are one of the staples of all programming languages, not just R, and can be a powerful tool (although in our opinion, used far too frequently when writing R code).\n\n5.5.1 For loop\nThe most commonly used loop structure when you want to repeat a task a defined number of times is the for loop. The most basic example of a for loop is:\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nBut what’s the code actually doing? This is a dynamic bit of code were an index i is iteratively replaced by each value in the vector 1:5. Let’s break it down. Because the first value in our sequence (1:5) is 1, the loop starts by replacing i with 1 and runs everything between the { }. Loops conventionally use i as the counter, short for iteration, but you are free to use whatever you like, even your pet’s name, it really does not matter (except when using nested loops, in which case the counters must be called different things, like SenorWhiskers and HerrFlufferkins).\nSo, if we were to do the first iteration of the loop manually\n\ni &lt;- 1\nprint(i)\n\n[1] 1\n\n\nOnce this first iteration is complete, the for loop loops back to the beginning and replaces i with the next value in our 1:5 sequence (2 in this case):\n\ni &lt;- 2\nprint(i)\n\n[1] 2\n\n\nThis process is then repeated until the loop reaches the final value in the sequence (5 in this example) after which point it stops.\nTo reinforce how for loops work and introduce you to a valuable feature of loops, we’ll alter our counter within the loop. This can be used, for example, if we’re using a loop to iterate through a vector but want to select the next row (or any other value). To show this we’ll simply add 1 to the value of our index every time we iterate our loop.\n\nfor (i in 1:5) {\n  print(i + 1)\n}\n\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n\n\nAs in the previous loop, the first value in our sequence is 1. The loop begins by replacing i with 1, but this time we’ve specified that a value of 1 must be added to i in the expression resulting in a value of 1 + 1.\n\ni &lt;- 1\ni + 1\n\n[1] 2\n\n\nAs before, once the iteration is complete, the loop moves onto the next value in the sequence and replaces i with the next value (2 in this case) so that i + 1 becomes 2 + 1.\n\ni &lt;- 2\ni + 1\n\n[1] 3\n\n\nAnd so on. We think you get the idea! In essence this is all a for loop is doing and nothing more.\nWhilst above we have been using simple addition in the body of the loop, you can also combine loops with functions.\nLet’s go back to our data frame dishes. Previously in the Chapter we created a function to multiply two columns and used it to create our lasagna_stovies, stovies_poutine, and poutine_tartiflette objects. We could have used a loop for this. Let’s remind ourselves what our data look like and the code for the multiple_columns() function.\n\ndishes &lt;- data.frame(\n  lasagna = rnorm(10),\n  stovies = rnorm(10),\n  poutine = c(rep(NA, 3), rnorm(7)),\n  tartiflette = rnorm(10)\n)\n\n\nmultiply_cols &lt;- function(data, x, y) {\n  temp_var &lt;- data %&gt;%\n    select({{ x }}, {{ y }}) %&gt;%\n    mutate(xy = {\n      .[1] * .[2]\n    }) %&gt;%\n    pull(xy)\n  if (any(is.na(temp_var))) {\n    warning(\"The function has produced NAs\")\n    return(temp_var)\n  } else {\n    return(temp_var)\n  }\n}\n\nTo use a list to iterate over these columns we need to first create an empty list (remember Section 3.2.3?) which we call temp (short for temporary) which will be used to store the output of the for loop.\n\ntemp &lt;- list()\nfor (i in 1:(ncol(dishes) - 1)) {\n  temp[[i]] &lt;- multiply_cols(dishes, x = colnames(dishes)[i], y = colnames(dishes)[i + 1])\n}\n\nWarning in multiply_cols(dishes, x = colnames(dishes)[i], y =\ncolnames(dishes)[i + : The function has produced NAs\n\nWarning in multiply_cols(dishes, x = colnames(dishes)[i], y =\ncolnames(dishes)[i + : The function has produced NAs\n\n\nWhen we specify our for loop notice how we subtracted 1 from ncol(dishes). The ncol() function returns the number of columns in our dishes data frame which is 4 and so our loop runs from i = 1 to i = 4 - 1 which is i = 3.\nSo in the first iteration of the loop i takes on the value 1. The multiply_cols() function multiplies the dishes[, 1] (lasagna) and dishes[, 1 + 1] (stovies) columns and stores it in the temp[[1]] which is the first element of the temp list.\nThe second iteration of the loop i takes on the value 2. The multiply_cols() function multiplies the dishes[, 2] (stovies) and dishes[, 2 + 1] (poutine) columns and stores it in the temp[[2]] which is the second element of the temp list.\nThe third and final iteration of the loop i takes on the value 3. The multiply_cols() function multiplies the dishes[, 3] (poutine) and dishes[, 3 + 1] (tartiflette) columns and stores it in the temp[[3]] which is the third element of the temp list.\nAgain, it’s a good idea to test that we are getting something sensible from our loop (remember, check, check and check again!). To do this we can use the identical() function to compare the variables we created by hand with each iteration of the loop manually.\n\nlasagna_stovies_func &lt;- multiply_cols(dishes, lasagna, stovies)\ni &lt;- 1\nidentical(\n  multiply_cols(dishes, colnames(dishes)[i], colnames(dishes)[i + 1]),\n  lasagna_stovies_func\n)\n\n[1] TRUE\n\nstovies_poutine_func &lt;- multiply_cols(dishes, stovies, poutine)\n\nWarning in multiply_cols(dishes, stovies, poutine): The function has produced\nNAs\n\ni &lt;- 2\nidentical(\n  multiply_cols(dishes, colnames(dishes)[i], colnames(dishes)[i + 1]),\n  stovies_poutine_func\n)\n\nWarning in multiply_cols(dishes, colnames(dishes)[i], colnames(dishes)[i + :\nThe function has produced NAs\n\n\n[1] TRUE\n\n\nIf you can follow the examples above, you’ll be in a good spot to begin writing some of your own for loops. That said there are other types of loops available to you.\n\n5.5.2 While loop\nAnother type of loop that you may use (albeit less frequently) is the while loop. The while loop is used when you want to keep looping until a specific logical condition is satisfied (contrast this with the for loop which will always iterate through an entire sequence).\nThe basic structure of the while loop is:\n\nwhile (logical_condition) {\n  expression\n}\n\nA simple example of a while loop is:\n\ni &lt;- 0\nwhile (i &lt;= 4) {\n  i &lt;- i + 1\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nHere the loop will only continue to pass values to the main body of the loop (the expression body) when i is less than or equal to 4 (specified using the &lt;= operator in this example). Once i is greater than 4 the loop will stop.\nThere is another, very rarely used type of loop; the repeat loop. The repeat loop has no conditional check so can keep iterating indefinitely (meaning a break, or “stop here”, has to be coded into it). It’s worthwhile being aware of it’s existence, but for now we don’t think you need to worry about it; the for and while loops will see you through the vast majority of your looping needs.\n\n5.5.3 When to use a loop?\nLoops are fairly commonly used, though sometimes a little overused in our opinion. Equivalent tasks can be performed with functions, which are often more efficient than loops. Though this raises the question when should you use a loop?\nIn general loops are implemented inefficiently in R and should be avoided when better alternatives exist, especially when you’re working with large datasets. However, loop are sometimes the only way to achieve the result we want.\nSome examples of when using loops can be appropriate:\n\nSome simulations (e.g. the Ricker model can, in part, be built using loops)\nRecursive relationships (a relationship which depends on the value of the previous relationship [“to understand recursion, you must understand recursion”])\nMore complex problems (e.g., how long since the last badger was seen at site \\(j\\), given a pine marten was seen at time \\(t\\), at the same location \\(j\\) as the badger, where the pine marten was detected in a specific 6 hour period, but exclude badgers seen 30 minutes before the pine marten arrival, repeated for all pine marten detections)\nWhile loops (keep jumping until you’ve reached the moon)\n\n5.5.4 If not loops, then what?\nIn short, use the apply family of functions; apply(), lapply(), tapply(), sapply(), vapply(), and mapply(). The apply functions can often do the tasks of most “home-brewed” loops, sometimes faster (though that won’t really be an issue for most people) but more importantly with a much lower risk of error. A strategy to have in the back of your mind which may be useful is; for every loop you make, try to remake it using an apply function (often lapply or sapply will work). If you can, use the apply version. There’s nothing worse than realizing there was a small, tiny, seemingly meaningless mistake in a loop which weeks, months or years down the line has propagated into a huge mess. We strongly recommend trying to use the apply functions whenever possible.\nlapply\nYour go to apply function will often be lapply() at least in the beginning. The way that lapply() works, and the reason it is often a good alternative to for loops, is that it will go through each element in a list and perform a task (i.e. run a function). It has the added benefit that it will output the results as a list - something you’d have to otherwise code yourself into a loop.\nAn lapply() has the following structure:\nlapply(X, FUN)\nHere X is the vector which we want to do something to. FUN stands for how much fun this is (just kidding!). It’s also short for “function”.\nLet’s start with a simple demonstration first. Let’s use the lapply() function create a sequence from 1 to 5 and add 1 to each observation (just like we did when we used a for loop):\n\nlapply(0:4, function(a) {\n  a + 1\n})\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n\nNotice that we need to specify our sequence as 0:4 to get the output 1 ,2 ,3 ,4 , 5 as we are adding 1 to each element of the sequence. See what happens if you use 1:5 instead.\nEquivalently, we could have defined the function first and then used the function in lapply()\n\nadd_fun &lt;- function(a) {\n  a + 1\n}\nlapply(0:4, add_fun)\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\n[[4]]\n[1] 4\n\n[[5]]\n[1] 5\n\n\nThe sapply() function does the same thing as lapply() but instead of storing the results as a list, it stores them as a vector.\n\nsapply(0:4, function(a) {\n  a + 1\n})\n\n[1] 1 2 3 4 5\n\n\nAs you can see, in both cases, we get exactly the same results as when we used the for loop.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Programming</span>"
    ]
  },
  {
    "objectID": "06-quarto.html",
    "href": "06-quarto.html",
    "title": "6  Reproducible reports with Quarto",
    "section": "",
    "text": "6.1 What is R markdown / Quarto?",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#what-is-r-markdown-quarto",
    "href": "06-quarto.html#what-is-r-markdown-quarto",
    "title": "6  Reproducible reports with Quarto",
    "section": "",
    "text": "6.1.1 R Markdown\nR markdown is a simple and easy to use plain text language used to combine your R code, results from your data analysis (including plots and tables) and written commentary into a single nicely formatted and reproducible document (like a report, publication, thesis chapter or a web page like this one).\nTechnically, R markdown is a combination of three languages, R, Markdown and YAML (yet another markup language). Both Markdown and YAML are a type of ‘markup’ language. A markup language simply provides a way of creating an easy to read plain text file which can incorporate formatted text, images, headers and links to other documents. If you’re interested you can find more information about markup languages here. Actually, you are exposed to a markup language on a daily basis, as most of the internet content you digest every day is underpinned by a markup language called HTML (Hypertext Markup Language). Anyway, the main point is that R markdown is very easy to learn (much, much easier than HTML) and when used with a good IDE (RStudio or VS Code) it’s ridiculously easy to integrate into your workflow to produce feature rich content (so why wouldn’t you?!).\n\n6.1.2 Quarto?\nQuarto is a multi-language, next generation version of R Markdown from Posit, with many new features and capabilities and is compatible not only with R but also with other language like Python and Julia. Like R Markdown, Quarto uses knitr 📦 package to execute R code, and is therefore able to render most existing .Rmd files without modification. However, it also comes with a plethora of new functionalities. More importantly, it makes it much easier to create different type of output since the coding is homogenize for specific format without having to rely on different r packages each with there own specificity (e.g bookdown, hugodown, blogdown, thesisdown, rticles, xaringan, …).\nIn the rest of this chapter, we will talk about Quarto but a lot can be done with R markdown. Quarto uses .qmd files while R markdown works with .Rmd but Quarto can render .Rmd files too.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#why-use-quarto",
    "href": "06-quarto.html#why-use-quarto",
    "title": "6  Reproducible reports with Quarto",
    "section": "\n6.2 Why use Quarto?",
    "text": "6.2 Why use Quarto?\nDuring the previous Chapters we talked a lot about conducting your research in a robust and reproducible manner to facilitate open science. In a nutshell, open science is about doing all we can to make our data, methods, results and inferences transparent and available to everyone. Some of the main tenets of open science are described here and include:\n\nTransparency in experimental methodology, observation, collection of data and analytical methods.\nPublic availability and re-usability of scientific data\nPublic accessibility and transparency of scientific communication\nUsing web-based tools to facilitate scientific collaboration\n\nBy now all of you will (hopefully) be using R to explore and analyse your interesting data. As such, you’re already well along the road to making your analysis more reproducible, transparent and shareable. However, perhaps your current workflow looks something like this:\n\n\n\n\n\n\n\nFigure 6.1: Non-reproducible workflow\n\n\n\n\nYour data is imported from your favourite spreadsheet software into R, you write your R code to explore and analyse your data, you save plots as external files, copy tables of analysis output and then manually combine all of this and your written prose into a single MS Word document (maybe for a paper or thesis chapter). Whilst there is nothing particularly wrong with this approach (and it’s certainly better than using point and click software to analyse your data) there are some limitations:\n\nIt’s not particularly reproducible. Because this workflow separates your R code from the final document there are multiple opportunities for undocumented decisions to be made (which plots did you use? what analysis did/didn’t you include? etc).\nIt’s inefficient. If you need to go back and change something (create a new plot or update your analysis etc) you will need to create or amend multiple documents increasing the risk of mistakes creeping into your workflow.\nIt’s difficult to maintain. If your analysis changes you again need to update multiple files and documents.\nIt can be difficult to decide what to share with others. Do you share all of your code (initial data exploration, model validation etc) or just the code specific to your final document? It’s quite a common (and bad!) practice for researchers to maintain two R scripts, one used for the actual analysis and one to share with the final paper or thesis chapter. This can be both time consuming and confusing and should be avoided.\n\nPerhaps a more efficient and robust workflow would look something like this:\n\n\n\n\n\n\n\nFigure 6.2: A-reproducible (and more fficient) workflow\n\n\n\n\nYour data is imported into R as before but this time all of the R code you used to analyse your data, produce your plots and your written text (Introduction, Materials and Methods, Discussion etc) is contained within a single Quarto document which is then used (along with your data) to automatically create your final document. This is exactly what Quarto allows you to do.\nSome of the advantages of using Quarto include:\n\nExplicitly links your data with your R code and output creating a fully reproducible workflow. ALL of the R code used to explore, summarise and analyse your data can be included in a single easy to read document. You can decide what to include in your final document (as you will learn below) but all of your R code can be included in the Quarto document.\nYou can create a wide variety of output formats (pdf, html web pages, MS Word and many others) from a single Quarto document which enhances both collaboration and communication.\nEnhances transparency of your research. Your data and Quarto file can be included with your publication or thesis chapter as supplementary material or hosted on a GitHub repository (see Chapter 7).\nIncreases the efficiency of your workflow. If you need to modify or extend your current analysis you just need to update your Quarto document and these changes will automatically be included in your final document.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#get-started-with-quarto",
    "href": "06-quarto.html#get-started-with-quarto",
    "title": "6  Reproducible reports with Quarto",
    "section": "\n6.3 Get started with Quarto",
    "text": "6.3 Get started with Quarto\nQuarto integrates really well with R Studio and VS Code, and provide both a source editor as well as a visual editor providing an experience close to your classic WYSIWYG (what you see is what you write) writing software (e.g. Microsoft Word or LibreOffice writer)\n\n6.3.1 Installation\nTo use Quarto you will first need to install the Quarto software and the quarto 📦 package (with its dependencies). You can find instructions on how to do this in Appendix B and on the Quarto website. If you would like to create pdf documents (or MS Word documents) from your Quarto file you will also need to install a version of \\(\\LaTeX\\) on your computer. If you’ve not installed \\(\\LaTeX\\) before, we recommend that you install TinyTeX. Again, instructions on how to do this can be found at Appendix B.\n\n6.3.2 Create a Quarto document, .qmd\n\nRight, time to create your first Quarto document. Within RStudio, click on the menu File -&gt; New File -&gt; Quarto.... In the pop up window, give the document a ‘Title’ and enter the ‘Author’ information (your name) and select HTML as the default output. We can change all of this later so don’t worry about it for the moment.\n\n\n\n\n\n\n\nFigure 6.3: Creating a Quarto document\n\n\n\n\nYou will notice that when your new Quarto document is created it includes some example Quarto code. Normally you would just highlight and delete everything in the document except the information at the top between the --- delimiters (this is called the YAML header which we will discuss in a bit) and then start writing your own code. However, just for now we will use this document to practice converting Quarto to both html and pdf formats and check everything is working.\n\n\n\n\n\n\n\nFigure 6.4: A new Quarto document\n\n\n\n\nOnce you’ve created your Quarto document it’s good practice to save this file somewhere convenient (Section 1.4 and Figure 1.11). You can do this by selecting File -&gt; Save from RStudio menu (or use the keyboard shortcut ctrl + s on Windows or cmd + s on a Mac) and enter an appropriate file name (maybe call it my_first_quarto). Notice the file extension of your new Quarto file is .qmd.\nNow, to convert your .qmd file to a HTML document click on the little black triangle next to the Knit icon at the top of the source window and select knit to HTML\n\n\n\n\n\n\n\nFigure 6.5: Knitting a Qmd file\n\n\n\n\nRStudio will now ‘knit’ (or render) your .qmd file into a HTML file. Notice that there is a new Quarto tab in your console window which provides you with information on the rendering process and will also display any errors if something goes wrong.\nIf everything went smoothly a new HTML file will have been created and saved in the same directory as your .qmd file (ours will be called my_first_quarto.html). To view this document simply double click on the file to open in a browser (like Chrome or Firefox) to display the rendered content. RStudio will also display a preview of the rendered file in a new window for you to check out (your window might look slightly different if you’re using a Windows computer).\n\n\n\n\n\n\n\nFigure 6.6: A my first rendered html\n\n\n\n\nGreat, you’ve just rendered your first Quarto document. If you want to knit your .qmd file to a pdf document then all you need to do is choose knit to PDF instead of knit to HTML when you click on the knit icon. This will create a file called my_first_quarto.pdf which you can double click to open. Give it a go!\nYou can also knit an .qmd file using the command line in the console rather than by clicking on the knit icon. To do this, just use the quarto_render() function from the quarto 📦 package as shown below. Again, you can change the output format using the output_format = argument as well as many other options.\nlibrary(quarto)\n\nquarto_render('my_first_quarto.qmd', output_format = 'html_document')\n\n# alternatively if you don't want to load the quarto package\n\nquarto::quarto_render('my_first_quarto.Rmd', output_format = 'html_document')",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#quarto-document-.qmd-anatomy",
    "href": "06-quarto.html#quarto-document-.qmd-anatomy",
    "title": "6  Reproducible reports with Quarto",
    "section": "\n6.4 Quarto document (.qmd) anatomy",
    "text": "6.4 Quarto document (.qmd) anatomy\nOK, now that you can render a Quarto file in RStudio into both HTML and pdf formats let’s take a closer look at the different components of a typical Quarto document. Normally each Quarto document is composed of 3 main components:\n\na YAML header\nformatted text\ncode chunks.\n\n\n\n\n\n\n\n\nFigure 6.7: Structure of a qmd file\n\n\n\n\n\n6.4.1 YAML header\nYAML stands for ‘YAML Ain’t Markup Language’ (it’s an ‘in’ joke!) and this optional component contains the metadata and options for the entire document such as the author name, date, output format, etc. The YAML header is surrounded before and after by a --- on its own line. In RStudio a minimal YAML header is automatically created for you when you create a new Quarto document as we did above (Section 6.3.2) but you can change this any time. A simple YAML header may look something like this:\n---\ntitle: My first Quarto document\nauthor: Jane Doe\ndate: March 01, 2020\nformat: html\n---\nIn the YAML header above the output format is set to HTML. If you would like to change the output to pdf format then you can change it from format: html to format: pdf (you can also set more than one output format if you like). You can also change the default font and font size for the whole document and even include fancy options such as a table of contents and inline references and a bibliography. If you want to explore the plethora of other options see here. Just a note of caution, many of the options you can specify in the YAML header will work with both HTML and pdf formatted documents, but not all. If you need multiple output formats for your Quarto document check whether your YAML options are compatible between these formats. Also, indentation in the YAML header has a meaning, so be careful when aligning text. For example, if you want to include a table of contents you would modify the output: field in the YAML header as follows\n---\ntitle: My first Quarto document\nauthor: Bob Hette\ndate: March 01, 2020\nformat:\n  html:\n    toc: true\n---\n\n6.4.2 Formatted text\nAs mentioned above, one of the great things about Quarto is that you don’t need to rely on your word processor to bring your R code, analysis and writing together. Quarto is able to render (almost) all of the text formatting that you are likely to need such as italics, bold, strike-through, super and subscript as well as bulleted and numbered lists, headers and footers, images, links to other documents or web pages and also equations. However, in contrast to your familiar What-You-See-Is-What-You-Get (WYSIWYG) word processing software you don’t see the final formatted text in your Quarto document (as you would in MS Word), rather you need to ‘markup’ the formatting in your text ready to be rendered in your output document. At first, this might seem like a right pain in the proverbial but it’s actually very easy to do and also has many advantages (do you find yourself spending more time on making your text look pretty in MS Word rather than writing good content?!).\nHere is an example of marking up text formatting in an Quarto document\n#### Tadpole sediment experiment\n\nThese data were obtained from a mesocosm experiment which aimed to examine the\neffect of bullfrog tadpoles (*Lithobates catesbeianus*) biomass on sediment\nnutrient (NH~4~, NO~3~ and PO~3~) release.\nAt the start of the experiment 15 replicate mesocosms were filled with\n20 cm^2^ of **homogenised** marine sediment and assigned to one of five \ntadpole biomass treatments.\nwhich would look like this in the final rendered document (can you spot the markups?)\n\nTadpole sediment experiment\n\n\nThese data were obtained from a mesocosm experiment which aimed to examine the effect of bullfrog tadpoles (Lithobates catesbeianus) biomass on sediment nutrient (NH4, NO3 and PO3) release. At the start of the experiment 15 replicate mesocosms were filled with 20 cm2 of homogenised marine sediment and assigned to one of five tadpole biomass treatments.\n\nEmphasis\nSome of the most common markdown syntax for providing emphasis and formatting text is given below.\n\n\nGoal\nQuarto\noutput\n\n\n\nbold text\n**mytext**\nmytext\n\n\nitalic text\n*mytext*\nmytext\n\n\nstrikethrough\n~~mytext~~\nmytext\n\n\nsuperscript\nmytext^2^\nmytext2\n\n\n\nsubscript\nmytext~2~\nmytext2\n\n\n\n\nInterestingly there is no underline in R markdown syntax by default, for more or less esoteric reasons (e.g. an underline is considered a stylistic element (there may well be other reasons)). Quarto fixed that problem, you can simply do [text to underline]{.underline} to underline your text.\nWhite space and line breaks\nOne of the things that can be confusing for new users of markdown is the use of spaces and carriage returns (the enter key on your keyboard). In markdown, multiple spaces within the text are generally ignored as are carriage returns. For example this markdown text\nThese      data were      obtained from a\nmesocosm experiment which    aimed to examine the\neffect\nof          bullfrog tadpoles (*Lithobates catesbeianus*) biomass.\nwill be rendered as\n\nThese data were obtained from a mesocosm experiment which aimed to examine the effect of bullfrog tadpoles (Lithobates catesbeianus) biomass.\n\nThis is generally a good thing (no more random multiple spaces in your text). If you want your text to start on a new line then you can simply add two blank spaces at the end of the preceding line\n\nThese data were obtained from a\nmesocosm experiment which aimed to examine the\neffect bullfrog tadpoles (Lithobates catesbeianus) biomass.\n\nIf you really want multiple spaces within your text then you can use the Non breaking space tag &nbsp;\nThese &nbsp; &nbsp; &nbsp; data were &nbsp; &nbsp; &nbsp; &nbsp; obtained from a  \nmesocosm experiment which &nbsp; &nbsp; aimed to examine the    \neffect &nbsp; &nbsp; &nbsp; &nbsp; bullfrog tadpoles (*Lithobates catesbeianus*) biomass.\n\nThese       data were         obtained from a\nmesocosm experiment which     aimed to examine the\neffect         bullfrog tadpoles (Lithobates catesbeianus) biomass.\n\nHeadings\nYou can add headings and subheadings to your Quarto document by using the # symbol at the beginning of the line. You can decrease the size of the headings by simply adding more # symbols. For example\n# Header 1\n## Header 2\n### Header 3\n#### Header 4\n##### Header 5\n###### Header 6\nresults in headings in decreasing size order\n\nHeader 1\nHeader 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6\n\nComments\nAs you can see above the meaning of the # symbol is different when formatting text in an Quarto document compared to a standard R script (which is used to included a comment - remember?!). You can, however, use a # symbol to comment code inside a code chunk (Section 6.4.3) as usual (more about this in a bit). If you want to include a comment in your Quarto document outside a code chunk which won’t be included in the final rendered document then enclose your comment between &lt;!-- and --&gt;.\n&lt;!--\nthis is an example of how to format a comment using Quarto.\n--&gt;\nLists\nIf you want to create a bullet point list of text you can format an unordered list with sub items. Notice that the sub-items need to be indented.\n- item 1\n- item 2\n   + sub-item 2\n   + sub-item 3\n- item 3\n- item 4\n\n\nitem 1\nitem 2\n\nsub-item 2\nsub-item 3\n\n\nitem 3\nitem 4\n\n\nIf you need an ordered list\n1. item 1\n1. item 2\n    + sub-item 2\n    + sub-item 3\n1. item 3\n1. item 4\n\n\nitem 1\nitem 2\n\nsub-item 2\nsub-item 3\n\n\nitem 3\nitem 4\n\n\nLinks\nIn addition to images you can also include links to webpages or other links in your document. Use the following syntax to create a clickable link to an existing webpage. The link text goes between the square brackets and the URL for the webpage between the round brackets immediately after.\nYou can include a text for your clickable [link](https://www.worldwildlife.org)\nwhich gives you:\n\nYou can include a text for your clickable link\n\n\n6.4.3 Code chunks\nNow to the heart of the matter. To include R code into your Quarto document you simply place your code into a ‘code chunk’. All code chunks start and end with three backticks ```. Note, these are also known as ‘grave accents’ or ‘back quotes’ and are not the same as an apostrophe! On most keyboards you can find the backtick on the same key as tilde (~).\n```{r}\nAny valid R code goes here\n```\nYou can insert a code chunk by either typing the chunk delimiters ```{r} and ``` manually or use your IDE option (RStudio toolbar (the Insert button) or by clicking on the menu Code -&gt; Insert Chunk. In VS Code you can use code snippets) Perhaps an even better way is to get familiar with the keyboard shortcuts for you IDE or code snippets.\nThere are a many things you can do with code chunks: you can produce text output from your analysis, create tables and figures and insert images amongst other things. Within the code chunk you can place rules and arguments between the curly brackets {} that give you control over how your code is interpreted and output is rendered. These are known as chunk options. The only mandatory chunk option is the first argument which specifies which language you’re using (r in our case but other languages are supported). Note, chunk options can be written in two ways:\n\neither all of your chunk options must be written between the curly brackets on one line with no line breaks\nor they can be written using a YAML notation within the code chunk using #| notation at the beginning of the line.\n\nYou can also specify an optional code chunk name (or label) which can be useful when trying to debug problems and when performing advanced document rendering. In the following block we name the code chunk summary-stats, create a dataframe (dataf) with two variables x and y and then use the summary() function to display some summary statistics . When we run the code chunk both the R code and the resulting output are displayed in the final document.\n```{r, summary-stats, echo = TRUE}\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nsummary(dataf)\n```\n```{r}\n#| label: summary-stats\n#| echo: true\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nsummary(dataf)\n```\nBoth will output\n\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nsummary(dataf)\n\n       x               y        \n Min.   : 1.00   Min.   : 1.00  \n 1st Qu.: 3.25   1st Qu.: 3.25  \n Median : 5.50   Median : 5.50  \n Mean   : 5.50   Mean   : 5.50  \n 3rd Qu.: 7.75   3rd Qu.: 7.75  \n Max.   :10.00   Max.   :10.00  \n\n\nWhen using chunk names make sure that you don’t have duplicate chunk names in your Quarto document and avoid spaces and full stops as this will cause problems when you come to knit your document (We use a - to separate words in our chunk names).\nIf we wanted to only display the output of our R code (just the summary statistics for example) and not the code itself in our final document we can use the chunk option echo=FALSE\n```{r}\n#| label: summary-stats2\n#| echo: false\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n```\n\n\n       x               y        \n Min.   : 1.00   Min.   : 1.00  \n 1st Qu.: 3.25   1st Qu.: 3.25  \n Median : 5.50   Median : 5.50  \n Mean   : 5.50   Mean   : 5.50  \n 3rd Qu.: 7.75   3rd Qu.: 7.75  \n Max.   :10.00   Max.   :10.00  \n\n\nTo display the R code but not the output use the results='hide' chunk option.\n```{r}\n#| label: summary-stats\n#| results: 'hide'\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n```\n\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n\nSometimes you may want to execute a code chunk without showing any output at all. You can suppress the entire output using the chunk option include=FALSE.\n```{r, summary-stats, include=FALSE}\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nsummary(dataf)\n```\nThere are a large number of chunk options documented here with a more condensed version here. Perhaps the most commonly used are summarised below with the default values shown.\n\n\n\n\n\n\n\nChunk option\ndefault value\nFunction\n\n\n\necho\necho: true\nIf false, will not display the code in the final document\n\n\nresults\nresults: 'markup'\nIf ‘hide’, will not display the code’s results in the final document.\n\n\n\nIf ‘hold’, will delay displaying all output pieces until the end of the chunk. If ‘asis’, will pass through results without reformatting them. | | include | include: true | If false, will run the chunk but not include the chunk in the final document. | | eval | eval: true | If false, will not run the code in the code chunk. | | message | message: true | If false, will not display any messages generated by the code. | | warning | warning: true | If false, will not display any warning messages generated by the code. |\n\n6.4.4 Inline R code\nUp till now we’ve been writing and executing our R code in code chunks. Another great reason to use Quarto is that we can also include our R code directly within our text. This is known as ‘inline code’. To include your code in your Quarto text you simply write `r write your code here`. This can come in really useful when you want to include summary statistics within your text. For example, we could describe the iris dataset as follows:\nMorphological characteristics (variable names: \n`r names(iris)[1:4]`) were measured from \n`r nrow(iris)` *Iris sp.* plants from \n`r length(levels(iris$Species))` different species.\nThe mean Sepal length was\n`r round(mean(iris$Sepal.Length), digits = 2)` mm.\n  \nwhich will be rendered as\n\nMorphological characteristics (variable names: Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) were measured from 150 iris plants from 3 different species. The mean Sepal length was 5.84 mm.\n\nThe great thing about including inline R code in your text is that these values will automatically be updated if your data changes.\n\n6.4.5 Images and photos\nA useful feature is the ability to embed images and links to web pages (or other documents) into your Quarto document. You can include images into your Quarto document in a number of different ways. Perhaps the simplest method is to use the Quarto markdown format:\n![Image caption](path/to/you/image){options}\nHere is an example with an image taking 75% of the width and centered.\n![Waiting for the eclipse](images/markdown/eclipse_ready.jpg){fig-align=\"center\" width=\"75%\"}\nresulting in:\n\n\n\n\n\nFigure 6.8: Waiting for the eclipse\n\n\nAn alternative way of including images in your document is to use the include_graphics() function from the knitr package. The following code will produce similar output.\n```{r}\n#| label: fig-knitr\n#| fig-align: center\n#| out-width: 75%\n#| fig-cap: Waiting for the eclipse\nknitr::include_graphics(\"images/markdown/eclipse_ready.jpg\")\n```\nThe code above will only work if the image file (eclipse_ready.jpg) is in the right place relative to where you saved your .qmd file. In the example the image file is in a sub directory (folder) called images/markdown in the directory where we saved our my_first_quarto.qmd file. You can embed images saved in many different file types but perhaps the most common are .jpg and .png.\n\n6.4.6 Figures\nBy default, figures produced by R code will be placed immediately after the code chunk they were generated from. For example:\n\n```{r}\n#| label: fig-simple-plot\n#| fig-cap: A simple plot\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\n\n\n\n\n\nFigure 6.9: A simple plot\n\n\n\n\nThe fig-cap: chunk option allow to provide a figure caption recognized by Quarto and using in figure numbering and cross referencing (Section 6.4.8).\nIf you want to change the plot dimensions in the final document you can use the fig-width: and fig-height: chunk options (in inches!). You can also change the alignment of the figure using the fig-align: chunk option.\n\n```{r}\n#| label: fig-simple-plot2\n#| fig-cap: A shrinked figure\n#| fig-width: 4\n#| fig-height: 3\n#| fig-align: center\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\n\n\n\n\n\nFigure 6.10: A shrinked figure\n\n\n\n\nYou can add a figure caption using the fig-cap: option.\n\n```{r}\n#| label: fig-simple-plot-cap\n#| class-source: fold-show\n#| fig-cap: A simple plot\n#| fig-align: center\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\n\n\n\n\n\nFigure 6.11: A simple plot\n\n\n\n\nIf you want to suppress the figure in the final document use the fig-show: 'hide' option.\n\n```{r}\n#| label: fig-simple-plot5\n#| fig-show: hide\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\nplot(dataf$x, dataf$y, xlab = \"x axis\", ylab = \"y axis\")\n```\n\nIf you’re using a package like ggplot2 📦 to create your plots then don’t forget you will need to make the package available with the library() function in the code chunk (or in a preceding code chunk).\n\n```{r}\n#| label: fig-simple-ggplot\n#| fig-cap: A simple ggplot\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nlibrary(ggplot2)\nggplot(dataf, aes(x = x, y = y)) +\n  geom_point()\n```\n\n\n\n\n\n\nFigure 6.12: A simple ggplot\n\n\n\n\nAgain, there are a large number of chunk options specific to producing plots and figures. See here for more details.\n\n6.4.7 Tables\nIn Quarto, you can create tables using native markdown syntax (this doesn’t need to be in a code chunk).\n|  x  |  y  |\n|:---:|:---:|\n|  1  |  5  | \n|  2  |  4  |\n|  3  |  3  |\n|  4  |  2  |\n|  5  |  1  |\n\n: Caption for a simple markdown table\n\n\nTable 6.1: Caption for a simple markdown table\n\n\n\nx\ny\n\n\n\n1\n5\n\n\n2\n4\n\n\n3\n3\n\n\n4\n2\n\n\n5\n1\n\n\n\n\n\n\nThe :-------: lets markdown know that the line above should be treated as a header and the lines below as the body of the table. Alignment within the table is set by the position of the :. To center align use :------:, to left align :------ and right align ------:. Whilst it can be fun(!) to create tables with raw markup it’s only practical for very small and simple tables.\nThe easiest way we know to include tables in an Quarto document is by using the kable() function from the knitr 📦 package. The kable() function can create tables for HTML, PDF and Word outputs.\nTo create a table of the first 2 rows per species of the iris data frame using the kable() function simply write\nlibrary(knitr)\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\nkable()\nor without loading knitr 📦 but indicating where to find the kable() function.\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\n  knitr::kable()\n\n\n\nTable 6.2: A simple kable table\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n7.0\n3.2\n4.7\n1.4\nversicolor\n\n\n6.4\n3.2\n4.5\n1.5\nversicolor\n\n\n6.3\n3.3\n6.0\n2.5\nvirginica\n\n\n5.8\n2.7\n5.1\n1.9\nvirginica\n\n\n\n\n\n\n\n\nThe kable() function offers plenty of options to change the formatting of the table. For example, if we want to round numeric values to one decimal place use the digits = argument. To center justify the table contents use align = 'c' and to provide custom column headings use the col.names = argument. See ?knitr::kable for more information.\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\n  knitr::kable(\n    digits=0,\n    align = 'c',\n    col.names = c(\n      'Sepal length', 'Sepal width',\n      'Petal length', 'Petal width', 'Species'\n    )\n)\n\n\n\nTable 6.3: A nicer kable table\n\n\n\n\nSepal length\nSepal width\nPetal length\nPetal width\nSpecies\n\n\n\n5\n4\n1\n0\nsetosa\n\n\n5\n3\n1\n0\nsetosa\n\n\n7\n3\n5\n1\nversicolor\n\n\n6\n3\n4\n2\nversicolor\n\n\n6\n3\n6\n2\nvirginica\n\n\n6\n3\n5\n2\nvirginica\n\n\n\n\n\n\n\n\nYou can further enhance the look of your kable tables using the kableExtra 📦 package (don’t forget to install the package first!). See here for more details and a helpful tutorial.\nIf you want even more control and customisation options for your tables take a look at the [gt][gt] package. gt is an acronym for grammar of tables and is based on similar principle for tables that are used for plots in ggplot.\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n = 2) %&gt;%\n  rename_with(~ gsub(\"([._])\", \" \", .x)) %&gt;%\n  gt()\n\n\nTable 6.4: A nice gt table\n\n\n\n\n\n\n\nSepal Length\nSepal Width\nPetal Length\nPetal Width\n\n\n\nsetosa\n\n\n5.1\n3.5\n1.4\n0.2\n\n\n4.9\n3.0\n1.4\n0.2\n\n\nversicolor\n\n\n7.0\n3.2\n4.7\n1.4\n\n\n6.4\n3.2\n4.5\n1.5\n\n\nvirginica\n\n\n6.3\n3.3\n6.0\n2.5\n\n\n5.8\n2.7\n5.1\n1.9\n\n\n\n\n\n\n\n\n\n\nWithin most R packages developped to produce tables, there are options to include table captions. However, if you want to add a table caption we recommend to do using the code chunk option in Quarto tbl-cap: since it will allow for cross-referencing (Section 6.4.8) and better integration in the document.\n```{r}\n#| label: tbl-gt-table\n#| tbl-cap: A nice gt table\n#| echo: true\niris %&gt;%\n  group_by(Species) %&gt;%\n  slice_head(n=2) %&gt;%\n  rename_with(~gsub(\"([._])\", \" \", .x)) %&gt;%\n  gt()\n```\n\n6.4.8 Cross-referencing\nCross-references make it easier for readers to navigate your document by providing numbered references and hyperlinks to various entities like figures and tables. Once set up, tables and figures numbering happens automatically, so you don’t need to re-number all the figures when you add or delete one.\nEvery cross-referenceable entity requires a label (a unique identifier) prefixed with a cross-reference type e.g. #fig-element\nFor more details see the cross-referencing section on Quarto website.\n\n6.4.8.1 Document sections\nYou can make cross-references to other sections of the document. To do so you need to:\n\nset up a identifier for the section you want to link to. The identifier should:\n\nstart with #sec-\n\nbe in lower case (Figure 6.3)\ndoe not have any space, using - instead\n\n\nuse the @ symbol and the identifier to refer to the section\n\n## Cross-referencing sections {#sec-cross-ref-sections}\n\n[...]\n\nAs seen before(@sec-cross-ref-sections)\n\n6.4.8.2 Images, figures and tables\nFor tables, images and figures, in addition to the identifier the element also needs a caption for cross-referencing to work.\nThe prefix for tables is #tbl- and #fig- for images and figures.\nHere is an example for an image included with markdown:\n![Rocking the eclipse](images/markdown/eclipse_ready.jpg){#fig-cute-dog}\n\nSee @fig-cute-dog for an illustration.\n\n\n\n\n\nFigure 6.13: Rocking the eclipse\n\n\nSee Figure 6.13 for an illustration.\nFor figures and tables produced with R code chunks, simply provide the identifier in the label chunk option and the caption also as a chunk option.\nHere is the code for a figure and a table.\n\n```{r}\n#| label: fig-cr-plot\n#| fig-cap: A nice figure\nx &lt;- 1:10    # create an x variable\ny &lt;- 10:1    # create a y variable\ndataf &lt;- data.frame(x = x, y = y)\n\nlibrary(ggplot2)\nggplot(dataf, aes(x = x, y = y)) +\n  geom_point()\n```\n\n\n\n\n\n\nFigure 6.14: A nice figure\n\n\n\n\n\n```{r}\n#| label: tbl-cr-table\n#| tbl-cap: A nice table\n#| warning: false\nlibrary(knitr)\nkable(iris[1:5,], digits=0, align = 'c', col.names = c('sepal length', 'sepal width', 'petal length', 'petal width', 'species'))\n```\n\n\nTable 6.5: A nice table\n\n\n\n\nsepal length\nsepal width\npetal length\npetal width\nspecies\n\n\n\n5\n4\n1\n0\nsetosa\n\n\n5\n3\n1\n0\nsetosa\n\n\n5\n3\n1\n0\nsetosa\n\n\n5\n3\n2\n0\nsetosa\n\n\n5\n4\n1\n0\nsetosa\n\n\n\n\n\n\n\n\nUsing cross-references, we can write:\nAs seen on @fig-cr-plot and @tbl-cr-table …\nTo get:\nAs seen on Figure 6.14 and Table 6.5 …\n\n6.4.9 Citations and bibliography\nTo generate citations and a bibliography, Quarto requires:\n\na properly formatted .qmd document\na bibliographic source file including all the information for the citations. It works with awide variatey of format but we suggest using BibTEX format.\n(optional) a CSL file which specifies the formatting to use when generating the citations and bibliography.\n\nThe bibliographic source and the (optional) csl file are specified in the yaml header as :\n---\ntitle: \"My Document\"\nbibliography: references.bib\ncsl: ecology.csl\n---\n\n6.4.9.1 Citations\nQuarto uses the standard Pandoc markdown representation for citations (e.g. [@citation]) — citations go inside square brackets and are separated by semicolons. Each citation must have a key, composed of ‘@’ + the citation identifier from the database, and may optionally have a prefix, a locator, and a suffix. The citation key must begin with a letter, digit, or , and may contain alphanumerics, , and internal punctuation characters (:.#$%&-+?&lt;&gt;~/).\n\n\n\n\n\n\nMarkdown Format\nOutput (default)\n\n\n\nUnicorns are the best [see @martin1219, pp. 33-35; also @martin2200, chap. 1]\nUnicorns are the best (see Martin 1219 pp. 33–35, also Martin 2200 chap. 1)\n\n\n\nUnicorns are the best [@martin2200; @martin1219]\nUnicorns are the best (Martin 1219, 2200)\n\n\n\nMartin says unicorns are the best [-@martin2200]\nMartin says unicorns are the best (2200)\n\n\n\n\n@martin1219 says unicorns are the best.\n\nMartin (1219) says unicorns are the best.\n\n\n\n@martin1219 [p. 33] says unicorns are the best.\n\nMartin (1219 p. 33) says unicorns are the best.\n\n\n\n6.4.9.2 Create the bibliography\nBy default, the list of works cited will automatically be generated and placed at the end of document if the style calls for it. It will be placed in a div with the id refs if one exists like\n### Bibliography\n\n::: {#refs}\n:::\nFor more details see the Citation page on Quarto website.\n\n6.4.9.3 Integration with Zotero\nQuarto integrates really well with Zotero if you are using the visual editor in either RStudio or VS Code.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#sec-tips_tricks",
    "href": "06-quarto.html#sec-tips_tricks",
    "title": "6  Reproducible reports with Quarto",
    "section": "\n6.5 Some tips and tricks",
    "text": "6.5 Some tips and tricks\nProblem :\nWhen rendering my Quarto document to pdf my code runs off the edge of the page.\nSolution:\nAdd a global_options argument at the start of your .qmd file in a code chunk:\n```{r}\n#| label: global_options\n#| include: false \nknitr::opts_chunk$set(message=FALSE, tidy.opts=list(width.cutoff=60), tidy=TRUE) \n```\nThis code chunk won’t be displayed in the final document due to the include: false argument and you should place the code chunk immediately after the YAML header to affect everything below that.\ntidy.opts = list(width.cutoff = 60), tidy=TRUE defines the margin cutoff point and wraps text to the next line. Play around with this value to get it right (60-80 should be OK for most documents).\nWith quarto you can also put the global knitr options in a knitrblock in the YAML header (see Quarto website for details).\n---\ntitle: \"My Document\"\nformat: html\nknitr:\n  opts_chunk: \n    message: false\n    tidy.opts: !expr 'list(width.cutoff=60)'\n    tidy: true \n---\nProblem:\nWhen I load a package in my Quarto document my rendered output contains all of the startup messages and/or warnings.\nSolution:\nYou can load all of your packages at the start of your Quarto document in a code chunk along with setting your global options.\n```{r}\n#| label: global_options\n#| include: false\nknitr::opts_chunk$set(\n  message = FALSE,\n  warning=FALSE,\n  tidy.opts=list(width.cutoff=60)\n) \nsuppressPackageStartupMessages(library(ggplot2))\n```\nThe message = FALSE and warning = FALSE arguments suppress messages and warnings. The suppressPackageStartupMessages(library(ggplot2)) will load the ggplot2 📦 package but suppress startup messages.\nProblem:\nWhen rendering my Quarto document to pdf my tables and/or figures are split over two pages.\nSolution:\nAdd a page break using the \\(\\LaTeX\\) \\pagebreak notation before your offending table or figure\nProblem:\nThe code in my rendered document looks ugly!\nSolution:\nAdd the argument tidy: true to your global arguments. Sometimes, however, this can cause problems especially with correct code indentation. The best solution is to write code that looks nice (insert space and use multiple lines)",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "06-quarto.html#further-information",
    "href": "06-quarto.html#further-information",
    "title": "6  Reproducible reports with Quarto",
    "section": "\n6.6 Further Information",
    "text": "6.6 Further Information\nAlthough we’ve covered more than enough to get you quite far using Quarto, as with most things R related, we’ve really only had time to scratch the surface. Happily, there’s a wealth of information available to you should you need to expand your knowledge and experience. A good place to start is the excellent quarto website here.\nAnother useful and concise Quarto reference guide can be found here\nA quick and easy R Markdown cheatsheet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdler, D., S. T. Kelly, T. Elliott, and J. Adamson. 2022. vioplot: Violin plot.\n\n\nAllaire, J., Y. Xie, C. Dervieux, J. McPherson, J. Luraschi, K. Ushey,\nA. Atkins, H. Wickham, J. Cheng, W. Chang, and R. Iannone. 2024. rmarkdown: Dynamic documents for r.\n\n\nCarr, D., ported by Nicholas Lewin-Koh, M. Maechler, and contains copies\nof lattice functions written by Deepayan Sarkar. 2023. hexbin: Hexagonal binning routines.\n\n\nDouglas, A. 2023. An introduction to\nr.\n\n\nFrancisco Rodriguez-Sanchez, and Connor P. Jackson. 2023. grateful: Facilitate citation of r packages.\n\n\nHorikoshi, M., and Y. Tang. 2018. ggfortify: Data visualization tools for\nstatistical analysis results.\n\n\nKassambara, A. 2023. ggpubr: “ggplot2” based publication ready plots.\n\n\nKoenker, R. 2023. quantreg: Quantile regression.\n\n\nMartin, J. 1219. Another lasagna recipe from medieval times. Journal of\nLasagna 4:1686.\n\n\nMartin, J. 2200. A silly example. Chapman; Hall/CRC, Boca Raton,\nFlorida.\n\n\nPedersen, T. L. 2024. patchwork: The composer of plots.\n\n\nPrunello, M., and G. Mari. 2021. ggcleveland: Implementation of plots from\ncleveland’s visualizing data book.\n\n\nR Core Team. 2024. R:\nA language and environment for statistical computing. R Foundation\nfor Statistical Computing, Vienna, Austria.\n\n\nSchloerke, B., D. Cook, J. Larmarange, F. Briatte, M. Marbach, E. Thoen,\nA. Elberg, and J. Crowley. 2024. GGally:\nExtension to “ggplot2”.\n\n\nTang, Y., M. Horikoshi, and W. Li. 2016. ggfortify: Unified interface to visualize\nstatistical result of popular r packages. The R Journal 8:474–485.\n\n\nWickham, H. 2007. Reshaping\ndata with the reshape package. Journal\nof Statistical Software 21:1–20.\n\n\nWickham, H., M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François,\nG. Grolemund, A. Hayes, L. Henry, J. Hester, M. Kuhn, T. L. Pedersen, E.\nMiller, S. M. Bache, K. Müller, J. Ooms, D. Robinson, D. P. Seidel, V.\nSpinu, K. Takahashi, D. Vaughan, C. Wilke, K. Woo, and H. Yutani. 2019.\nWelcome to the tidyverse. Journal of Open Source Software\n4:1686.\n\n\nWilkinson, L. 2005. The Grammar of Graphics.\nSpringer Science & Business Media.\n\n\nXie, Y. 2014. knitr: A comprehensive tool\nfor reproducible research in R. in V. Stodden, F.\nLeisch, and R. D. Peng, editors. Implementing reproducible computational\nresearch. Chapman; Hall/CRC.\n\n\nXie, Y. 2015. Dynamic documents with\nR and knitr. 2nd edition. Chapman; Hall/CRC, Boca\nRaton, Florida.\n\n\nXie, Y. 2023. knitr: A general-purpose package for dynamic\nreport generation in r.\n\n\nXie, Y., J. J. Allaire, and G. Grolemund. 2018. R markdown: The definitive\nguide. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y., C. Dervieux, and E. Riederer. 2020. R markdown\ncookbook. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nZhu, H. 2024. kableExtra: Construct complex table with\n“kable” and pipe syntax.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Reproducible reports with Quarto</span>"
    ]
  },
  {
    "objectID": "07-github.html",
    "href": "07-github.html",
    "title": "7  Version control with Git and GitHub",
    "section": "",
    "text": "7.1 What is version control?\nA Version Control System (VCS) keeps a record of all the changes you make to your files that make up a particular project and allows you to revert to previous versions of files if you need to. To put it another way, if you muck things up or accidentally lose important files you can easily roll back to a previous stage in your project to sort things out. Version control was originally designed for collaborative software development, but it’s equally useful for scientific research and collaborations (although admittedly a lot of the terms, jargon and functionality are focused on the software development side). There are many different version control systems currently available, but we’ we’ll focus on using Git, because it’s free and open source and it integrates nicely with RStudio. This means that its can easily become part of your usual workflow with minimal additional overhead.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#why-use-version-control",
    "href": "07-github.html#why-use-version-control",
    "title": "7  Version control with Git and GitHub",
    "section": "\n7.2 Why use version control?",
    "text": "7.2 Why use version control?\nSo why should you worry about version control? Well, first of all it helps avoid this (familiar?) situation when you’re working on a project\n\n\n\n\nYou need version control\n\n\n\nusually arising from this (familiar?) scenario\n\n\n\n\n\n\n\n\nVersion control automatically takes care of keeping a record of all the versions of a particular file and allows you to revert back to previous versions if you need to. Version control also helps you (especially the future you) keep track of all your files in a single place and it helps others (especially collaborators) review, contribute to and reuse your work through the GitHub website. Lastly, your files are always available from anywhere and on any computer, all you need is an internet connection.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#what-is-git-and-github",
    "href": "07-github.html#what-is-git-and-github",
    "title": "7  Version control with Git and GitHub",
    "section": "\n7.3 What is Git and GitHub?",
    "text": "7.3 What is Git and GitHub?\nGit is a version control system originally developed by Linus Torvalds that lets you track changes to a set of files. These files can be any type of file including the menagerie of files that typically make up a data orientated project (.pdf, .Rmd, .docx, .txt, .jpg etc) although plain text files work the best. All the files that make up a project is called a repository (or just repo).\nGitHub is a web-based hosting service for Git repositories which allows you to create a remote copy of your local version-controlled project. This can be used as a backup or archive of your project or make it accessible to you and to your colleagues so you can work collaboratively.\nAt the start of a project we typically (but not always) create a remote repository on GitHub, then clone (think of this as copying) this repository to our local computer (the one in front of you). This cloning is usually a one time event and you shouldn’t need to clone this repository again unless you really muck things up. Once you have cloned your repository you can then work locally on your project as usual, creating and saving files for your data analysis (scripts, R markdown documents, figures etc). Along the way you can take snapshots (called commits) of these files after you’ve made important changes. We can then push these changes to the remote GitHub repository to make a backup or make available to our collaborators. If other people are working on the same project (repository), or maybe you’re working on a different computer, you can pull any changes back to your local repository so everything is synchronised.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-setup_git",
    "href": "07-github.html#sec-setup_git",
    "title": "7  Version control with Git and GitHub",
    "section": "\n7.4 Getting started",
    "text": "7.4 Getting started\nThis Chapter assumes that you have already installed the latest versions of R and an IDE (RStudio or VSCode). If you haven’t done this yet you can find instructions in Section 1.1.1.\n\n7.4.1 Install Git\nTo get started, you first need to install Git. If you’re lucky you may already have Git installed (especially if you have a Mac or Linux computer). You can check if you already have Git installed by clicking on the Terminal tab in the Console window in RStudio and typing git --version (the space after the git command is important). If you see something that looks like git version 2.25.0 (the version number may be different on your computer) then you already have Git installed (happy days). If you get an error (something like git: command not found) this means you don’t have Git installed (yet!).\n\n\n\n\n\n\n\n\nYou can also do this check outside RStudio by opening up a separate Terminal if you want. On Windows go to the ‘Start menu’ and in the search bar (or run box) type cmd and press enter. On a Mac go to ‘Applications’ in Finder, click on the ‘Utilities’ folder and then on the ‘Terminal’ program. On a Linux machine simply open the Terminal (Ctrl+Alt+T often does it).\nTo install Git on a Windows computer we recommend you download and install Git for Windows (also known as ‘Git Bash’). You can find the download file and installation instructions here.\nFor those of you using a Mac computer we recommend you download Git from here and install in the usual way (double click on the installer package once downloaded). If you’ve previously installed Xcode on your Mac and want to use a more up to date version of Git then you will need to follow a few more steps documented here. If you’ve never heard of Xcode then don’t worry about it!\nFor those of you lucky enough to be working on a Linux machine you can simply use your OS package manager to install Git from the official repository. For Ubuntu Linux (or variants of) open your Terminal and type\nsudo apt update\nsudo apt install git\nYou will need administrative privileges to do this. For other versions of Linux see here for further installation instructions.\nWhatever version of Git you’re installing, once the installation has finished verify that the installation process has been successful by running the command git --version in the Terminal tab in RStudio (as described above). On some installations of Git (yes we’re looking at you MS Windows) this may still produce an error as you will also need to setup RStudio so it can find the Git executable (described in Section 7.4.3).\n\n7.4.2 Configure Git\nAfter installing Git, you need to configure it so you can use it. Click on the Terminal tab in the Console window again and type the following:\ngit config --global user.email 'you@youremail.com'\n\ngit config --global user.name 'Your Name'\nsubstituting 'Your Name' for your actual name and 'you@youremail.com' with your email address. We recommend you use your University email address (if you have one) as you will also use this address when you register for your GitHub account (coming up in a bit).\nIf this was successful, you should see no error messages from these commands. To verify that you have successfully configured Git type the following into the Terminal\ngit config --global --list\nYou should see both your user.name and user.email configured.\n\n7.4.3 Configure RStudio\nAs you can see above, Git can be used from the command line, but it also integrates well with RStudio, providing a friendly graphical user interface. If you want to use RStudio’s Git integration (we recommend you do - at least at the start), you need to check that the path to the Git executable is specified correctly. In RStudio, go to the menu Tools -&gt; Global Options -&gt; Git/SVN and make sure that ‘Enable version control interface for RStudio projects’ is ticked and that the ‘Git executable:’ path is correct for your installation. If it’s not correct hit the Browse... button and navigate to where you installed git and click on the executable file. You will need to restart RStudio after doing this.\n\n\n\n\n\n\n\n\n\n7.4.4 Configure VSCode\n\nto develop\n\n\n7.4.5 Register a GitHub account\nIf all you want to do is to keep track of files and file versions on your local computer then Git is sufficient. If however, you would like to make an off-site copy of your project or make it available to your collaborators then you will need a web-based hosting service for your Git repositories. This is where GitHub comes into play (there are also other services like GitLab, Bitbucket and Savannah). You can sign up for a free account on GitHub here. You will need to specify a username, an email address and a strong password. We suggest that you use your University email address (if you have one) as this will also allow you to apply for a free educator or researcher account later on which gives you some useful benefits (don’t worry about this now though). When it comes to choosing a username we suggest you give this some thought. Choose a short(ish) rather than a long username, use all lowercase and hyphenate if you want to include multiple words, find a way of incorporating your actual name and lastly, choose a username that you will feel comfortable revealing to your future employer!\nNext click on the ‘Select a plan’ (you may have to solve a simple puzzle first to verify you’re human) and choose the ‘Free Plan’ option. Github will send an email to the email address you supplied for you to verify.\nOnce you’ve completed all those steps you should have both Git and GitHub setup up ready for you to use (Finally!).",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#setting-up-a-project",
    "href": "07-github.html#setting-up-a-project",
    "title": "7  Version control with Git and GitHub",
    "section": "\n7.5 Setting up a project",
    "text": "7.5 Setting up a project\n\n7.5.1 in RStudio\nNow that you’re all set up, let’s create your first version controlled RStudio project. There are a couple of different approaches you can use to do this. You can either setup a remote GitHub repository first then connect an RStudio project to this repository (we’ll call this Option 1). Another option is to setup a local repository first and then link a remote GitHub repository to this repository (Option 2). You can also connect an existing project to a GitHub repository but we won’t cover this here. We suggest that if you’re completely new to Git and GitHub then use Option 1 as this approach sets up your local Git repository nicely and you can push and pull immediately. Option 2 requires a little more work and therefore there are more opportunities to go wrong. We will cover both of these options below.\n\n7.5.2 Option 1 - GitHub first\nTo use the GitHub first approach you will first need to create a repository (repo) on GitHub. Go to your GitHub page and sign in if necessary. Click on the ‘Repositories’ tab at the top and then on the green ‘New’ button on the right\n\n\n\n\n\n\n\n\nGive your new repo a name (let’s call it first_repo for this Chapter), select ‘Public’, tick on the ‘Initialize this repository with a README’ (this is important) and then click on ‘Create repository’ (ignore the other options for now).\n\n\n\n\n\n\n\n\nYour new GitHub repository will now be created. Notice the README has been rendered in GitHub and is in markdown (.md) format (see Chapter 6 on R markdown if this doesn’t mean anything to you). Next click on the green ‘Clone or Download’ button and copy the https//... URL that pops up for later (either highlight it all and copy or click on the copy to clipboard icon to the right).\n\n\n\n\n\n\n\n\nOk, we now switch our attention to RStudio. In RStudio click on the File -&gt; New Project menu. In the pop up window select Version Control.\n\n\n\n\n\n\n\n\nNow paste the the URL you previously copied from GitHub into the Repository URL: box. This should then automatically fill out the Project Directory Name: section with the correct repository name (it’s important that the name of this directory has the same name as the repository you created in GitHub). You can then select where you want to create this directory by clicking on the Browse button opposite the Create project as a subdirectory of: option. Navigate to where you want the directory created and click OK. We also tick the Open in new session option.\n\n\n\n\n\n\n\n\nRStudio will now create a new directory with the same name as your repository on your local computer and will then clone your remote repository to this directory. The directory will contain three new files; first_repo.Rproj (or whatever you called your repository), README.md and .gitignore. You can check this out in the Files tab usually in the bottom right pane in RStudio. You will also have a Git tab in the top right pane with two files listed (we will come to this later on in the Chapter). That’s it for Option 1, you now have a remote GitHub repository set up and this is linked to your local repository managed by RStudio. Any changes you make to files in this directory will be version controlled by Git.\n\n\n\n\n\n\n\n\n\n7.5.3 Option 2 - RStudio first\nAn alternative approach is to create a local RStudio project first and then link to a remote Github repository. As we mentioned before, this option is more involved than Option 1 so feel free to skip this now and come back later to it if you’re interested. This option is also useful if you just want to create a local RStudio project linked to a local Git repository (i.e. no GitHub involved). If you want to do this then just follow the instructions below omitting the GitHub bit.\nIn RStudio click on the File -&gt; New Project menu and select the New Directory option.\n\n\n\n\n\n\n\n\nIn the pop up window select the New Project option\n\n\n\n\n\n\n\n\nIn the New Project window specify a Directory name (choose second_repo for this Chapter) and select where you would like to create this directory on you computer (click the Browse button). Make sure the Create a git repository option is ticked\n\n\n\n\n\n\n\n\nThis will create a version controlled directory called second_repo on your computer that contains two files, second_repo.Rproj and .gitignore (there might also be a .Rhistory file but ignore this). You can check this by looking in the Files tab in RStudio (usually in the bottom right pane).\n\n\n\n\n\n\n\n\nOK, before we go on to create a repository on GitHub we need to do one more thing - we need to place our second_repo.Rproj and .gitignorefiles under version control. Unfortunately we haven’t covered this in detail yet so just follow the next few instructions (blindly!) and we’ll revisit them in Section 7.6 of this Chapter.\nTo get our two files under version control click on the ‘Git’ tab which is usually in the top tight pane in RStudio\n\n\n\n\n\n\n\n\nYou can see that both files are listed. Next, tick the boxes under the ‘Staged’ column for both files and then click on the ‘Commit’ button.\n\n\n\n\n\n\n\n\nThis will take you to the ‘Review Changes’ window. Type in the commit message ‘First commit’ in the ‘Commit message’ window and click on the ‘Commit’ button. A new window will appear with some messages which you can ignore for now. Click ‘Close’ to close this window and also close the ‘Review Changes’ window. The two files should now have disappeared from the Git pane in RStudio indicating a successful commit.\n\n\n\n\n\n\n\n\nOK, that’s those two files now under version control. Now we need to create a new repository on GitHub. In your browser go to your GitHub page and sign in if necessary. Click on the ‘Repositories’ tab and then click on the green ‘New’ button on the right. Give your new repo the name second_repo (the same as your version controlled directory name) and select ‘Public’. This time do not tick the ‘Initialize this repository with a README’ (this is important) and then click on ‘Create repository’.\n\n\n\n\n\n\n\n\nThis will take you to a Quick setup page which provides you with some code for various situations. The code we are interested in is the code under ...or push an existing repository from the command line heading.\n\n\n\n\n\n\n\n\nHighlight and copy the first line of code (note: yours will be slightly different as it will include your GitHub username not mine)\ngit remote add origin https://github.com/alexd106/second_repo.git\nSwitch to RStudio, click on the ‘Terminal’ tab and paste the command into the Terminal. Now go back to GitHub and copy the second line of code\ngit push -u origin master\nand paste this into the Terminal in RStudio. You should see something like this\n\n\n\n\n\n\n\n\nIf you take a look at your repo back on GitHub (click on the /second_repo link at the top) you will see the second_repo.Rproj and .gitignore files have now been pushed to GitHub from your local repository.\n\n\n\n\n\n\n\n\nThe last thing we need to do is create and add a README file to your repository. A README file describes your project and is written using the same Markdown language you learned in Chapter 6. A good README file makes it easy for others (or the future you!) to use your code and reproduce your project. You can create a README file in RStudio or in GitHub. Let’s use the second option.\nIn your repository on GitHub click on the green Add a README button.\n\n\n\n\n\n\n\n\nNow write a short description of your project in the &lt;&gt; Edit new file section and then click on the green Commit new file button.\n\n\n\n\n\n\n\n\nYou should now see the README.md file listed in your repository. It won’t actually exist on your computer yet as you will need to pull these changes back to your local repository, but more about that in the next section.\nWhether you followed Option 1 or Option 2 (or both) you have now successfully setup a version controlled RStudio project (and associated directory) and linked this to a GitHub repository. Git will now monitor this directory for any changes you make to files and also if you add or delete files. If the steps above seem like a bit of an ordeal, just remember, you only need to do this once for each project and it gets much easier over time.\n\n7.5.4 in VSCode\n\nto develop",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-use_git",
    "href": "07-github.html#sec-use_git",
    "title": "7  Version control with Git and GitHub",
    "section": "\n7.6 Using Git with RStudio",
    "text": "7.6 Using Git with RStudio\nNow that we have our project and repositories (both local and remote) set up, it’s finally time to learn how to use Git in your IDE!\nTypically, when using Git your workflow will go something like this:\n\nYou create/delete and edit files in your project directory on your computer as usual (saving these changes as you go)\nOnce you’ve reached a natural ‘break point’ in your progress (i.e. you’d be sad if you lost this progress) you stage these files\nYou then commit the changes you made to these staged files (along with a useful commit message) which creates a permanent snapshot of these changes\nYou keep on with this cycle until you get to a point when you would like to push these changes to GitHub\nIf you’re working with other people on the same project you may also need to pull their changes to your local computer\n\n\n\n\n\n\n\n\n\nOK, let’s go through an example to help clarify this workflow.\nOpen up the first_repo.Rproj you created previously during Option 1. Either use the File -&gt; Open Project menu or click on the top right project icon and select the appropriate project.\n\n\n\n\n\n\n\n\nCreate an R markdown document inside this project by clicking on the File -&gt; New File -&gt; R markdown menu (remember Chapter 6?).\nOnce created, we can delete all the example R markdown code (except the YAML header) as usual and write some interesting R markdown text and include a plot. We’ll use the inbuilt cars dataset to do this. Save this file (cmd + s for Mac or ctrl + s in Windows). Your R markdown document should look something like the following (it doesn’t matter if it’s not exactly the same).\n\n\n\n\n\n\n\n\nTake a look at the ‘Git’ tab which should list your new R markdown document (first_doc.Rmd in this example) along with first_repo.Rproj, and .gitignore (you created these files previously when following Option 1).\n\n\n\n\n\n\n\n\nFollowing our workflow, we now need to stage these files. To do this tick the boxes under the ‘Staged’ column for all files. Notice that there is a status icon next to the box which gives you an indication of how the files were changed. In our case all of the files are to be added (capital A) as we have just created them.\n\n\n\n\n\n\n\n\nAfter you have staged the files the next step is to commit the files. This is done by clicking on the ‘Commit’ button.\n\n\n\n\n\n\n\n\nAfter clicking on the ‘Commit’ button you will be taken to the ‘Review Changes’ window. You should see the three files you staged from the previous step in the left pane. If you click on the file name first_doc.Rmd you will see the changes you have made to this file highlighted in the bottom pane. Any content that you have added is highlighted in green and deleted content is highlighted in red. As you have only just created this file, all the content is highlighted in green. To commit these files (take a snapshot) first enter a mandatory commit message in the ‘Commit message’ box. This message should be relatively short and informative (to you and your collaborators) and indicate why you made the changes, not what you changed. This makes sense as Git keeps track of what has changed and so it is best not to use commit messages for this purpose. It’s traditional to enter the message ‘First commit’ (or ‘Initial commit’) when you commit files for the first time. Now click on the ‘Commit’ button to commit these changes.\n\n\n\n\n\n\n\n\nA summary of the commit you just performed will be shown. Now click on the ‘Close’ button to return to the ‘Review Changes’ window. Note that the staged files have now been removed.\n\n\n\n\n\n\n\n\nNow that you have committed your changes the next step is to push these changes to GitHub. Before you push your changes it’s good practice to first pull any changes from GitHub. This is especially important if both you and your collaborators are working on the same files as it keeps you local copy up to date and avoids any potential conflicts. In this case your repository will already be up to date but it’s a good habit to get into. To do this, click on the ‘Pull’ button on the top right of the ‘Review Changes’ window. Once you have pulled any changes click on the green ‘Push’ button to push your changes. You will see a summary of the push you just performed. Hit the ‘Close’ button and then close the ‘Review Changes’ window.\n\n\n\n\n\n\n\n\nTo confirm the changes you made to the project have been pushed to GitHub, open your GitHub page, click on the Repositories link and then click on the first_repo repository. You should see four files listed including the first_doc.Rmd you just pushed. Along side the file name you will see your last commit message (‘First commit’ in this case) and when you made the last commit.\n\n\n\n\n\n\n\n\nTo see the contents of the file click on the first_doc.Rmd file name.\n\n\n\n\n\n\n\n\n\n7.6.1 Tracking changes\nAfter following the steps outlined above, you will have successfully modified an RStudio project by creating a new R markdown document, staged and then committed these changes and finally pushed the changes to your GitHub repository. Now let’s make some further changes to your R markdown file and follow the workflow once again but this time we ’ll take a look at how to identify changes made to files, examine the commit history and how to restore to a previous version of the document.\nIn RStudio open up the first_repo.Rproj file you created previously (if not already open) then open the first_doc.Rmd file (click on the file name in the Files tab in RStudio).\nLet’s make some changes to this document. Delete the line beginning with ‘My first version controlled …’ and replace it with something more informative (see figure below). We will also change the plotted symbols to red and give the plot axes labels. Lastly, let’s add a summary table of the dataframe using the kable() and summary() functions (you may need to install the knitr package if you haven’t done so previously to use the kable() function) and finally render this document to pdf by changing the YAML option to output: pdf_document.\n\n\n\n\n\n\n\n\nNow save these changes and then click the knit button to render to pdf. A new pdf file named first_doc.pdf will be created which you can view by clicking on the file name in the Files tab in RStudio.\nNotice that these two files have been added to the Git tab in RStudio. The status icons indicate that the first_doc.Rmd file has been modified (capital M) and the first_doc.pdf file is currently untracked (question mark).\n\n\n\n\n\n\n\n\nTo stage these files tick the ‘Staged’ box for each file and click on the ‘Commit’ button to take you to the ‘Review Changes’ window\n\n\n\n\n\n\n\n\nBefore you commit your changes notice the status of first_doc.pdf has changed from untracked to added (A). You can view the changes you have made to the first_doc.Rmd by clicking on the file name in the top left pane which will provide you with a useful summary of the changes in the bottom pane (technically called diffs). Lines that have been deleted are highlighted in red and lines that have been added are highlighted in green (note that from Git’s point of view, a modification to a line is actually two operations: the removal of the original line followed by the creation of a new line). Once you’re happy, commit these changes by writing a suitable commit message and click on the ‘Commit’ button.\n\n\n\n\n\n\n\n\nTo push the changes to GitHub, click on the ‘Pull’ button first (remember this is good practice even though you are only collaborating with yourself at the moment) and then click on the ‘Push’ button. Go to your online GitHub repository and you will see your new commits, including the first_doc.pdf file you created when you rendered your R markdown document.\n\n\n\n\n\n\n\n\nTo view the changes in first_doc.Rmd click on the file name for this file.\n\n\n\n\n\n\n\n\n\n7.6.2 Commit history\nOne of the great things about Git and GitHub is that you can view the history of all the commits you have made along with the associated commit messages. You can do this locally using RStudio (or the Git command line) or if you have pushed your commits to GitHub you can check them out on the GitHub website.\nTo view your commit history in RStudio click on the ‘History’ button (the one that looks like a clock) in the Git pane to bring up the history view in the ‘Review Changes’ window. You can also click on the ‘Commit’ or ‘Diff’ buttons which takes you to the same window (you just need to additionally click on the ‘History’ button in the ‘Review Changes’ window).\n\n\n\n\n\n\n\n\nThe history window is split into two parts. The top pane lists every commit you have made in this repository (with associated commit messages) starting with the most recent one at the top and oldest at the bottom. You can click on each of these commits and the bottom pane shows you the changes you have made along with a summary of the Date the commit was made, Author of the commit and the commit message (Subject). There is also a unique identifier for the commit (SHA - Secure Hash Algorithm) and a Parent SHA which identifies the previous commit. These SHA identifiers are really important as you can use them to view and revert to previous versions of files (details below Section 7.6.3). You can also view the contents of each file by clicking on the ‘View file @ SHA key` link (in our case ’View file @ 2b4693d1’).\n\n\n\n\n\n\n\n\nYou can also view your commit history on GitHub website but this will be limited to only those commits you have already pushed to GitHub. To view the commit history navigate to the repository and click on the ‘commits’ link (in our case the link will be labelled ‘3 commits’ as we have made 3 commits).\n\n\n\n\n\n\n\n\nYou will see a list of all the commits you have made, along with commit messages, date of commit and the SHA identifier (these are the same SHA identifiers you saw in the RStudio history). You can even browse the repository at a particular point in time by clicking on the &lt;&gt; link. To view the changes in files associated with the commit simply click on the relevant commit link in the list.\n\n\n\n\n\n\n\n\nWhich will display changes using the usual format of green for additions and red for deletions.\n\n\n\n\n\n\n\n\n\n7.6.3 Reverting changes\nOne the great things about using Git is that you are able to revert to previous versions of files if you’ve made a mistake, broke something or just prefer and earlier approach. How you do this will depend on whether the changes you want to discard have been staged, committed or pushed to GitHub. We’ll go through some common scenarios below mostly using RStudio but occasionally we will need to resort to using the Terminal (still in RStudio though).\nChanges saved but not staged, committed or pushed\nIf you have saved changes to your file(s) but not staged, committed or pushed these files to GitHub you can right click on the offending file in the Git pane and select ‘Revert …’. This will roll back all of the changes you have made to the same state as your last commit. Just be aware that you cannot undo this operation so use with caution.\n\n\n\n\n\n\n\n\nYou can also undo changes to just part of a file by opening up the ‘Diff’ window (click on the ‘Diff’ button in the Git pane). Select the line you wish to discard by double clicking on the line and then click on the ‘Discard line’ button. In a similar fashion you can discard chunks of code by clicking on the ‘Discard chunk’ button.\n\n\n\n\n\n\n\n\nStaged but not committed and not pushed\nIf you have staged your files, but not committed them then simply unstage them by clicking on the ‘Staged’ check box in the Git pane (or in the ‘Review Changes’ window) to remove the tick. You can then revert all or parts of the file as described in the section above.\nStaged and committed but not pushed\nIf you have made a mistake or have forgotten to include a file in your last commit which you have not yet pushed to GitHub, you can just fix your mistake, save your changes, and then amend your previous commit. You can do this by staging your file and then tick the ‘Amend previous commit` box in the ’Review Changes’ window before committing.\n\n\n\n\n\n\n\n\nIf we check out our commit history you can see that our latest commit contains both changes to the file rather than having two separate commits. We use the amend commit approach alot but it’s important to understand that you should not do this if you have already pushed your last commit to GitHub as you are effectively rewriting history and all sorts bad things may happen!\n\n\n\n\n\n\n\n\nIf you spot a mistake that has happened multiple commits back or you just want to revert to a previous version of a document you have a number of options.\nOption 1 - (probably the easiest but very unGit - but like, whatever!) is to look in your commit history in RStudio, find the commit that you would like to go back to and click on the ‘View file @’ button to show the file contents.\n\n\n\n\n\n\n\n\nYou can then copy the contents of the file to the clipboard and paste it into your current file to replace your duff code or text. Alternatively, you can click on the ‘Save As’ button and save the file with a different file name. Once you have saved your new file you can delete your current unwanted file and then carry on working on your new file. Don’t forget to stage and commit this new file.\n\n\n\n\n\n\n\n\nOption 2 - (Git like) Go to your Git history, find the commit you would like to roll back to and write down (or copy) its SHA identifier.\n\n\n\n\n\n\n\n\nNow go to the Terminal in RStudio and type git checkout &lt;SHA&gt; &lt;filename&gt;. In our case the SHA key is 2b4693d1 and the filename is first_doc.Rmd so our command would look like this:\ngit checkout 2b4693d1 first_doc.Rmd\nThe command above will copy the selected file version from the past and place it into the present. RStudio may ask you whether you want to reload the file as it now changed - select yes. You will also need to stage and commit the file as usual.\nIf you want to revert all your files to the same state as a previous commit rather than just one file you can use (the single ‘dot’ . is important otherwise your HEAD will detach!):\ngit rm -r .\ngit checkout 2b4693d1 .\n\nNote that this will delete all files that you have created since you made this commit so be careful!\nStaged, committed and pushed\nIf you have already pushed your commits to GitHub you can use the git checkout strategy described above and then commit and push to update GitHub (although this is not really considered ‘best’ practice). Another approach would be to use git revert (Note: as far as we can tell git revert is not the same as the ‘Revert’ option in RStudio). The revert command in Git essentially creates a new commit based on a previous commit and therefore preserves all of your commit history. To rollback to a previous state (commit) you first need to identify the SHA for the commit you wish to go back to (as we did above) and then use the revert command in the Terminal. Let’s say we want to revert back to our ‘First commit’ which has a SHA identifier d27e79f1.\n\n\n\n\n\n\n\n\nWe can use the revert command as shown below in the Terminal. The --no-commit option is used to prevent us from having to deal with each intermediate commit.\ngit revert --no-commit d27e79f1..HEAD\nYour first_doc.Rmd file will now revert back to the same state as it was when you did your ‘First commit’. Notice also that the first_doc.pdf file has been deleted as this wasn’t present when we made our first commit. You can now stage and commit these files with a new commit message and finally push them to GitHub. Notice that if we look at our commit history all of the commits we have made are still present.\n\n\n\n\n\n\n\n\nand our repo on GitHub also reflects these changes",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-use_git_vsc",
    "href": "07-github.html#sec-use_git_vsc",
    "title": "7  Version control with Git and GitHub",
    "section": "\n7.7 Using Git with VSCode",
    "text": "7.7 Using Git with VSCode\nNow that we have our project and repositories (both local and remote) set up, it’s finally time to learn how to use Git in VSCode!\nTypically, when using Git your workflow will go something like this:\n\nYou create/delete and edit files in your project directory on your computer as usual (saving these changes as you go)\nOnce you’ve reached a natural ‘break point’ in your progress (i.e. you’d be sad if you lost this progress) you stage these files\nYou then commit the changes you made to these staged files (along with a useful commit message) which creates a permanent snapshot of these changes\nYou keep on with this cycle until you get to a point when you would like to push these changes to GitHub\nIf you’re working with other people on the same project you may also need to pull their changes to your local computer\n\n\n\n\n\n\n\n\n\nOK, let’s go through an example to help clarify this workflow.\nTracking changes\nCommit History\nReverting changes",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-collab",
    "href": "07-github.html#sec-collab",
    "title": "7  Version control with Git and GitHub",
    "section": "\n7.8 Collaborate with Git",
    "text": "7.8 Collaborate with Git\nGitHub is a great tool for collaboration, it can seem scary and complicated at first, but it is worth investing some time to learn how it works. What makes GitHub so good for collaboration is that it is a distributed system, which means that every collaborator works on their own copy of the project and changes are then merged together in the remote repository. There are two main ways you can set up a collaborative project on GitHub. One is the workflow we went through above, where everybody connects their local repository to the same remote one; this system works well with small projects where different people mainly work on different aspects of the project but can quickly become unwieldy if many people are collaborating and are working on the same files (merge misery!). The second approach consists of every collaborator creating a copy (or fork) of the main repository, which becomes their remote repository. Every collaborator then needs to send a request (a pull request) to the owner of the main repository to incorporate any changes into the main repository and this includes a review process before the changes are integrated. More detail of these topics can be found in Section 7.10.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#git-tips",
    "href": "07-github.html#git-tips",
    "title": "7  Version control with Git and GitHub",
    "section": "\n7.9 Git tips",
    "text": "7.9 Git tips\nGenerally speaking you should commit often (including amended commits) but push much less often. This makes collaboration easier and also makes the process of reverting to previous versions of documents much more straight forward. We generally only push changes to GitHub when we’re happy for our collaborators (or the rest of the world) to see our work. However, this is entirely up to you and depends on the project (and who you are working with) and what your priorities are when using Git.\nIf you don’t want to track a file in your repository (maybe they are too large or transient files) you can get Git to ignore the file by adding it to the .gitignore file. On RStudio, in the git pane, you can right clicking on the filename to exclude and selecting ‘Ignore…’\n\n\n\n\n\n\n\n\nThis will add the filename to the .gitignore file. If you want to ignore multiple files or a particular type of file you can also include wildcards in the .gitignore file. For example to ignore all png files you can include the expression *.png in your .gitignore file and save.\nIf it all goes pear shaped and you end up completely trashing your Git repository don’t despair (we’ve all been there!). As long as your GitHub repository is good, all you need to do is delete the offending project directory on your computer, create a new RStudio project and link this with your remote GitHub repository using Option 2 (-Section 7.5.3). Once you have cloned the remote repository you should be good to go.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "07-github.html#sec-resources",
    "href": "07-github.html#sec-resources",
    "title": "7  Version control with Git and GitHub",
    "section": "\n7.10 Further resources",
    "text": "7.10 Further resources\nThere are many good online guides to learn more about git and GitHub and as with any open source software there is a huge community that can be a great resource:\n\nThe British Ecological Society guide to Reproducible Code\nThe GitHub guides\nThe Mozilla Science Lab GitHub for Collaboration on Open Projects guide\nJenny Bryan’s Happy Git and GitHub. We borrowed the idea (but with different content) of RStudio first, RStudio second in the ‘Setting up a version controlled Project in RStudio’ section.\nMelanie Frazier’s GitHub: A beginner’s guide to going back in time (aka fixing mistakes). We followed this structure (with modifications and different content) in the ‘Reverting changes’ section.\nIf you have done something terribly wrong and don’t know how to fix it try Oh Shit, Git or if you’re easily offended Dangit, Git\n\nThese are only a couple of examples, all you need to do is search for “version control with git and GitHub” to see how huge the community around these open source projects is and how many free resources are available for you to become a version control expert.",
    "crumbs": [
      "Data",
      "Using R",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Version control with Git and GitHub</span>"
    ]
  },
  {
    "objectID": "11-stats.html",
    "href": "11-stats.html",
    "title": "8  Simple Statistics in R",
    "section": "",
    "text": "8.1 Simple linear modelling\nLinear models are one of the most widely used models in statistics and data science. They are often thought of as simple models but they’re very flexible and able to model a wide variety of experimental and survey designs. Many of the statistical approaches you may have used previously (such as linear regression, t-test, ANOVA, ANCOVA etc) can be expressed as a linear model so the good news is that you’re probably already familiar with linear models (albeit indirectly). They also form the foundation of more complicated modelling approaches and are relatively easy to extended to incorporate additional complexity. During this section we’ll learn how to fit some simple linear models using R and cover some of the more common applications. We won’t go into any detail of the underlying linear modelling theory but rather focus on the practicalities of model fitting and R code.\nThe main function for fitting linear models in R is the lm() function (short for linear model!). The lm() function has many arguments but the most important is the first argument which specifies the model you want to fit using a model formula which typically takes the general form:\nThis model formula is simply read as\nThe response variable is also commonly known as the ‘dependent variable’ and the explanatory variables are sometimes referred to as ‘independent variables’ (or less frequently as ‘predictor variables’). There is also an additional term in our model formula which represents the variation in our response variable not explained by our explanatory variables but you don’t need to specify this when using the lm() function.\nAs mentioned above, many of the statistical ‘tests’ you might have previously used can be expressed as a linear model. For example, if we wanted to perform a bivariate linear regression between a response variable (y) and a single continuous explanatory variable (x) our model formula would simply be\nOn the other hand, if we wanted to use an ANOVA to test whether the group means of a response variable (y) were different between a three level factor (x) our model formula would look like\nOK, hang on, they both look identical, what gives? In addition to the model formula, the type of linear model you fit is also determined by the type of data in your explanatory variable(s) (i.e. what class of data). If your explanatory variable is continuous then you will fit a bivariate linear regression. If your explanatory variable is a factor (i.e. categorical data) you will fit an ANOVA type model.\nYou can also increase the complexity of your linear model by including additional explanatory variables in your model formula. For example, if we wanted to fit a two-way ANOVA both of our explanatory variables x and z would need to be factors and separated by a + symbol\nIf we wanted to perform a factorial ANOVA to identify an interaction between both explanatory variables we would separate our explanatory variables with a : symbol whilst also including our main effects in our model formula\nor by using the equivalent shortcut notation\nIt’s important that you get comfortable with using model formula (and we’ve only given the briefest of explanations above) when using the lm() function (and other functions) as it’s remarkably easy to specifiy a model which is either nonsense or isn’t the model you really wanted to fit. A summary table of various linear model formula and equivalent R code given below.\nOK, time for an example. The data file smoking.txt summarises the results of a study investigating the possible relationship between mortality rate and smoking across 25 occupational groups in the UK. The variable occupational.group specifies the different occupational groups studied, the risk.group variable indicates the relative risk to lung disease for the various occupational groups and smoking is an index of the average number of cigarettes smoked each day (relative to the number smoked across all occupations). The variable mortality is an index of the death rate from lung cancer in each group (relative to the death rate across all occupational groups). In this data set, the response variable is mortality and the potential explanatory variables are smoking which is numeric and risk.group which is a three level factor. The first thing to do is import our data file using the read.table() function as usual and assign the data to an object called smoke. You can find a link to download these data here.\nsmoke &lt;- read.table(\"data/smoking.txt\",\n  header = TRUE, sep = \"\\t\",\n  stringsAsFactors = TRUE\n)\nstr(smoke, vec.len = 2)\n\n'data.frame':   25 obs. of  4 variables:\n $ occupational.group: Factor w/ 25 levels \"Administrators\",..: 9 14 2 11 10 ...\n $ risk.group        : Factor w/ 3 levels \"high\",\"low\",\"medium\": 2 1 1 3 1 ...\n $ smoking           : int  77 137 117 94 116 ...\n $ mortality         : int  84 116 123 128 155 ...\n\n# vec.len argument to limited number of ' first elements' to display\nNext, let’s investigate the relationship between the mortality and smoking variables by plotting a scatter plot. We can use either the ggplot2 package or base R graphics to do this. We’ll use ggplot2 this time and our old friend the ggplot() function.\nlibrary(ggplot2)\nggplot(mapping = aes(x = smoking, y = mortality), data = smoke) +\n  geom_point()\nThe plot does suggest that there is a positive relationship between the smoking index and mortality index.\nTo fit a simple linear model to these data we will use the lm() function and include our model formula mortality ~ smoking and assign the results to an object called smoke_lm.\nsmoke_lm &lt;- lm(mortality ~ smoking, data = smoke)\nNotice that we have not used the $ notation to specify the variables in our model formula, instead we’ve used the data = smoke argument. Although the $ notation will work (i.e. smoke$mortality ~ smoke$smoking) it will more than likely cause you problems later on and should be avoided. In fact, we would go as far to suggest that if any function has a data = argument you should always use it. How do you know if a function has a data = argument? Just look in the associated help file.\nPerhaps somewhat confusingly (at least at first) it appears that nothing much has happened, you don’t automatically get the voluminous output that you normally get with other statistical packages. In fact, what R does, is store the output of the analysis in what is known as a lm class object (which we have called smoke_lm) from which you are able to extract exactly what you want using other functions. If you’re brave, you can examine the structure of the smoke_lm model object using the str() function.\nstr(smoke_lm)\n\nList of 12\n $ coefficients : Named num [1:2] -2.89 1.09\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"smoking\"\n $ residuals    : Named num [1:25] 3.15 -30.11 -1.36 28.66 31.73 ...\n  ..- attr(*, \"names\")= chr [1:25] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:25] -545 -91.63 2.72 26.99 35.56 ...\n  ..- attr(*, \"names\")= chr [1:25] \"(Intercept)\" \"smoking\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:25] 80.9 146.1 124.4 99.3 123.3 ...\n  ..- attr(*, \"names\")= chr [1:25] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:25, 1:2] -5 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 0.2 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:25] \"1\" \"2\" \"3\" \"4\" ...\n  .. .. ..$ : chr [1:2] \"(Intercept)\" \"smoking\"\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.2 1.46\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 1e-07\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 23\n $ xlevels      : Named list()\n $ call         : language lm(formula = mortality ~ smoking, data = smoke)\n $ terms        :Classes 'terms', 'formula'  language mortality ~ smoking\n  .. ..- attr(*, \"variables\")= language list(mortality, smoking)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. ..$ : chr [1:2] \"mortality\" \"smoking\"\n  .. .. .. ..$ : chr \"smoking\"\n  .. ..- attr(*, \"term.labels\")= chr \"smoking\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(mortality, smoking)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mortality\" \"smoking\"\n $ model        :'data.frame':  25 obs. of  2 variables:\n  ..$ mortality: int [1:25] 84 116 123 128 155 101 118 113 104 88 ...\n  ..$ smoking  : int [1:25] 77 137 117 94 116 102 111 93 88 102 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mortality ~ smoking\n  .. .. ..- attr(*, \"variables\")= language list(mortality, smoking)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. .. .. ..$ : chr [1:2] \"mortality\" \"smoking\"\n  .. .. .. .. ..$ : chr \"smoking\"\n  .. .. ..- attr(*, \"term.labels\")= chr \"smoking\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mortality, smoking)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mortality\" \"smoking\"\n - attr(*, \"class\")= chr \"lm\"\nTo obtain a summary of our analysis we can use the summary() function on our smoke_lm model object.\nsummary(smoke_lm)\n\n\nCall:\nlm(formula = mortality ~ smoking, data = smoke)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-30.107 -17.892   3.145  14.132  31.732 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2.8853    23.0337  -0.125    0.901    \nsmoking       1.0875     0.2209   4.922 5.66e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.62 on 23 degrees of freedom\nMultiple R-squared:  0.513, Adjusted R-squared:  0.4918 \nF-statistic: 24.23 on 1 and 23 DF,  p-value: 5.658e-05\nThis shows you everything you need to know about the parameter estimates (intercept and slope), their standard errors and associated t statistics and p values. The estimate for the Intercept suggests that when the relative smoking index is 0 the relative mortality rate is -2.885! The p value associated with the intercept tests the null hypothesis that the intercept is equal to zero. As the p value is large we fail to reject this null hypothesis. The smoking parameter estimate (1.0875) is the estimate of the slope and suggests that for every unit increase in the average number of cigarettes smoked each day the mortality risk index increases by 1.0875. The p value associated with the smoking parameter tests whether the slope of this relationship is equal to zero (i.e. no relationship). As our p value is small we reject this null hypothesis and therefore the slope is different from zero and therefore there is a significant relationship. The summary table also includes other important information such as the coefficient of determination (R2), adjusted R2 , F statistic, associated degrees of freedom and p value. This information is a condensed form of an ANOVA table which you can see by using the anova() function.\nanova(smoke_lm)\n\nAnalysis of Variance Table\n\nResponse: mortality\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsmoking    1 8395.7  8395.7  24.228 5.658e-05 ***\nResiduals 23 7970.3   346.5                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nNow let’s fit another linear model, but this time we will use the risk.group variable as our explanatory variable. Remember the risk.group variable is a factor and so our linear model will be equivalent to an ANOVA type analysis. We will be testing the null hypothesis that there is no difference in the mean mortality rate between the low, medium and high groups. We fit the model in exactly the same way as before.\nsmoke_risk_lm &lt;- lm(mortality ~ risk.group, data = smoke)\nAgain, we can produce an ANOVA table using the anova() function\nanova(smoke_risk_lm)\n\nAnalysis of Variance Table\n\nResponse: mortality\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nrisk.group  2 11514.4  5757.2  26.107 1.554e-06 ***\nResiduals  22  4851.6   220.5                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThe results presented in the ANOVA table suggest that we can reject the null hypothesis (very small p value) and therefore the mean mortality rate index is different between low, medium and high risk groups.\nAs we did with our first linear model we can also produce a summary of the estimated parameters using the summary() function.\nsummary(smoke_risk_lm)\n\n\nCall:\nlm(formula = mortality ~ risk.group, data = smoke)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-26.17 -11.45   4.00   9.00  26.83 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        135.00       5.25  25.713  &lt; 2e-16 ***\nrisk.grouplow      -57.83       8.02  -7.211 3.16e-07 ***\nrisk.groupmedium   -27.55       6.90  -3.992 0.000615 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.85 on 22 degrees of freedom\nMultiple R-squared:  0.7036,    Adjusted R-squared:  0.6766 \nF-statistic: 26.11 on 2 and 22 DF,  p-value: 1.554e-06\nIn the summary table the Intercept is set to the first level of risk.group (high) as this occurs first alphabetically. Therefore, the estimated mean mortality index for high risk individuals is 135. The estimates for risk.grouplow and risk.groupmedium are mean differences from the intercept (high group). So the mortality index for the low group is 135 - 57.83 = 77.17 and for the medium group is 135 - 27.55 = 107.45. The t values and p values in the summary table are associated with testing specific hypotheses. The p value associated with the intercept tests the null hypothesis that the mean mortality index for the high group is equal to zero. To be honest this is not a particularly meaningful hypothesis to test but we can reject it anyway as we have a very small p value. The p value for the risk.grouplow parameter tests the null hypothesis that the mean difference between high and low risk groups is equal to zero (i.e. there is no difference). Again we reject this null hypothesis and conclude that the means are different between these two groups. Similarly, the p value for risk.groupmedium tests the null hypothesis that the mean difference between high and medium groups is equal to zero which we also reject.\nDon’t worry too much if you find the output from the summary() function a little confusing. Its takes a bit of practice and experience to be able to make sense of all the numbers. Remember though, the more complicated your model is, the more complicated your interpretion will be. And always remember, a model that you can’t interpret is not worth fitting (most of the time!).\nAnother approach to interpreting your model output is to plot a graph of your data and then add the fitted model to this plot. Let’s go back to the first linear model we fitted (smoke_lm). We can add the fitted line to our previous plot using the ggplot2 package and the geom_smooth geom. We can easily include the standard errors by specifying the se = TRUE argument.\nggplot(mapping = aes(x = smoking, y = mortality), data = smoke) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = TRUE)\nYou can also do this with R’s base graphics. Note though that the fitted line extends beyond the data which is not great practice. If you want to prevent this you can generate predicted values from the model using the predict() function within the range of your data and then add these values to the plot using the lines() function (not shown).\nplot(smoke$smoking, smoke$mortality, xlab = \"smoking rate\", ylab = \" mortality rate\")\nabline(smoke_lm, col = \"red\")\nBefore we sit back and relax and admire our model (or go write that high impact paper your supervisor/boss has been harassing you about) our work is not finished. It’s vitally important to check the underlying assumptions of your linear model. Two of the most important assumption are equal variances (homogeneity of variance) and normality of residuals. To check for equal variances we can construct a graph of residuals versus fitted values. We can do this by first extracting the residuals and fitted values from our model object using the resid() and fitted() functions.\nsmoke_res &lt;- resid(smoke_lm)\nsmoke_fit &lt;- fitted(smoke_lm)\nAnd then plot them using ggplot or base R graphics.\nggplot(mapping = aes(x = smoke_fit, y = smoke_res)) +\n  geom_point() +\n  geom_hline(yintercept = 0, colour = \"red\", linetype = \"dashed\")\nIt takes a little practice to interpret these types of graph, but what you are looking for is no pattern or structure in your residuals. What you definitely don’t want to see is the scatter increasing around the zero line (red dashed line) as the fitted values get bigger (this has been described as looking like a trumpet, a wedge of cheese or even a slice of pizza) which would indicate unequal variances (heteroscedacity).\nTo check for normality of residuals we can use our old friend the Q-Q plot using the residuals stored in the smoke_res object we created earlier.\nggplot(mapping = aes(sample = smoke_res)) +\n  stat_qq() +\n  stat_qq_line()\nOr the same plot with base graphics.\nqqnorm(smoke_res)\nqqline(smoke_res)\nAlternatively, you can get R to do most of the hard work by using the plot() function on the model object smoke_lm. Before we do this we should tell R that we want to plot four graphs in the same plotting window in RStudio using the par(mfrow = c(2,2)). This command splits the plotting window into 2 rows and 2 columns.\npar(mfrow = c(2, 2))\nplot(smoke_lm)\nThe first two graphs (top left and top right) are the same residual versus fitted and Q-Q plots we produced before. The third graph (bottom left) is the same as the first but plotted on a different scale (the absolute value of the square root of the standardised residuals) and again you are looking for no pattern or structure in the data points. The fourth graph (bottom right) gives you an indication whether any of your observations are having a large influence (Cook’s distance) on your regression coefficient estimates. Levearge identifies observations which have unusually large values in their explanatory variables.\nYou can also produce these diagnostic plots using ggplot by installing the package ggfortify and using the autoplot() function.\nlibrary(ggfortify)\nautoplot(smoke_lm, which = 1:6, ncol = 2, label.size = 3)\nWhat you do about influential data points or data points with high leverage is up to you. If you would like to examine the effect of removing one of these points on the parameter estimates you can use the update() function. Let’s remove data point 2 (miners, mortality = 116 and smoking = 137) and store the results in a new object called smoke_lm2. Note, we do this to demonstrate the use of the update() function. You should think long and hard about removing any data point(s) and if you do you should always report this and justify your reasoning.\nsmoke_lm2 &lt;- update(smoke_lm, subset = -2)\nsummary(smoke_lm2)\n\n\nCall:\nlm(formula = mortality ~ smoking, data = smoke, subset = -2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-29.7425 -11.6920  -0.4745  13.6141  28.7587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -20.0755    23.5798  -0.851    0.404    \nsmoking       1.2693     0.2297   5.526 1.49e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.62 on 22 degrees of freedom\nMultiple R-squared:  0.5813,    Adjusted R-squared:  0.5622 \nF-statistic: 30.54 on 1 and 22 DF,  p-value: 1.488e-05\nThere are numerous other functions which are useful for producing diagnostic plots. For example, rstandard() and rstudent() returns the standardised and studentised residuals. The function dffits() expresses how much an observation influences the associated fitted value and the function dfbetas() gives the change in the estimated parameters if an observation is excluded, relative to its standard error (intercept is the solid line and slope is the dashed line in the example below). The solid bold line in the same graph represents the Cook’s distance. Examples of how to use these functions are given below.\npar(mfrow = c(2, 2))\nplot(dffits(smoke_lm), type = \"l\")\nplot(rstudent(smoke_lm))\nmatplot(dfbetas(smoke_lm), type = \"l\", col = \"black\")\nlines(sqrt(cooks.distance(smoke_lm)), lwd = 2)",
    "crumbs": [
      "Data",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simple Statistics in R</span>"
    ]
  },
  {
    "objectID": "11-stats.html#sec-simple_lm",
    "href": "11-stats.html#sec-simple_lm",
    "title": "8  Simple Statistics in R",
    "section": "",
    "text": "response variable ~ explanatory variable(s)\n\n\n\n‘variation in the response variable modelled as a function (~) of the explanatory variable(s)’.\n\n\n\ny ~ x\n\ny ~ x\n\n\ny ~ x + z\n\ny ~ x + z + x:z\n\ny ~ x * z \n\n\n\n\n\n\n\n\nTraditional name\nModel formula\nR code\n\n\n\nBivariate regression\nY ~ X1 (continuous)\nlm(Y ~ X)\n\n\nOne-way ANOVA\nY ~ X1 (categorical)\nlm(Y ~ X)\n\n\nTwo-way ANOVA\nY ~ X1 (cat) + X2(cat)\nlm(Y ~ X1 + X2)\n\n\nANCOVA\nY ~ X1 (cat) + X2(cont)\nlm(Y ~ X1 + X2)\n\n\nMultiple regression\nY ~ X1 (cont) + X2(cont)\nlm(Y ~ X1 + X2)\n\n\nFactorial ANOVA\nY ~ X1 (cat) * X2(cat)\n\nlm(Y ~ X1 * X2) or lm(Y ~ X1 + X2 + X1:X2)",
    "crumbs": [
      "Data",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simple Statistics in R</span>"
    ]
  },
  {
    "objectID": "11-stats.html#other-modelling-approaches",
    "href": "11-stats.html#other-modelling-approaches",
    "title": "8  Simple Statistics in R",
    "section": "\n8.2 Other modelling approaches",
    "text": "8.2 Other modelling approaches\nAs with most things R related, a complete description of the variety and flexibility of different statistical analyses you can perform is beyond the scope of this introductory text. Further information can be found in any of the excellent documents referred to in Section 2.6.2. A table of some of the more common statistical functions is given below to get you started.\n\n\n\n\n\n\nR function\nUse\n\n\n\nglm()\nFit a generalised linear model with a specific error structure specified using the family = argument (Poisson, binomial, gamma)\n\n\ngam()\nFit a generalised additive model. The R package mgcv must be loaded\n\n\n\nlme() & nlme()\n\nFit linear and non-linear mixed effects models. The R package nlme must be loaded\n\n\nlmer()\nFit linear and generalised linear and non-linear mixed effects models.\n\n\n\nThe package lme4 must be installed and loaded\n\n\ngls()\nFit generalised least squares models. The R package nlme must be loaded\n\n\nkruskal.test()\nPerforms a Kruskal-Wallis rank sum test\n\n\nfriedman.test()\nPerforms a Friedman’s test",
    "crumbs": [
      "Data",
      "Linear models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Simple Statistics in R</span>"
    ]
  },
  {
    "objectID": "901-references.html",
    "href": "901-references.html",
    "title": "References",
    "section": "",
    "text": "R packages\nThis book was produced all packages (excluding dependencies) listed in table Table 1. As recommended by the ‘tidyverse’ team, all citations to tidyverse packages are collapsed into a single citation.\nTable 1: R packages used in this book\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nbase\n4.3.3\nR Core Team (2024)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggcleveland\n0.1.0\nPrunello and Mari (2021)\n\n\nggfortify\n0.4.16\n\nTang et al. (2016); Horikoshi and Tang (2018)\n\n\n\nggpubr\n0.6.0\nKassambara (2023)\n\n\ngrateful\n0.2.4\nFrancisco Rodriguez-Sanchez and Connor P. Jackson (2023)\n\n\nhexbin\n1.28.3\nCarr et al. (2023)\n\n\nkableExtra\n1.4.0\nZhu (2024)\n\n\nknitr\n1.45\n\nXie (2014); Xie (2015); Xie (2023)\n\n\n\npatchwork\n1.2.0\nPedersen (2024)\n\n\nquantreg\n5.97\nKoenker (2023)\n\n\nreshape2\n1.4.4\nWickham (2007)\n\n\nrmarkdown\n2.26\n\nXie et al. (2018); Xie et al. (2020); Allaire et al. (2024)\n\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\nvioplot\n0.4.0\nAdler et al. (2022)",
    "crumbs": [
      "Data",
      "References"
    ]
  },
  {
    "objectID": "901-references.html#bibliography",
    "href": "901-references.html#bibliography",
    "title": "References",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\n\n\n\n\nAdler, D., S. T. Kelly, T. Elliott, and J. Adamson. 2022. vioplot: Violin plot.\n\n\nAllaire, J., Y. Xie, C. Dervieux, J. McPherson, J. Luraschi, K. Ushey, A. Atkins, H. Wickham, J. Cheng, W. Chang, and R. Iannone. 2024. rmarkdown: Dynamic documents for r.\n\n\nCarr, D., ported by Nicholas Lewin-Koh, M. Maechler, and contains copies of lattice functions written by Deepayan Sarkar. 2023. hexbin: Hexagonal binning routines.\n\n\nFrancisco Rodriguez-Sanchez, and Connor P. Jackson. 2023. grateful: Facilitate citation of r packages.\n\n\nHorikoshi, M., and Y. Tang. 2018. ggfortify: Data visualization tools for statistical analysis results.\n\n\nKassambara, A. 2023. ggpubr: “ggplot2” based publication ready plots.\n\n\nKoenker, R. 2023. quantreg: Quantile regression.\n\n\nPedersen, T. L. 2024. patchwork: The composer of plots.\n\n\nPrunello, M., and G. Mari. 2021. ggcleveland: Implementation of plots from cleveland’s visualizing data book.\n\n\nR Core Team. 2024. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.\n\n\nSchloerke, B., D. Cook, J. Larmarange, F. Briatte, M. Marbach, E. Thoen, A. Elberg, and J. Crowley. 2024. GGally: Extension to “ggplot2”.\n\n\nTang, Y., M. Horikoshi, and W. Li. 2016. ggfortify: Unified interface to visualize statistical result of popular r packages. The R Journal 8:474–485.\n\n\nWickham, H. 2007. Reshaping data with the reshape package. Journal of Statistical Software 21:1–20.\n\n\nWickham, H., M. Averick, J. Bryan, W. Chang, L. D. McGowan, R. François, G. Grolemund, A. Hayes, L. Henry, J. Hester, M. Kuhn, T. L. Pedersen, E. Miller, S. M. Bache, K. Müller, J. Ooms, D. Robinson, D. P. Seidel, V. Spinu, K. Takahashi, D. Vaughan, C. Wilke, K. Woo, and H. Yutani. 2019. Welcome to the tidyverse. Journal of Open Source Software 4:1686.\n\n\nXie, Y. 2014. knitr: A comprehensive tool for reproducible research in R. in V. Stodden, F. Leisch, and R. D. Peng, editors. Implementing reproducible computational research. Chapman; Hall/CRC.\n\n\nXie, Y. 2015. Dynamic documents with R and knitr. 2nd edition. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y. 2023. knitr: A general-purpose package for dynamic report generation in r.\n\n\nXie, Y., J. J. Allaire, and G. Grolemund. 2018. R markdown: The definitive guide. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nXie, Y., C. Dervieux, and E. Riederer. 2020. R markdown cookbook. Chapman; Hall/CRC, Boca Raton, Florida.\n\n\nZhu, H. 2024. kableExtra: Construct complex table with “kable” and pipe syntax.",
    "crumbs": [
      "Data",
      "References"
    ]
  },
  {
    "objectID": "902-data-files.html",
    "href": "902-data-files.html",
    "title": "Appendix A — Data used in this book",
    "section": "",
    "text": "check what is done for BIO8940\nuse downlaod this to create nice downlaod options potentially\npath_files &lt;- list.files(path = system.file(\"assets/css\", package = \"downloadthis\"), full.names = TRUE)\n\ndownload_file(\n  path = path_files,\n  output_name = \"Files from downloadthis\",\n  button_label = \"Download files\",\n  button_type = \"danger\",\n  has_icon = TRUE,\n  icon = \"fa fa-save\",\n  self_contained = FALSE\n)",
    "crumbs": [
      "Data",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Data used in this book</span>"
    ]
  },
  {
    "objectID": "903-latex.html",
    "href": "903-latex.html",
    "title": "Appendix B — Installing Quarto and LateX",
    "section": "",
    "text": "B.1 MS Windows\nAn alternative option would be to install MiKTeX instead. You can download the latest distribution of MiKTeX. Installing MiKTeX is pretty straight forward, but it can sometimes be a pain to get it to play nicely with RStudio. If at all possible we recommend that you use TinyTex.",
    "crumbs": [
      "Data",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Installing Quarto and LateX</span>"
    ]
  },
  {
    "objectID": "903-latex.html#mac-osx",
    "href": "903-latex.html#mac-osx",
    "title": "Appendix B — Installing Quarto and LateX",
    "section": "\nB.2 Mac OSX",
    "text": "B.2 Mac OSX\nIf for some reason TinyTeX does not work on your Mac computer then you can try to install MacTeX instead. You can download the latest version of MacTeX here.",
    "crumbs": [
      "Data",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Installing Quarto and LateX</span>"
    ]
  },
  {
    "objectID": "903-latex.html#linux",
    "href": "903-latex.html#linux",
    "title": "Appendix B — Installing Quarto and LateX",
    "section": "\nB.3 Linux",
    "text": "B.3 Linux\nAn alternative to TinyTex on linux would be to use a full fledge distribution of \\(\\LaTeX\\) such as TexLive",
    "crumbs": [
      "Data",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Installing Quarto and LateX</span>"
    ]
  }
]